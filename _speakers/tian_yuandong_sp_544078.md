---
name: "Yuandong Tian"
role: "Speaker"
affiliation: "Meta AI Research"
website: "https://yuandong-tian.com/"
photo: "tian.png"
talk: "Emergence of Various Structures During Transformer Training via the Lens of Training Dynamics"
abstract: "Large Language Models (LLMs) have demonstrated remarkable performance across diverse applications. However, most empirical works treat the underlying architecture as black boxes, and it remains a mystery what representation the model learns and how it learns. I will cover two aspects of our theoretical analysis, including the training dynamics of self-attention layers when learning Transformers (i.e. how it learns), as well as intriguing structure of the resulting representations (i.e. what it learns), which includes not only basic structure of sparsity and low rankness, but also more complicated ones such as algebraic, hierarchical and spectral structures. Our analysis provides insights into the complicated nonlinear learning process beyond the scope of traditional learning theory, leads to development of novel empirical approaches and shed light on a possible unification of neural and symbolic representations."
bio: "TBA"
day: "4"
start: "3:30 PM"
end: "4:30 PM"
location: "Simonyi Conference Center"
---
