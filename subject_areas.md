---
layout: page
title: Subject Areas
description: List of subject areas for conference submissions
parent: Call for Papers
nav_order: 0
---

{% include splash.html %}

# Subject Areas

## Models and Algorithms 
- Parsimonious training and inference algorithms for deep networks, including but not limited to:
  - Pruning, sparse training, lottery ticket hypothesis
  - Low-rank training, quantization, distillation, retrieval-augmented models
  - Adaptive/conditional computation, including mixture of experts
  - Parsimonious transfer learning methods, such as sparse tuning or LoRA
- Compact and efficient neural network architectures by design
- Model-based architectures as inspired by structured models (such as unrolling)
- Robust/stable/invariant models or training/inference algorithms, guided by parsimony principles
- Nonlinear dimension reduction methods (such as autoencoders, subspace learning, manifold learning, etc.)
- Interpretability induced by parsimonious modeling (such as feature selection, model visualization)
- Generative models guided by parsimony principles
- Distributed, federated, and communicated-efficient training or inference, that leverage model parsimony
- Efficient neural scaling, and next-generation parsimonious architectures beyond common in-use models

## Data 
- Modern signal models: probabilistic, geometric (manifolds, graphical),
  visual/3D, language, dynamical, hierarchical structures 
- Dataset parsimony (such as data filtering, coreset selection), and sparse
  data formats in non-Euclidean domains such as graph
- Empirical and theoretical studies of representation learning with structured
  data

## Theory 
- Generalization, optimization, robustness,  and approximation in deep
  learning, rigorously relating to its implicit parsimony
- Theories for classical sparse coding, dictionary learning, structured
  sparsity, subspace learning, etc., and their connections to neural network
  sparsity
- Forgetting owing to sparsity, including fairness, privacy and bias concerns

## Hardware and Systems
- Libraries, kernels, and compilers for accelerating sparse computation
- Hardware with customized support for sparse computation
- Resource-efficient learning and co-design applications at the edge or the cloud

## Applications and Science
- Parsimonious AI for science and engineering applications, such as various
  inverse problems that benefit from parsimonious priors
- Theoretical neuroscience and cognitive science foundations for parsimony, and
  biologically inspired algorithms
- Other application fields crossing disciplinary boundaries and suggesting
  further collaborations under the theme of parsimony: computer vision,
  robotics, reinforcement learning, and more

The above is intended as a high-level overview of CPALâ€™s focus and by no means
exclusive. If you doubt that your paper fits the venue, feel free to contact
the program chairs via email at [pcs@cpal.cc](mailto:pcs@cpal.cc).
