---
name: "Wei Huang"
role: "Research Scientist"
affiliation: "RIKEN"
website: "https://weihuang05.github.io/"
photo: "image7.png"
talk: "Advancing Feature Learning Theory: Optimization and Generalization for Foundation Models"
abstract: "Foundation models, particularly Transformers, have revolutionized modern machine learning, showcasing remarkable capabilities such as in-context learning (ICL), multi-modal representation learning, and vision-specific applications. However, a deep theoretical understanding of their optimization dynamics, generalization mechanisms, and emergent behaviors remains incomplete. My recent research addresses these challenges, developing principled frameworks to unravel the intricate mechanisms of foundation models. This talk will explore three key contributions: (1) Optimization and Generalization in Transformers, where I analyze training dynamics and characterize the transition between effective and poor generalization in noisy data settings; (2) In-Context Learning, with a novel mathematical framework explaining how Transformers leverage multi-concept word semantics for efficient task adaptation; and (3) Multi-Modal Contrastive Learning, establishing a unified feature learning theory to explain why multi-modal learning outperforms single-modal approaches in both optimization and downstream generalization. These contributions bridge the gap between theoretical advancements and practical implementations, paving the way for the design of scalable, trustworthy, and efficient foundation model."
session: "2"
order: "2"
---
