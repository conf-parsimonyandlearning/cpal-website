---
layout: page
title: Conference Vision
nav_order: 11
---

{% include splash.html %}

# Conference Vision

{: .quote}
> *“Everything should be made as simple as possible, but not any simpler.”* <br>
> {: .text-left .fs-4}
> -- Albert Einstein
> {:.text-right}

One of the most fundamental reasons for the very existence and therefore
emergence of intelligence or science is that the world is not fully random, but
highly structured and predictable. Hence, a fundamental purpose and function of
intelligence or science is to learn parsimonious models (or laws) for such
predicable structures, from massive sensed data of the world. 

Over the past decade, the advent of machine learning and large-scale computing
has immeasurably changed the ways we process, interpret, and predict with data
in engineering and science. The 'traditional' approach to algorithm design,
based around parametric models for specific structures of signals and
measurements -- say sparse and low-rank models -- and the associated optimization
toolkit, is now significantly enriched with data-driven learning-based
techniques, where large-scale networks are pre-trained and then adapted to a
variety of specific tasks. Nevertheless, the successes of both modern
data-driven and classic model-based paradigms rely crucially on correctly
identifying the low-dimensional structures present in real-world data, to the
extent that we see the roles of *learning* and *compression* of
data processing algorithms -- whether explicit or implicit, as with deep
networks -- as inextricably linked.

Over the last ten or so years, several rich lines of research, including
theoretical, computational, and practical, have explored the interplay between
learning and compression. Some works explore the role of signal models in the
era of deep learning, attempting to understand the interaction between deep
networks and nonlinear, multi-modal data structures. Others have applied these
insights to the principled design of deep architectures that incorporate
desired  structures in data into the learning process. Still others have
considered generic deep networks as first-class citizens in their own right,
exploring ways to compress and sparsify models for greater efficiency, often
accompanied by hardware or system-aware co-designs. Across each of these
settings, theoretical works rooted in low-dimensional modeling have begun to
explain the foundations of deep architectures and efficient learning -- from
optimization to generalization -- in spite of "overparameterization" and other
obstructions. Most recently, the advent of foundation models has led some to
posit that parsimony and compression itself are a fundamental part of the
learning objective of an intelligent system, connecting to ideas from
neuroscience on compression as a guiding principle for the brain representing
the sensory data of the world.

By and large, these lines of work have so far developed somewhat in isolation
from one another, in spite of their common basis and purpose for parsimony and
learning. Our intention in organizing this conference is to address this issue
and go beyond: *we envision the conference as a general scientific forum
where researchers in machine learning, applied mathematics, signal processing,
optimization, intelligent systems, and all associated science and engineering
fields can gather, share insights, and ultimately work towards a common modern
theoretical and computational framework for understanding intelligence and
science from the perspective of parsimonious learning.*
