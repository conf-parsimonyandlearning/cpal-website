<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Keynote Speakers | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Keynote Speakers" /><meta property="og:locale" content="en_US" /><meta name="description" content="Conference on Parsimony and Learning (CPAL) - Addressing the low-dimensional structures in high-dimensional data that prevail in machine learning, signal processing, optimization, and beyond." /><meta property="og:description" content="Conference on Parsimony and Learning (CPAL) - Addressing the low-dimensional structures in high-dimensional data that prevail in machine learning, signal processing, optimization, and beyond." /><link rel="canonical" href="https://cpal.cc/speakers/" /><meta property="og:url" content="https://cpal.cc/speakers/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Keynote Speakers" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Conference on Parsimony and Learning (CPAL) - Addressing the low-dimensional structures in high-dimensional data that prevail in machine learning, signal processing, optimization, and beyond.","headline":"Keynote Speakers","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/speakers/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">Venue</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/program_highlights/" class="nav-list-link">Program Highlights</a><li class="nav-list-item "> <a href="/oral_and_spotlight_presentations/" class="nav-list-link">Orals and Recent Spotlights</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a><li class="nav-list-item "> <a href="/tutorials/" class="nav-list-link">Tutorials</a><li class="nav-list-item "> <a href="/wellness/" class="nav-list-link">Tailored Wellness Sessions</a></ul><li class="nav-list-item active"> <a href="/speakers/" class="nav-list-link active">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal-registration" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="keynote-speakers"> <a href="#keynote-speakers" class="anchor-heading" aria-labelledby="keynote-speakers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Keynote Speakers</h1><p>Clicking a speaker’s photo will jump to their talk information below.</p><div style="clear: both; display: flex; flex-wrap: wrap; justify-content: center; max-width: 575px; column-gap: 10px;"><div class="speaker" style="width: 140px;"> <a href="/speakers/#dan-alistarh"><img class="speaker-image" src="/assets/images/speakers/alistarh.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://people.csail.mit.edu/alistarh/">Dan Alistarh</a></h3><p>Institute of Science and Technology Austria / Neural Magic</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#sueyeon-chung"><img class="speaker-image" src="/assets/images/speakers/sueyeon.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://sites.google.com/site/sueyeonchung/">SueYeon Chung</a></h3><p>New York University / Flatiron Institute</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#kostas-daniilidis"><img class="speaker-image" src="/assets/images/speakers/kostas.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a></h3><p>University of Pennsylvania</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#maryam-fazel"><img class="speaker-image" src="/assets/images/speakers/mfazel.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://people.ece.uw.edu/fazel_maryam/">Maryam Fazel</a></h3><p>University of Washington</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#tom-goldstein"><img class="speaker-image" src="/assets/images/speakers/goldstein.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a></h3><p>University of Maryland</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#yingbin-liang"><img class="speaker-image" src="/assets/images/speakers/liang.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://sites.google.com/view/yingbinliang/home">Yingbin Liang</a></h3><p>Ohio State University</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#dimitris-papailiopoulos"><img class="speaker-image" src="/assets/images/speakers/papailiopoulos.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://papail.io/">Dimitris Papailiopoulos</a></h3><p>University of Wisconsin-Madison</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#stefano-soatto"><img class="speaker-image" src="/assets/images/speakers/soatto.jpeg" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://web.cs.ucla.edu/~soatto/">Stefano Soatto</a></h3><p>University of California, Los Angeles</p></div></div><div class="speaker" style="width: 140px;"> <a href="/speakers/#jong-chul-ye"><img class="speaker-image" src="/assets/images/speakers/jongchulye.png" alt="" /></a><div><h3 class="speaker-name no_anchor"> <a href="https://bispl.weebly.com/professor.html">Jong Chul Ye</a></h3><p>Korea Advanced Institute of Science and Technology (KAIST)</p></div></div></div><h1 id="talk-details"> <a href="#talk-details" class="anchor-heading" aria-labelledby="talk-details"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Talk Details</h1><h3 id="dan-alistarh"> <a href="#dan-alistarh" class="anchor-heading" aria-labelledby="dan-alistarh"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://people.csail.mit.edu/alistarh/">Dan Alistarh</a></h3><p>Institute of Science and Technology Austria / Neural Magic</p><h4 id="title-accurate-model-compression-at-gpt-scale"> <a href="#title-accurate-model-compression-at-gpt-scale" class="anchor-heading" aria-labelledby="title-accurate-model-compression-at-gpt-scale"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Accurate Model Compression at GPT Scale</h4><h4 id="time-and-location-day-1-400-pm-hkt-tba"> <a href="#time-and-location-day-1-400-pm-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-1-400-pm-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 1, 4:00 PM HKT</a>, TBA</h4><h4 id="abstract"> <a href="#abstract" class="anchor-heading" aria-labelledby="abstract"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>A key barrier to the wide deployment of highly-accurate machine learning models, whether for language or vision, is their high computational and memory overhead. Although we possess the mathematical tools for highly-accurate compression of such models, these theoretically-elegant techniques require second-order information of the model’s loss function, which is hard to even approximate efficiently at the scale of billion-parameter models.In this talk, I will describe our work on bridging this computational divide, which enables the accurate second-order pruning and quantization of models at truly massive scale. Compressed using our techniques, models with billions and even trillions of parameters can be executed efficiently on a few GPUs, with significant speedups, and negligible accuracy loss. Based in part on our work, the community has been able to run accurate billion or even trillion-parameter models on computationally-limited devices.</p><h4 id="bio"> <a href="#bio" class="anchor-heading" aria-labelledby="bio"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Dan Alistarh is a Professor at IST Austria, in Vienna. Previously, he was a Researcher with Microsoft, a Postdoc at MIT CSAIL, and received his PhD from the EPFL. His research is on algorithms for efficient machine learning and high-performance computing, with a focus on scalable DNN inference and training, for which he was awarded an ERC Starting Grant in 2018. In his spare time, he works with the ML research team at Neural Magic, a startup based in Boston, on making compression faster, more accurate and accessible to practitioners.</p><h3 id="sueyeon-chung"> <a href="#sueyeon-chung" class="anchor-heading" aria-labelledby="sueyeon-chung"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.google.com/site/sueyeonchung/">SueYeon Chung</a></h3><p>New York University / Flatiron Institute</p><h4 id="title-multi-level-theory-of-neural-representationscapacity-of-neural-manifolds-in-biological-and-artificial-neural-networks"> <a href="#title-multi-level-theory-of-neural-representationscapacity-of-neural-manifolds-in-biological-and-artificial-neural-networks" class="anchor-heading" aria-labelledby="title-multi-level-theory-of-neural-representationscapacity-of-neural-manifolds-in-biological-and-artificial-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Multi-level theory of neural representations: Capacity of neural manifolds in biological and artificial neural networks</h4><h4 id="time-and-location-day-4-1000-am-hkt-tba"> <a href="#time-and-location-day-4-1000-am-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-4-1000-am-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 4, 10:00 AM HKT</a>, TBA</h4><h4 id="abstract-1"> <a href="#abstract-1" class="anchor-heading" aria-labelledby="abstract-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>A central goal in neuroscience is to understand how orchestrated computations in the brain arise from the properties of single neurons and networks of such neurons. Answering this question requires theoretical advances that shine a light on the ‘black box’ of representations in neural circuits. In this talk, we will demonstrate theoretical approaches that help describe how cognitive task implementations emerge from the structure in neural populations and from biologically plausible neural networks. <br /> We will introduce a new theory that connects geometric structures that arise from neural population responses (i.e., neural manifolds) to the neural representation’s efficiency in implementing a task. In particular, this theory describes how many neural manifolds can be represented (or ‘packed’) in the neural activity space while they can be linearly decoded by a downstream readout neuron. The intuition from this theory is remarkably simple: like a sphere packing problem in physical space, we can encode many “neural manifolds” into the neural activity space if these manifolds are small and low-dimensional, and vice versa. <br /> Next, we will describe how such an approach can, in fact, open the ‘black box’ of distributed neuronal circuits in a range of settings, such as experimental neural datasets and artificial neural networks. In particular, our method overcomes the limitations of traditional dimensionality reduction techniques, as it operates directly on the high-dimensional representations. Furthermore, this method allows for simultaneous multi-level analysis, by measuring geometric properties in neural population data and estimating the amount of task information embedded in the same population. <br /> Finally, we will discuss our recent efforts to fully extend this multi-level description of neural populations by (1) understanding how task-implementing neural manifolds emerge across brain regions and during learning, (2) investigating how neural tuning properties shape the representation geometry in early sensory areas, and (3) demonstrating the impressive task performance and neural predictivity achieved by optimizing a deep network to maximize the capacity of neural manifolds. By expanding our mathematical toolkit for analyzing representations underlying complex neuronal networks, we hope to contribute to the long-term challenge of understanding the neuronal basis of tasks and behaviors.</p><h4 id="bio-1"> <a href="#bio-1" class="anchor-heading" aria-labelledby="bio-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>SueYeon Chung is an Assistant Professor in the Center for Neural Science at NYU, with a joint appointment in the Center for Computational Neuroscience at the Flatiron Institute, an internal research division of the Simons Foundation. She is also an affiliated faculty member at the Center for Data Science and Cognition and Perception Program at NYU. Prior to joining NYU, she was a Postdoctoral Fellow in the Center for Theoretical Neuroscience at Columbia University, and BCS Fellow in Computation at MIT. Before that, she received a Ph.D. in applied physics at Harvard University, and a B.A. in mathematics and physics at Cornell University. She received the Klingenstein-Simons Fellowship Award in Neuroscience in 2023. Her main research interests lie at the intersection between computational neuroscience and deep learning, with a particular focus on understanding and interpreting neural computation in biological and artificial neural networks by employing methods from neural network theory, statistical physics, and high-dimensional statistics.</p><h3 id="kostas-daniilidis"> <a href="#kostas-daniilidis" class="anchor-heading" aria-labelledby="kostas-daniilidis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a></h3><p>University of Pennsylvania</p><h4 id="title-parsimony-through-equivariance"> <a href="#title-parsimony-through-equivariance" class="anchor-heading" aria-labelledby="title-parsimony-through-equivariance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Parsimony through Equivariance</h4><h4 id="time-and-location-day-2-130-pm-hkt-tba"> <a href="#time-and-location-day-2-130-pm-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-2-130-pm-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 2, 1:30 PM HKT</a>, TBA</h4><h4 id="abstract-2"> <a href="#abstract-2" class="anchor-heading" aria-labelledby="abstract-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Equivariant representations are crucial in various scientific and engineering domains because they encode the inherent symmetries present in physical and biological systems, thereby providing a more natural and efficient way to model them. In the context of machine learning and perception, equivariant representations ensure that the output of a model changes in a predictable way in response to transformations of its input, such as 2D or 3D rotation or scaling. In this talk, we will show a systematic way of how to achieve equivariance by design and how such an approach can yield parsimony in training data and model capacity.</p><h4 id="bio-2"> <a href="#bio-2" class="anchor-heading" aria-labelledby="bio-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Kostas Daniilidis is the Ruth Yalom Stone Professor of Computer and Information Science at the University of Pennsylvania where he has been faculty since 1998. He is an IEEE Fellow. He was the director of the GRASP laboratory from 2008 to 2013, Associate Dean for Graduate Education from 2012-2016, and Faculty Director of Online Learning from 2013- 2017. He obtained his undergraduate degree in Electrical Engineering from the National Technical University of Athens, 1986, and his PhD (Dr.rer.nat.) in Computer Science from the University of Karlsruhe, 1992, under the supervision of Hans-Hellmut Nagel. He received the Best Conference Paper Award at ICRA 2017. He co-chaired ECCV 2010 and 3DPVT 2006. His most cited works have been on event-based vision, equivariant learning, 3D human pose, and hand-eye calibration.</p><h3 id="maryam-fazel"> <a href="#maryam-fazel" class="anchor-heading" aria-labelledby="maryam-fazel"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://people.ece.uw.edu/fazel_maryam/">Maryam Fazel</a></h3><p>University of Washington</p><h4 id="title-flat-minima-and-generalization-in-learning-the-case-of-low-rank-matrix-recovery"> <a href="#title-flat-minima-and-generalization-in-learning-the-case-of-low-rank-matrix-recovery" class="anchor-heading" aria-labelledby="title-flat-minima-and-generalization-in-learning-the-case-of-low-rank-matrix-recovery"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Flat Minima and Generalization in Learning: The Case of Low-rank Matrix Recovery</h4><h4 id="time-and-location-day-1-130-pm-hkt-tba"> <a href="#time-and-location-day-1-130-pm-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-1-130-pm-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 1, 1:30 PM HKT</a>, TBA</h4><h4 id="abstract-3"> <a href="#abstract-3" class="anchor-heading" aria-labelledby="abstract-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Many behaviors observed in deep neural networks still lack satisfactory explanation; e.g., how does an overparameterized neural network avoid overfitting and generalize to unseen data? Empirical evidence suggests that generalization depends on which zero-loss local minimum is attained during training. The shape of the training loss around a local minimum affects the model’s performance: “Flat” minima—around which the loss grows slowly—appear to generalize well. Clarifying this phenomenon helps explain generalization properties, which still largely remain a mystery.<br /> In this talk we focus on a simple class of overparameterized nonlinear models, those arising in low-rank matrix recovery. We study several key models: matrix sensing, phase retrieval, robust Principal Component Analysis, covariance matrix estimation, and single hidden layer neural networks with quadratic activation. We prove that in these models, flat minima (measured by average curvature) exactly recover the ground truth under standard statistical assumptions, and we prove weak recovery for matrix completion. These results suggest (i) a theoretical basis for favoring methods that bias iterates towards flat solutions, (ii) use of Hessian trace as a good regularizer. Since the landscape properties we prove are algorithm-agnostic, a future direction is to pair these findings with the analysis of common training algorithms to better understand the interplay between the loss landscape and algorithmic implicit bias.</p><h4 id="bio-3"> <a href="#bio-3" class="anchor-heading" aria-labelledby="bio-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Maryam Fazel is the Moorthy Family Professor of Electrical and Computer Engineering at the University of Washington, with adjunct appointments in Computer Science and Engineering, Mathematics, and Statistics. Maryam received her MS and PhD from Stanford University, her BS from Sharif University of Technology in Iran, and was a postdoctoral scholar at Caltech before joining UW. She is a recipient of the NSF Career Award, UWEE Outstanding Teaching Award, and UAI conference Best Student Paper Award with her student. She directs the Institute for Foundations of Data Science (IFDS), a multi-site NSF TRIPODS Institute. She serves on the Editorial board of the MOS-SIAM Book Series on Optimization, is an Associate Editor of the SIAM Journal on Mathematics of Data Science and an Action Editor of Journal of Machine Learning Research. Her current research interests are in the area of optimization in machine learning and control.</p><h3 id="tom-goldstein"> <a href="#tom-goldstein" class="anchor-heading" aria-labelledby="tom-goldstein"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a></h3><p>University of Maryland</p><h4 id="title-statistical-methods-for-addressing-safety-and-security-issues-of-generative-models"> <a href="#title-statistical-methods-for-addressing-safety-and-security-issues-of-generative-models" class="anchor-heading" aria-labelledby="title-statistical-methods-for-addressing-safety-and-security-issues-of-generative-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Statistical methods for addressing safety and security issues of generative models</h4><h4 id="time-and-location-day-4-900-am-hkt-tba"> <a href="#time-and-location-day-4-900-am-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-4-900-am-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 4, 9:00 AM HKT</a>, TBA</h4><h4 id="abstract-4"> <a href="#abstract-4" class="anchor-heading" aria-labelledby="abstract-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>This talk will have two parts. In the first part, I’ll talk about mathematical perspectives on how to watermark generative models to prevent parameter theft, ways to watermark generative model outputs to enable detection, and ways to perform post-hoc detection of language models without relying on watermarks. I’ll emphasize the important idea of using statistical hypothesis testing and p-values to provide rigorous control of the false-positive rate of detection. In the second part of the talk, I’ll present methods for constructing neural networks that exhibit “slow” thinking abilities akin to human logical reasoning. Rather than learning simple pattern matching rules, these networks have the ability to synthesize algorithmic reasoning processes and solve difficult discrete search and planning problems that cannot be solved by conventional AI systems. Interestingly, these reasoning systems naturally exhibit error correction and robustness properties that make them more difficult to break than their fast thinking counterparts.</p><h4 id="bio-4"> <a href="#bio-4" class="anchor-heading" aria-labelledby="bio-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Tom Goldstein is the Volpi-Cupal Professor of Computer Science at the University of Maryland, and director of the Maryland Center for Machine Learning. His research lies at the intersection of machine learning and optimization, and targets applications in computer vision and signal processing. Professor Goldstein has been the recipient of several awards, including SIAM’s DiPrima Prize, a DARPA Young Faculty Award, a JP Morgan Faculty award, an Amazon Research Award, and a Sloan Fellowship.</p><h3 id="yingbin-liang"> <a href="#yingbin-liang" class="anchor-heading" aria-labelledby="yingbin-liang"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.google.com/view/yingbinliang/home">Yingbin Liang</a></h3><p>Ohio State University</p><h4 id="title-in-context-convergence-of-transformers"> <a href="#title-in-context-convergence-of-transformers" class="anchor-heading" aria-labelledby="title-in-context-convergence-of-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: In-Context Convergence of Transformers</h4><h4 id="time-and-location-day-2-900-am-hkt-tba"> <a href="#time-and-location-day-2-900-am-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-2-900-am-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 2, 9:00 AM HKT</a>, TBA</h4><h4 id="abstract-5"> <a href="#abstract-5" class="anchor-heading" aria-labelledby="abstract-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Transformers have recently revolutionized many machine learning domains and one salient discovery is their remarkable in-context learning capability, where models can capture an unseen task by utilizing task-specific prompts without further parameters fine-tuning. In this talk, I will present our recent work that aims at understanding the in-context learning mechanism of transformers. Our focus is on the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. I will first present our characterization of the training convergence of in-context learning for data with balanced and imbalanced features, respectively. I will then discuss the insights that we obtain about attention models and training processes. I will also talk about the analysis techniques that we develop which may be useful for a broader set of problems. I will finally conclude my talk with comments on a few future directions.<br />This is a joint work with Yu Huang (UPenn) and Yuan Cheng (NUS).</p><h4 id="bio-5"> <a href="#bio-5" class="anchor-heading" aria-labelledby="bio-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Dr. Yingbin Liang is currently a Professor at the Department of Electrical and Computer Engineering at the Ohio State University (OSU), and a core faculty of the Ohio State Translational Data Analytics Institute (TDAI). She also serves as the Deputy Director of the AI-EDGE Institute at OSU. Dr. Liang received the Ph.D. degree in Electrical Engineering from the University of Illinois at Urbana-Champaign in 2005, and served on the faculty of University of Hawaii and Syracuse University before she joined OSU. Dr. Liang’s research interests include machine learning, optimization, information theory, and statistical signal processing. Dr. Liang received the National Science Foundation CAREER Award and the State of Hawaii Governor Innovation Award in 2009. She also received EURASIP Best Paper Award in 2014. She is an IEEE fellow.</p><h3 id="dimitris-papailiopoulos"> <a href="#dimitris-papailiopoulos" class="anchor-heading" aria-labelledby="dimitris-papailiopoulos"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://papail.io/">Dimitris Papailiopoulos</a></h3><p>University of Wisconsin-Madison</p><h4 id="title-teaching-arithmetic-to-small-language-models"> <a href="#title-teaching-arithmetic-to-small-language-models" class="anchor-heading" aria-labelledby="title-teaching-arithmetic-to-small-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Teaching arithmetic to small language models</h4><h4 id="time-and-location-day-3-130-pm-hkt-tba"> <a href="#time-and-location-day-3-130-pm-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-3-130-pm-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 3, 1:30 PM HKT</a>, TBA</h4><h4 id="abstract-6"> <a href="#abstract-6" class="anchor-heading" aria-labelledby="abstract-6"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Can a language model truly “understand” arithmetic? We explore this by trying to teach small transformers from scratch to perform elementary arithmetic operations, using the next-token prediction objective. We first demonstrate that conventional training data (i.e., “A+B=C”) is not effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions which, in some cases, can be explained through connections to low-rank matrix completion. We then train these small models on chain-of-thought data that includes intermediate steps. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We finally discuss the issue of length generalization: can a model trained on n digits add n+1 digit numbers? Humans don’t need to be taught every digit length of addition to be able to perform it. It turns out that language models aren’t great at length generalization, but we catch glimpses of it in “unstable” scenarios. Surprisingly, the infamous U-shaped overfitting curve makes an appearance!</p><h4 id="bio-6"> <a href="#bio-6" class="anchor-heading" aria-labelledby="bio-6"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Dimitris Papailiopoulos is the Jay &amp; Cynthia Ihlenfeld Associate Professor of Electrical and Computer Engineering at the University of Wisconsin-Madison. His research interests span machine learning, information theory, and distributed systems, with a current focus on understanding the intricacies of large-language models. Before coming to Madison, Dimitris was a postdoctoral researcher at UC Berkeley and a member of the AMPLab. He earned his Ph.D. in ECE from UT Austin, under the supervision of Alex Dimakis. He received his ECE Diploma M.Sc. degree from the Technical University of Crete, in Greece. Dimitris is a recipient of the NSF CAREER Award (2019), three years of Sony Faculty Innovation Awards (2018, 2019 and 2020), a joint IEEE ComSoc/ITSoc Best Paper Award (2020), an IEEE Signal Processing Society, Young Author Best Paper Award (2015), the Vilas Associate Award (2021), the Emil Steiger Distinguished Teaching Award (2021), and the Benjamin Smith Reynolds Award for Excellence in Teaching (2019). In 2018, he co-founded MLSys, a new conference that targets research at the intersection of machine learning and systems.</p><h3 id="stefano-soatto"> <a href="#stefano-soatto" class="anchor-heading" aria-labelledby="stefano-soatto"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://web.cs.ucla.edu/~soatto/">Stefano Soatto</a></h3><p>University of California, Los Angeles</p><h4 id="title-representation-and-control-of-meanings-in-large-language-models-and-multimodal-foundation-models"> <a href="#title-representation-and-control-of-meanings-in-large-language-models-and-multimodal-foundation-models" class="anchor-heading" aria-labelledby="title-representation-and-control-of-meanings-in-large-language-models-and-multimodal-foundation-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Representation and Control of Meanings in Large Language Models and Multimodal Foundation Models</h4><h4 id="time-and-location-day-1-1000-am-hkt-tba"> <a href="#time-and-location-day-1-1000-am-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-1-1000-am-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 1, 10:00 AM HKT</a>, TBA</h4><h4 id="abstract-7"> <a href="#abstract-7" class="anchor-heading" aria-labelledby="abstract-7"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Large Language Models and Multimodal Foundation Models, despite the simple predictive learning criterion and absence of explicit complexity bias, have shown the ability to capture the structure and “meaning” of data. I will introduce a notion of “meaning” for large language models as equivalence classes of sentences, and describe methods to establish a geometry and topology in the space of meanings, as well as an algebra so meanings can be composed and asymmetric relations such as entailment and implication can be quantified. Meanings as equivalence classes of sentences determined by the trained embedings can be defined, computed and quantified for pre-trained models, without the need for instruction tuning, reinforcement learning, or prompt engineering. Meanings as trajectories can be shown to align with human assessment through manually annotated benchmarks and can, as the outputs of dynamical systems, be controlled. I will show illustrative examples using both text and imaging modalities.</p><h4 id="bio-7"> <a href="#bio-7" class="anchor-heading" aria-labelledby="bio-7"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Professor Soatto received his Ph.D. in Control and Dynamical Systems from the California Institute of Technology in 1996; he joined UCLA in 2000 after being Assistant and then Associate Professor of Electrical and Biomedical Engineering at Washington University, and Research Associate in Applied Sciences at Harvard University. Between 1995 and 1998 he was also Ricercatore in the Department of Mathematics and Computer Science at the University of Udine - Italy. He received his D.Ing. degree (highest honors) from the University of Padova- Italy in 1992. His general research interests are in Computer Vision and Nonlinear Estimation and Control Theory. In particular, he is interested in ways for computers to use sensory information (e.g. vision, sound, touch) to interact with humans and the environment. Dr. Soatto is the recipient of the David Marr Prize (with Y. Ma, J. Kosecka and S. Sastry of U.C. Berkeley) for work on Euclidean reconstruction and reprojection up to subgroups. He also received the Siemens Prize with the Outstanding Paper Award from the IEEE Computer Society for his work on optimal structure from motion (with R. Brockett of Harvard). He received the National Science Foundation Career Award and the Okawa Foundation Grant. He is Associate Editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) and a Member of the Editorial Board of the International Journal of Computer Vision (IJCV) and Foundations and Trends in Computer Graphics and Vision.</p><h3 id="jong-chul-ye"> <a href="#jong-chul-ye" class="anchor-heading" aria-labelledby="jong-chul-ye"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://bispl.weebly.com/professor.html">Jong Chul Ye</a></h3><p>Korea Advanced Institute of Science and Technology (KAIST)</p><h4 id="title-enlarging-the-capability-of-diffusion-inverse-solvers-by-guidance"> <a href="#title-enlarging-the-capability-of-diffusion-inverse-solvers-by-guidance" class="anchor-heading" aria-labelledby="title-enlarging-the-capability-of-diffusion-inverse-solvers-by-guidance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Enlarging the Capability of Diffusion Inverse Solvers by Guidance</h4><h4 id="time-and-location-day-3-900-am-hkt-tba"> <a href="#time-and-location-day-3-900-am-hkt-tba" class="anchor-heading" aria-labelledby="time-and-location-day-3-900-am-hkt-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time and Location: <a href="/program_schedule/">Day 3, 9:00 AM HKT</a>, TBA</h4><h4 id="abstract-8"> <a href="#abstract-8" class="anchor-heading" aria-labelledby="abstract-8"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>The recent advent of diffusion models has led to significant progress in solving inverse problems, leveraging these models as effective generative priors. Nonetheless, challenges related to the ill-posed nature of such problems remain, such as 3D extension and overcoming inherent ambiguities in measurements. In this talk, we introduce strategies to address these issues. First, to enable 3D extension using only 2D diffusion models, we propose a novel approach using two perpendicular pre-trained 2D diffusion models which guides each solver to solve the 3D inverse problem. Specifically, by modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality from the image guidance from the perpendicular direction. Second, drawing inspiration from the human ability to resolve visual ambiguities through perceptual biases, we introduce a novel latent diffusion inverse solver by incorporating guidance by text prompts. Specifically, our method applies the textual description of the preconception of the solution during the reverse sampling phase, of which description is dynamically reinforced through null-text optimization for adaptive negation. Our comprehensive experimental results show that our method successfully mitigates ambiguity in latent diffusion inverse solvers, enhancing their effectiveness and accuracy.</p><h4 id="bio-8"> <a href="#bio-8" class="anchor-heading" aria-labelledby="bio-8"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bio</h4><p>Jong Chul Ye is a Professor at the Kim Jaechul Graduate School of Artificial Intelligence (AI) of Korea Advanced Institute of Science and Technology (KAIST), Korea. He received his B.Sc. and M.Sc. degrees from Seoul National University, Korea, and his PhD from Purdue University. Before joining KAIST, he worked at Philips Research and GE Global Research in New York. He has served as an associate editor of IEEE Trans. on Image Processing and an editorial board member for Magnetic Resonance in Medicine. He is currently an associate editor for IEEE Trans. on Medical Imaging and a Senior Editor of IEEE Signal Processing Magazine. He is an IEEE Fellow, was the Chair of IEEE SPS Computational Imaging TC, and IEEE EMBS Distinguished Lecturer. He was a General co-chair (with Mathews Jacob) for IEEE Symposium on Biomedical Imaging (ISBI) 2020. His research interest is in machine learning for biomedical imaging and computer vision.</p></main></div></div><div class="search-overlay"></div></div>
