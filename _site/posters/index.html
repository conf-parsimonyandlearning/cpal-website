<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Poster Presentations | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Poster Presentations" /><meta property="og:locale" content="en_US" /><meta name="description" content="Scheduling information for oral presentations at CPAL" /><meta property="og:description" content="Scheduling information for oral presentations at CPAL" /><link rel="canonical" href="https://cpal.cc/posters/" /><meta property="og:url" content="https://cpal.cc/posters/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Poster Presentations" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Scheduling information for oral presentations at CPAL","headline":"Poster Presentations","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/posters/"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script> <script defer src="/assets/js/mathtex-script-type.js"> </script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body, { globalGroup: true, trust: true, strict: false, throwOnError: false, });"></script><style> .katex { font-size: 1em; }</style><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/orals/" class="nav-list-link">Oral Presentations</a><li class="nav-list-item active"> <a href="/posters/" class="nav-list-link active">Poster Presentations</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="false"> Tutorials </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tutorial_info/" class="nav-list-link">List of Tutorials</a><li class="nav-list-item "> <a href="/tutorial_call/" class="nav-list-link">Call for Tutorials</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Call for Applications</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> Conference Sponsors </button><ul class="nav-list"><li class="nav-list-item "> <a href="/sponsors/" class="nav-list-link">Sponsors</a></ul><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a><li class="nav-list-item"> <a href="/other_years/" class="nav-list-link">Past CPAL Websites</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://cvent.me/X5aaar" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2025" class="site-button" > CPAL OpenReview </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/conference_program/">Conference Program</a><li class="breadcrumb-nav-list-item"><span>Poster Presentations</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/stanford.jpg" alt="Splash photo of Stanford" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> March 2025,&nbsp;Stanford</div></div><h1 class="no_toc" id="poster-sessions-at-cpal-2025"> <a href="#poster-sessions-at-cpal-2025" class="anchor-heading" aria-labelledby="poster-sessions-at-cpal-2025"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Poster Sessions at CPAL 2025</h1><h2 class="no_toc" id="presentation-format"> <a href="#presentation-format" class="anchor-heading" aria-labelledby="presentation-format"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presentation Format</h2><p>All accepted papers at CPAL 2025, from both the Proceedings and Spotlight tracks, will be presented as posters at the conference. A select number of Proceedings track papers are also presented as orals, as specified on the <a href="/orals">orals page</a>.</p><p>See the <a href="/program_schedule/">full program</a> for an aggregated view of the precise times and locations of each poster and oral session.</p><h2 class="no_toc" id="logistical-information"> <a href="#logistical-information" class="anchor-heading" aria-labelledby="logistical-information"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Logistical Information</h2><p>Posters should be printed in <strong><em>A0 size</em></strong>, with vertical orientation preferred.</p><h2 class="no_toc" id="quick-links"> <a href="#quick-links" class="anchor-heading" aria-labelledby="quick-links"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Quick Links</h2><ol id="markdown-toc"><li><a href="#reception--poster-session-1" id="markdown-toc-reception--poster-session-1">Reception + Poster Session 1</a><ol><li><a href="#time-day-2-mar-25--tuesday--445-pm-to-615-pm" id="markdown-toc-time-day-2-mar-25--tuesday--445-pm-to-615-pm">Time: Day 2 (Mar 25) – Tuesday – 4:45 PM to 6:15 PM</a></ol><li><a href="#poster-session-2" id="markdown-toc-poster-session-2">Poster Session 2</a><ol><li><a href="#time-day-3-mar-26--wednesday--445-pm-to-615-pm" id="markdown-toc-time-day-3-mar-26--wednesday--445-pm-to-615-pm">Time: Day 3 (Mar 26) – Wednesday – 4:45 PM to 6:15 PM</a></ol><li><a href="#coffee-break--poster-session-3" id="markdown-toc-coffee-break--poster-session-3">Coffee Break + Poster Session 3</a><ol><li><a href="#time-day-4-mar-27--thursday--1100-am-to-1230-pm" id="markdown-toc-time-day-4-mar-27--thursday--1100-am-to-1230-pm">Time: Day 4 (Mar 27) – Thursday – 11:00 AM to 12:30 PM</a></ol></ol><h2 id="reception--poster-session-1"> <a href="#reception--poster-session-1" class="anchor-heading" aria-labelledby="reception--poster-session-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Reception + Poster Session 1</h2><h4 id="time-day-2-mar-25--tuesday--445-pm-to-615-pm"> <a href="#time-day-2-mar-25--tuesday--445-pm-to-615-pm" class="anchor-heading" aria-labelledby="time-day-2-mar-25--tuesday--445-pm-to-615-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 2 (Mar 25) – Tuesday – 4:45 PM to 6:15 PM</a></h4><h3 class="no_toc" id="1-progressive-gradient-flow-for-robust-nm-sparsity-training-in-transformers"> <a href="#1-progressive-gradient-flow-for-robust-nm-sparsity-training-in-transformers" class="anchor-heading" aria-labelledby="1-progressive-gradient-flow-for-robust-nm-sparsity-training-in-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=HJjauwys0B">Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers</a></h3><p>Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, Tushar Krishna</p><p class="fs-2">Keywords: <em>N:M structured sparsity, sparsity, model compression, attention-based models, sparse training recipe</em></p><h3 class="no_toc" id="2-sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning"> <a href="#2-sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning" class="anchor-heading" aria-labelledby="2-sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=IFfyezixou">Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning Issues in Multi-Modal Multi-Task Learning</a></h3><p>Jie Peng, Sukwon Yun, Kaixiong Zhou, Ruida Zhou, Thomas Hartvigsen, Yanyong Zhang, Zhangyang Wang, Tianlong Chen</p><p class="fs-2">Keywords: <em>transformer, sparse mixture-of-experts, multi-modal learning, multi-task learning</em></p><h3 class="no_toc" id="3-theoretical-and-empirical-advances-in-forest-pruning"> <a href="#3-theoretical-and-empirical-advances-in-forest-pruning" class="anchor-heading" aria-labelledby="3-theoretical-and-empirical-advances-in-forest-pruning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=eVAjcRE8tx">Theoretical and Empirical Advances in Forest Pruning</a></h3><p>Albert Dorador</p><p class="fs-2">Keywords: <em>Regression, Decision Trees, Ensemble Learning, Pruning, Interpretable Machine Learning</em></p><h3 class="no_toc" id="4-on-how-iterative-magnitude-pruning-discovers-local-receptive-fields-in-fully-connected-neural-networks"> <a href="#4-on-how-iterative-magnitude-pruning-discovers-local-receptive-fields-in-fully-connected-neural-networks" class="anchor-heading" aria-labelledby="4-on-how-iterative-magnitude-pruning-discovers-local-receptive-fields-in-fully-connected-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=B936pXBrz5">On How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully Connected Neural Networks</a></h3><p>William T Redman, Zhangyang Wang, Alessandro Ingrosso, Sebastian Goldt</p><p class="fs-2">Keywords: <em>iterative magnitude pruning, lottery tickets, sparse machine learning, gaussian statistics</em></p><h3 class="no_toc" id="5-dimension-mixer-group-mixing-of-input-dimensions-for-efficient-function-approximation"> <a href="#5-dimension-mixer-group-mixing-of-input-dimensions-for-efficient-function-approximation" class="anchor-heading" aria-labelledby="5-dimension-mixer-group-mixing-of-input-dimensions-for-efficient-function-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=nJ4fuCF5aX">Dimension Mixer: Group Mixing of Input Dimensions for Efficient Function Approximation</a></h3><p>Suman Sapkota, Binod Bhattarai</p><p class="fs-2">Keywords: <em>Sparse Architectures, Structured Sparsity, Butterfly Sparsity, Butterfly MLP, Butterfly Attention, Long Range Arena (LRA), Solving Pathfinder-X, Patch Only MLP-Mixer, Dimension Mixer</em></p><h3 class="no_toc" id="6-hsr-enhanced-sparse-attention-acceleration"> <a href="#6-hsr-enhanced-sparse-attention-acceleration" class="anchor-heading" aria-labelledby="6-hsr-enhanced-sparse-attention-acceleration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 6. <a href="https://openreview.net/forum?id=wso1gABiPZ">HSR-Enhanced Sparse Attention Acceleration</a></h3><p>Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</p><p class="fs-2">Keywords: <em>Half-Space Reporting, Attention Acceleration, Sparse Attention</em></p><h3 class="no_toc" id="7-taming-sensitive-weights--noise-perturbation-fine-tuning-for-robust-llm-quantization"> <a href="#7-taming-sensitive-weights--noise-perturbation-fine-tuning-for-robust-llm-quantization" class="anchor-heading" aria-labelledby="7-taming-sensitive-weights--noise-perturbation-fine-tuning-for-robust-llm-quantization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 7. <a href="https://openreview.net/forum?id=VehapTAftQ">Taming Sensitive Weights : Noise Perturbation Fine-tuning for Robust LLM Quantization</a></h3><p>DONGWEI WANG, Huanrui Yang</p><p class="fs-2">Keywords: <em>LLM quantization, Hessian trace, Noise-aware finetuning</em></p><h3 class="no_toc" id="8-greedy-output-approximation-towards-efficient-structured-pruning-for-llms-without-retraining"> <a href="#8-greedy-output-approximation-towards-efficient-structured-pruning-for-llms-without-retraining" class="anchor-heading" aria-labelledby="8-greedy-output-approximation-towards-efficient-structured-pruning-for-llms-without-retraining"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 8. <a href="https://openreview.net/forum?id=hp7txxx8hv">Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining</a></h3><p>Jianwei Li, Yijun Dong, Qi Lei</p><p class="fs-2">Keywords: <em>Efficient, Structured Pruning, LLMs</em></p><h3 class="no_toc" id="9-q-galore-quantized-galore-with-int4-projection-and-layer-adaptive-low-rank-gradients"> <a href="#9-q-galore-quantized-galore-with-int4-projection-and-layer-adaptive-low-rank-gradients" class="anchor-heading" aria-labelledby="9-q-galore-quantized-galore-with-int4-projection-and-layer-adaptive-low-rank-gradients"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 9. <a href="https://openreview.net/forum?id=KTAPk6c2hl">Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients</a></h3><p>Zhenyu Zhang, AJAY KUMAR JAISWAL, Lu Yin, Shiwei Liu, Jiawei Zhao, Yuandong Tian, Zhangyang Wang</p><p class="fs-2">Keywords: <em>Large Language Models; Memory Efficient Training; Low Rank</em></p><h3 class="no_toc" id="10-adaptive-batch-size-schedules-for-distributed-training-of-language-models-with-data-and-model-parallelism"> <a href="#10-adaptive-batch-size-schedules-for-distributed-training-of-language-models-with-data-and-model-parallelism" class="anchor-heading" aria-labelledby="10-adaptive-batch-size-schedules-for-distributed-training-of-language-models-with-data-and-model-parallelism"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 10. <a href="https://openreview.net/forum?id=pi2TiX7er9">Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism</a></h3><p>Tim Tsz-Kit Lau, Weijian Li, Chenwei Xu, Han Liu, Mladen Kolar</p><p class="fs-2">Keywords: <em>Distributed training, adaptive batch size, data parallelism, model parallelism</em></p><h3 class="no_toc" id="11-a-unified-framework-for-sparse-plus-low-rank-matrix-decomposition-for-llms"> <a href="#11-a-unified-framework-for-sparse-plus-low-rank-matrix-decomposition-for-llms" class="anchor-heading" aria-labelledby="11-a-unified-framework-for-sparse-plus-low-rank-matrix-decomposition-for-llms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 11. <a href="https://openreview.net/forum?id=hyN75SAJTI">A unified framework for Sparse plus Low-Rank Matrix Decomposition for LLMs</a></h3><p>Mehdi Makni, Kayhan Behdin, Zheng Xu, Natalia Ponomareva, Rahul Mazumder</p><p class="fs-2">Keywords: <em>model compression, sparse plus low-rank, optimization, inference acceleration, 2:4 sparsity, hardware and system co-design</em></p><h3 class="no_toc" id="12-unlock-the-theory-behind-scaling-1-bit-neural-networks"> <a href="#12-unlock-the-theory-behind-scaling-1-bit-neural-networks" class="anchor-heading" aria-labelledby="12-unlock-the-theory-behind-scaling-1-bit-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 12. <a href="https://openreview.net/forum?id=fcpRnXWvWk">Unlock the Theory behind Scaling 1-bit Neural Networks</a></h3><p>Majid Daliri, Zhao Song, Chiwun Yang</p><p class="fs-2">Keywords: <em>1-bit neural network, neural tangent kernel, scaling law theory</em></p><h3 class="no_toc" id="13-adversarially-robust-spiking-neural-networks-with-sparse-connectivity"> <a href="#13-adversarially-robust-spiking-neural-networks-with-sparse-connectivity" class="anchor-heading" aria-labelledby="13-adversarially-robust-spiking-neural-networks-with-sparse-connectivity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 13. <a href="https://openreview.net/forum?id=VhCOSdgFl2">Adversarially Robust Spiking Neural Networks with Sparse Connectivity</a></h3><p>Mathias Schmolli, Maximilian Baronig, Robert Legenstein, Ozan Ozdenizci</p><p class="fs-2">Keywords: <em>adversarial robustness, spiking neural networks, ANN-to-SNN conversion, sparsity, robust pruning</em></p><h3 class="no_toc" id="14-sgd-with-weight-decay-secretly-minimizes-the-ranks-of-your-neural-networks"> <a href="#14-sgd-with-weight-decay-secretly-minimizes-the-ranks-of-your-neural-networks" class="anchor-heading" aria-labelledby="14-sgd-with-weight-decay-secretly-minimizes-the-ranks-of-your-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 14. <a href="https://openreview.net/forum?id=0LzE9AROwD">SGD with Weight Decay Secretly Minimizes the Ranks of Your Neural Networks</a></h3><p>Tomer Galanti, Zachary S Siegel, Aparna Gupte, Tomaso A Poggio</p><p class="fs-2">Keywords: <em>Low-Rank, SGD, Implicit Bias, Rank, Rank Minimization, Weight Decay</em></p><h3 class="no_toc" id="15-sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry"> <a href="#15-sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry" class="anchor-heading" aria-labelledby="15-sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 15. <a href="https://openreview.net/forum?id=SXxu6BxEGD">Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry</a></h3><p>Mohammed Adnan, Rohan Jain, Ekansh Sharma, Yani Ioannou</p><p class="fs-2">Keywords: <em>Lottery Ticket Hypothesis, sparse training, linear mode connectivity, weight symmetry, deep learning, deep neural networks, random initialization, git re-basin, optimization</em></p><h3 class="no_toc" id="16-mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models"> <a href="#16-mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models" class="anchor-heading" aria-labelledby="16-mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 16. <a href="https://openreview.net/forum?id=OutjGuJnNk">Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models</a></h3><p>Weixin Liang, LILI YU, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin</p><p class="fs-2">Keywords: <em>Sparse architecture, Efficient deep architecture, Multi-modal foundation models, Mixture-of-Experts, Transformer</em></p><h3 class="no_toc" id="17-mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity"> <a href="#17-mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity" class="anchor-heading" aria-labelledby="17-mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 17. <a href="https://openreview.net/forum?id=i1mxsDrzv3">Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity</a></h3><p>Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, LILI YU</p><p class="fs-2">Keywords: <em>Sparse architecture, Efficient deep architecture, Multi-modal foundation models, Mixture-of-Experts, State Space Model</em></p><h3 class="no_toc" id="18-training-bayesian-neural-networks-with-sparse-subspace-variational-inference"> <a href="#18-training-bayesian-neural-networks-with-sparse-subspace-variational-inference" class="anchor-heading" aria-labelledby="18-training-bayesian-neural-networks-with-sparse-subspace-variational-inference"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 18. <a href="https://openreview.net/forum?id=jMbvSOtpQD">Training Bayesian Neural Networks with Sparse Subspace Variational Inference</a></h3><p>Junbo Li, Zichen Miao, Qiang Qiu, Ruqi Zhang</p><p class="fs-2">Keywords: <em>Bayesian neural networks, sparse Bayesian learning, variational inference</em></p><h3 class="no_toc" id="19-wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models"> <a href="#19-wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models" class="anchor-heading" aria-labelledby="19-wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 19. <a href="https://openreview.net/forum?id=Vuak9eodET">WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</a></h3><p>Jinghan Jia, Jiancheng Liu, Yihua Zhang, Parikshit Ram, Nathalie Baracaldo, Sijia Liu</p><p class="fs-2">Keywords: <em>Machine Unlearning, LLMs</em></p><h3 class="no_toc" id="20-masks-signs-and-learning-rate-rewinding"> <a href="#20-masks-signs-and-learning-rate-rewinding" class="anchor-heading" aria-labelledby="20-masks-signs-and-learning-rate-rewinding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 20. <a href="https://openreview.net/forum?id=W7Q5q3DEKZ">Masks, Signs, And Learning Rate Rewinding</a></h3><p>Advait Gadhikar, Rebekka Burkholz</p><p class="fs-2">Keywords: <em>sparsity, pruning, lottery tickets, learning rate rewinding, iterative magnitude pruning</em></p><h3 class="no_toc" id="21-streaming-kernel-pca-algorithm-with-small-space"> <a href="#21-streaming-kernel-pca-algorithm-with-small-space" class="anchor-heading" aria-labelledby="21-streaming-kernel-pca-algorithm-with-small-space"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 21. <a href="https://openreview.net/forum?id=Gfl6APFUri">Streaming Kernel PCA Algorithm With Small Space</a></h3><p>Yichuan Deng, Jiangxuan Long, Zhao Song, Zifan Wang, Han Zhang</p><p class="fs-2">Keywords: <em>Principal Component Analysis, Kernel Method, Streaming Algorithm</em></p><h3 class="no_toc" id="22-collaborative-and-efficient-personalization-with-mixtures-of-adaptors"> <a href="#22-collaborative-and-efficient-personalization-with-mixtures-of-adaptors" class="anchor-heading" aria-labelledby="22-collaborative-and-efficient-personalization-with-mixtures-of-adaptors"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 22. <a href="https://openreview.net/forum?id=3J6AXM2HfN">Collaborative and Efficient Personalization with Mixtures of Adaptors</a></h3><p>Abdulla Jasem Almansoori, Samuel Horváth, Martin Takáč</p><p class="fs-2">Keywords: <em>federated learning, personalization, multi-task learning, clustering, parameter-efficient</em></p><h3 class="no_toc" id="23-pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge"> <a href="#23-pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge" class="anchor-heading" aria-labelledby="23-pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 23. <a href="https://openreview.net/forum?id=N03O7qEPWI">Pruning neural network models for gene regulatory dynamics using data and domain knowledge</a></h3><p>Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush</p><p class="fs-2">Keywords: <em>sparsification, pruning, lottery tickets, explainability, gene regulation, domain knowledge, neural architecture design, NeuralODEs</em></p><h3 class="no_toc" id="24-towards-vector-optimization-on-low-dimensional-vector-symbolic-architecture"> <a href="#24-towards-vector-optimization-on-low-dimensional-vector-symbolic-architecture" class="anchor-heading" aria-labelledby="24-towards-vector-optimization-on-low-dimensional-vector-symbolic-architecture"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 24. <a href="https://openreview.net/forum?id=08PRND19BY">Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture</a></h3><p>Shijin Duan, Yejia Liu, Gaowen Liu, Ramana Rao Kompella, Shaolei Ren, Xiaolin Xu</p><p class="fs-2">Keywords: <em>Vector Symbolic Architecture, Batch Normalization, Knowledge Distillation</em></p><h3 class="no_toc" id="25-mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln"> <a href="#25-mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln" class="anchor-heading" aria-labelledby="25-mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 25. <a href="https://openreview.net/forum?id=6Urduk1GNx">Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN</a></h3><p>Pengxiang Li, Lu Yin, Shiwei Liu</p><p class="fs-2">Keywords: <em>LayerNorm, LLM, Transformer</em></p><h3 class="no_toc" id="26-approximate-nullspace-augmented-finetuning-for-robust-vision-transformers"> <a href="#26-approximate-nullspace-augmented-finetuning-for-robust-vision-transformers" class="anchor-heading" aria-labelledby="26-approximate-nullspace-augmented-finetuning-for-robust-vision-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 26. <a href="https://openreview.net/forum?id=zH3Zwx3dLQ">Approximate Nullspace Augmented Finetuning for Robust Vision Transformers</a></h3><p>Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang</p><p class="fs-2">Keywords: <em>Robustness, Vision Transformer, Invariance</em></p><h3 class="no_toc" id="27-learning-of-patch-based-smooth-plus-sparse-models-for-image-reconstruction"> <a href="#27-learning-of-patch-based-smooth-plus-sparse-models-for-image-reconstruction" class="anchor-heading" aria-labelledby="27-learning-of-patch-based-smooth-plus-sparse-models-for-image-reconstruction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 27. <a href="https://openreview.net/forum?id=xJpLu0Dicu">Learning of Patch-Based Smooth-Plus-Sparse Models for Image Reconstruction</a></h3><p>Stanislas Ducotterd, Sebastian Neumayer, Michael Unser</p><p class="fs-2">Keywords: <em>Image reconstruction, sparsity, dictionary learning, deep equilibrium</em></p><h3 class="no_toc" id="28-prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis"> <a href="#28-prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis" class="anchor-heading" aria-labelledby="28-prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 28. <a href="https://openreview.net/forum?id=gMI4dRErrA">Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis</a></h3><p>Shirin Shoushtari, Jiaming Liu, Edward P. Chandler, M. Salman Asif, Ulugbek S. Kamilov</p><p class="fs-2">Keywords: <em>Computational Imaging, Plug-and-Play Priors, Imaging Inverse Problems, Mismatched Priors, Domain Adaptation</em></p><h3 class="no_toc" id="29-stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes"> <a href="#29-stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes" class="anchor-heading" aria-labelledby="29-stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 29. <a href="https://openreview.net/forum?id=0q9E8QiEuu">Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes</a></h3><p>Dan Qiao, Kaiqi Zhang, Esha Singh, Daniel Soudry, Yu-Xiang Wang</p><p class="fs-2">Keywords: <em>Minima Stability, Edge-of-Stability, Generalization, Flat Local Minima, Curvature</em></p><h3 class="no_toc" id="30-certified-robustness-against-sparse-adversarial-perturbations-via-data-localization"> <a href="#30-certified-robustness-against-sparse-adversarial-perturbations-via-data-localization" class="anchor-heading" aria-labelledby="30-certified-robustness-against-sparse-adversarial-perturbations-via-data-localization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 30. <a href="https://openreview.net/forum?id=O7dYb0VAQw">Certified Robustness against Sparse Adversarial Perturbations via Data Localization</a></h3><p>Ambar Pal, Rene Vidal, Jeremias Sulam</p><p class="fs-2">Keywords: <em>Adversarial Robustness, Certified Robustness, Sparse Perturbations, Data Localization</em></p><h3 class="no_toc" id="31-the-computational-limits-of-state-space-models-and-mamba-via-the-lens-of-circuit-complexity"> <a href="#31-the-computational-limits-of-state-space-models-and-mamba-via-the-lens-of-circuit-complexity" class="anchor-heading" aria-labelledby="31-the-computational-limits-of-state-space-models-and-mamba-via-the-lens-of-circuit-complexity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 31. <a href="https://openreview.net/forum?id=bImlLT3r62">The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity</a></h3><p>Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song</p><p class="fs-2">Keywords: <em>State-Space Models, Mamba, Circuit Complexity, Computational Limits</em></p><h3 class="no_toc" id="32-fast-john-ellipsoid-computation-with-differential-privacy-optimization"> <a href="#32-fast-john-ellipsoid-computation-with-differential-privacy-optimization" class="anchor-heading" aria-labelledby="32-fast-john-ellipsoid-computation-with-differential-privacy-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 32. <a href="https://openreview.net/forum?id=yfmFSc5ZPG">Fast John Ellipsoid Computation with Differential Privacy Optimization</a></h3><p>Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu</p><p class="fs-2">Keywords: <em>Fast Optimization, Differential Privacy, John Ellipsoid Computation</em></p><h3 class="no_toc" id="33-understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data"> <a href="#33-understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data" class="anchor-heading" aria-labelledby="33-understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 33. <a href="https://openreview.net/forum?id=sACJw28GV4">Understanding How Nonlinear Networks Create Linearly Separable Features for Low-Dimensional Data</a></h3><p>Alec S Xu, Can Yaras, Peng Wang, Qing Qu</p><p class="fs-2">Keywords: <em>union of subspaces, shallow nonlinear networks, random feature model</em></p><h3 class="no_toc" id="34-on-generalization-bounds-for-neural-networks-with-low-rank-layers"> <a href="#34-on-generalization-bounds-for-neural-networks-with-low-rank-layers" class="anchor-heading" aria-labelledby="34-on-generalization-bounds-for-neural-networks-with-low-rank-layers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 34. <a href="https://openreview.net/forum?id=BoT0ecevAT">On Generalization Bounds for Neural Networks with Low Rank Layers</a></h3><p>Andrea Pinto, Akshay Rangamani, Tomaso A Poggio</p><p class="fs-2">Keywords: <em>Gaussian Complexity, Low Rank, Neural Collapse</em></p><h3 class="no_toc" id="35-fast-and-efficient-matching-algorithm-with-deadline-instances"> <a href="#35-fast-and-efficient-matching-algorithm-with-deadline-instances" class="anchor-heading" aria-labelledby="35-fast-and-efficient-matching-algorithm-with-deadline-instances"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 35. <a href="https://openreview.net/forum?id=TIneXGrWZt">Fast and Efficient Matching Algorithm with Deadline Instances</a></h3><p>Zhao Song, Weixin Wang, Chenbo Yin, Junze Yin</p><p class="fs-2">Keywords: <em>online weighted matching problem, sketching</em></p><h2 id="poster-session-2"> <a href="#poster-session-2" class="anchor-heading" aria-labelledby="poster-session-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Poster Session 2</h2><h4 id="time-day-3-mar-26--wednesday--445-pm-to-615-pm"> <a href="#time-day-3-mar-26--wednesday--445-pm-to-615-pm" class="anchor-heading" aria-labelledby="time-day-3-mar-26--wednesday--445-pm-to-615-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Mar 26) – Wednesday – 4:45 PM to 6:15 PM</a></h4><h3 class="no_toc" id="1-adaprox-a-novel-method-for-bilevel-optimization-under-pessimistic-framework"> <a href="#1-adaprox-a-novel-method-for-bilevel-optimization-under-pessimistic-framework" class="anchor-heading" aria-labelledby="1-adaprox-a-novel-method-for-bilevel-optimization-under-pessimistic-framework"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=wRMu5wPgQ7">AdaProx: A Novel Method for Bilevel Optimization under Pessimistic Framework</a></h3><p>Ziwei Guan, Daouda Sow, Sen Lin, Yingbin Liang</p><p class="fs-2">Keywords: <em>pessimistic bilevel optimization, convergence analysis, nonconvex, gradient-based method</em></p><h3 class="no_toc" id="2-revisiting-the-initial-steps-in-adaptive-gradient-descent-optimization"> <a href="#2-revisiting-the-initial-steps-in-adaptive-gradient-descent-optimization" class="anchor-heading" aria-labelledby="2-revisiting-the-initial-steps-in-adaptive-gradient-descent-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=pJVxTOUCkw">Revisiting the Initial Steps in Adaptive Gradient Descent Optimization</a></h3><p>Abulikemu Abuduweili, Changliu Liu</p><p class="fs-2">Keywords: <em>Optimization, Adam, Adaptive Gradient Decent, Neural Networks</em></p><h3 class="no_toc" id="3-exact-and-rich-feature-learning-dynamics-of-two-layer-linear-networks"> <a href="#3-exact-and-rich-feature-learning-dynamics-of-two-layer-linear-networks" class="anchor-heading" aria-labelledby="3-exact-and-rich-feature-learning-dynamics-of-two-layer-linear-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=J7dZuX0DGI">Exact and Rich Feature Learning Dynamics of Two-Layer Linear Networks</a></h3><p>Wei Huang, Wuyang Chen, zhiqiang xu, Zhangyang Wang, Taiji Suzuki</p><p class="fs-2">Keywords: <em>Neural networks dyanmics, Feature Learning, Optimization</em></p><h3 class="no_toc" id="4-hamiltonian-mechanics-of-feature-learning-bottleneck-structure-in-leaky-resnets"> <a href="#4-hamiltonian-mechanics-of-feature-learning-bottleneck-structure-in-leaky-resnets" class="anchor-heading" aria-labelledby="4-hamiltonian-mechanics-of-feature-learning-bottleneck-structure-in-leaky-resnets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=C8OsIiVcyC">Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets</a></h3><p>Arthur Jacot, Alexandre Kaiser</p><p class="fs-2">Keywords: <em>Low-rank bias, NeuralODE, Hamiltonian, Bottleneck structure</em></p><h3 class="no_toc" id="5-quantum-eigengame-for-excited-state-calculation"> <a href="#5-quantum-eigengame-for-excited-state-calculation" class="anchor-heading" aria-labelledby="5-quantum-eigengame-for-excited-state-calculation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=XFjwzut6c5">Quantum EigenGame for excited state calculation</a></h3><p>David A. Quiroga, Jason Han, Anastasios Kyrillidis</p><p class="fs-2">Keywords: <em>variational quantum algorithms, PCA, EigenGame, eigensolvers</em></p><h3 class="no_toc" id="6-asymptotic-behavior-of-the-coordinate-ascent-variational-inference-in-singular-models"> <a href="#6-asymptotic-behavior-of-the-coordinate-ascent-variational-inference-in-singular-models" class="anchor-heading" aria-labelledby="6-asymptotic-behavior-of-the-coordinate-ascent-variational-inference-in-singular-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 6. <a href="https://openreview.net/forum?id=e9Isd3GFDb">Asymptotic Behavior of the Coordinate Ascent Variational Inference in Singular Models</a></h3><p>Sean C Plummer, Anirban Bhattacharya, Debdeep Pati, Yun Yang</p><p class="fs-2">Keywords: <em>Coordinate Ascent Variational Inference, Singular Models, Dynmaical Systems</em></p><h3 class="no_toc" id="7-grouped-sequential-optimization-strategy---the-application-of-hyperparameter-importance-assessment-in-deep-learning"> <a href="#7-grouped-sequential-optimization-strategy---the-application-of-hyperparameter-importance-assessment-in-deep-learning" class="anchor-heading" aria-labelledby="7-grouped-sequential-optimization-strategy---the-application-of-hyperparameter-importance-assessment-in-deep-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 7. <a href="https://openreview.net/forum?id=YIhDWlfQsH">Grouped Sequential Optimization Strategy - the Application of Hyperparameter Importance Assessment in Deep Learning</a></h3><p>Ruinan Wang, Ian T. Nabney, MOHAMMAD GOLBABAEE</p><p class="fs-2">Keywords: <em>Optimization, Hyperparameter Optimization, Hyperparameter Importance Assessment, Model Efficiency, Search Space Exploration, Resource Allocation</em></p><h3 class="no_toc" id="8-provable-model-parallel-distributed-principal-component-analysis-with-parallel-deflation"> <a href="#8-provable-model-parallel-distributed-principal-component-analysis-with-parallel-deflation" class="anchor-heading" aria-labelledby="8-provable-model-parallel-distributed-principal-component-analysis-with-parallel-deflation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 8. <a href="https://openreview.net/forum?id=n3SpSUzGi0">Provable Model-Parallel Distributed Principal Component Analysis with Parallel Deflation</a></h3><p>Fangshuo Liao, Wenyi Su, Anastasios Kyrillidis</p><p class="fs-2">Keywords: <em>Principal Component Analysis, Distributed Learning</em></p><h3 class="no_toc" id="9-agenthpo-large-language-model-agent-for--hyper-parameter-optimization"> <a href="#9-agenthpo-large-language-model-agent-for--hyper-parameter-optimization" class="anchor-heading" aria-labelledby="9-agenthpo-large-language-model-agent-for--hyper-parameter-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 9. <a href="https://openreview.net/forum?id=HU3yfXcoKU">AgentHPO: Large Language Model Agent for Hyper-Parameter Optimization</a></h3><p>Siyi Liu, Chen Gao, Yong Li</p><p class="fs-2">Keywords: <em>Large Language Models, Agent, Hyperparameter Optimization</em></p><h3 class="no_toc" id="10-fedosaa-improving-federated-learning-with-one-step-anderson-acceleration"> <a href="#10-fedosaa-improving-federated-learning-with-one-step-anderson-acceleration" class="anchor-heading" aria-labelledby="10-fedosaa-improving-federated-learning-with-one-step-anderson-acceleration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 10. <a href="https://openreview.net/forum?id=OoYcaWhfwB">FedOSAA: Improving Federated Learning with One-Step Anderson Acceleration</a></h3><p>Xue Feng, M. Paul Laiu, Thomas Strohmer</p><p class="fs-2">Keywords: <em>federated learning, quasi-Newton methods, Anderson acceleration</em></p><h3 class="no_toc" id="11-unlocking-global-optimality-in-bilevel-optimization-a-pilot-study"> <a href="#11-unlocking-global-optimality-in-bilevel-optimization-a-pilot-study" class="anchor-heading" aria-labelledby="11-unlocking-global-optimality-in-bilevel-optimization-a-pilot-study"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 11. <a href="https://openreview.net/forum?id=ph1gNAlFoW">Unlocking Global Optimality in Bilevel Optimization: A Pilot Study</a></h3><p>Quan Xiao, Tianyi Chen</p><p class="fs-2">Keywords: <em>bilevel optimization; global convergence</em></p><h3 class="no_toc" id="12-on-the-crucial-role-of-initialization-for-matrix-factorization"> <a href="#12-on-the-crucial-role-of-initialization-for-matrix-factorization" class="anchor-heading" aria-labelledby="12-on-the-crucial-role-of-initialization-for-matrix-factorization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 12. <a href="https://openreview.net/forum?id=tCrEQpnilb">On the Crucial Role of Initialization for Matrix Factorization</a></h3><p>Bingcong Li, Liang Zhang, Aryan Mokhtari, Niao He</p><p class="fs-2">Keywords: <em>nonconvex optimization, initialization, quadratic rate, low rank adapter, lora</em></p><h3 class="no_toc" id="13-geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces"> <a href="#13-geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces" class="anchor-heading" aria-labelledby="13-geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 13. <a href="https://openreview.net/forum?id=aLGjGWqh8O">Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</a></h3><p>Saket Tiwari, Omer Gottesman, George Konidaris</p><p class="fs-2">Keywords: <em>resinforcement learning, continuous control, geometry</em></p><h3 class="no_toc" id="14-learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence"> <a href="#14-learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence" class="anchor-heading" aria-labelledby="14-learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 14. <a href="https://openreview.net/forum?id=AvZr0c0Kog">Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence</a></h3><p>Berfin Simsek, Amire Bendjeddou, Daniel Hsu</p><p class="fs-2">Keywords: <em>time complexity, gradient flow dynamics, hardness</em></p><h3 class="no_toc" id="15-learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability"> <a href="#15-learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability" class="anchor-heading" aria-labelledby="15-learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 15. <a href="https://openreview.net/forum?id=vc0SPDkozY">Learning Dynamics of Deep Matrix Factorization Beyond the Edge of Stability</a></h3><p>Avrajit Ghosh, Soo Min Kwon, Rongrong Wang, Saiprasad Ravishankar, Qing Qu</p><p class="fs-2">Keywords: <em>edge of stability, deep linear networks</em></p><h3 class="no_toc" id="16-non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity"> <a href="#16-non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity" class="anchor-heading" aria-labelledby="16-non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 16. <a href="https://openreview.net/forum?id=tFoVuC5vg1">Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</a></h3><p>Dominik Stöger, Yizhe Zhu</p><p class="fs-2">Keywords: <em>non-convex optimization, factorized gradient descent, matrix sensing, sample complexity, virtual sequences</em></p><h3 class="no_toc" id="17-relaxed-contrastive-learning-for-federated-learning"> <a href="#17-relaxed-contrastive-learning-for-federated-learning" class="anchor-heading" aria-labelledby="17-relaxed-contrastive-learning-for-federated-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 17. <a href="https://openreview.net/forum?id=hduCLXDhS4">Relaxed Contrastive Learning for Federated Learning</a></h3><p>Seonguk Seo, Jinkyu Kim, Geeho Kim, Bohyung Han</p><p class="fs-2">Keywords: <em>dimensional collapse, transferability, federated learning, local deviation</em></p><h3 class="no_toc" id="18-do-global-and-local-perform-cooperatively-or-adversarially-in-heterogeneous-federated-learning"> <a href="#18-do-global-and-local-perform-cooperatively-or-adversarially-in-heterogeneous-federated-learning" class="anchor-heading" aria-labelledby="18-do-global-and-local-perform-cooperatively-or-adversarially-in-heterogeneous-federated-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 18. <a href="https://openreview.net/forum?id=qOzt8VNQMx">Do Global and Local Perform Cooperatively or Adversarially in Heterogeneous Federated Learning?</a></h3><p>Huiwen Wu, Shuo Zhang</p><p class="fs-2">Keywords: <em>federated learning; multilevel optimization; learning dynamics</em></p><h3 class="no_toc" id="19-fedpews-personalized-warmup-via-subnetworks-for-enhanced-heterogeneous-federated-learning"> <a href="#19-fedpews-personalized-warmup-via-subnetworks-for-enhanced-heterogeneous-federated-learning" class="anchor-heading" aria-labelledby="19-fedpews-personalized-warmup-via-subnetworks-for-enhanced-heterogeneous-federated-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 19. <a href="https://openreview.net/forum?id=iYwiyS1YdQ">FedPeWS: Personalized Warmup via Subnetworks for Enhanced Heterogeneous Federated Learning</a></h3><p>Nurbek Tastan, Samuel Horváth, Martin Takáč, Karthik Nandakumar</p><p class="fs-2">Keywords: <em>federated learning, heterogeneous federated learning, personalized warmup, subnetworks</em></p><h3 class="no_toc" id="20-characterizing-resnets-universal-approximation-capability"> <a href="#20-characterizing-resnets-universal-approximation-capability" class="anchor-heading" aria-labelledby="20-characterizing-resnets-universal-approximation-capability"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 20. <a href="https://openreview.net/forum?id=x8fPISanbb">Characterizing ResNet’s Universal Approximation Capability</a></h3><p>Chenghao Liu, Enming Liang, Minghua Chen</p><p class="fs-2">Keywords: <em>universal approximation, ResNet, optimal approximation rate</em></p><h3 class="no_toc" id="21-a-validation-approach-to-over-parameterized-matrix-and-image-recovery"> <a href="#21-a-validation-approach-to-over-parameterized-matrix-and-image-recovery" class="anchor-heading" aria-labelledby="21-a-validation-approach-to-over-parameterized-matrix-and-image-recovery"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 21. <a href="https://openreview.net/forum?id=oyQnduevQw">A Validation Approach to Over-parameterized Matrix and Image Recovery</a></h3><p>Lijun Ding, Zhen Qin, Liwei Jiang, Jinxin Zhou, Zhihui Zhu</p><p class="fs-2">Keywords: <em>Matrix recovery, low-rank, validation, gradient descent, nonconvex optimization</em></p><h3 class="no_toc" id="22-whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity"> <a href="#22-whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity" class="anchor-heading" aria-labelledby="22-whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 22. <a href="https://openreview.net/forum?id=76H1lRBgup">WHOMP: Optimizing Randomized Controlled Trials via Wasserstein Homogeneity</a></h3><p>Shizhou Xu, Thomas Strohmer</p><p class="fs-2">Keywords: <em>randomized controlled trial, Wasserstein homogeneity, anti-clustering, diverse K-means, control/test group splitting, cross-validation</em></p><h3 class="no_toc" id="23-whats-in-a-prior-learned-proximal-networks-for-inverse-problems"> <a href="#23-whats-in-a-prior-learned-proximal-networks-for-inverse-problems" class="anchor-heading" aria-labelledby="23-whats-in-a-prior-learned-proximal-networks-for-inverse-problems"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 23. <a href="https://openreview.net/forum?id=MGiamJl0YD">What’s in a Prior? Learned Proximal Networks for Inverse Problems</a></h3><p>Zhenghan Fang, Sam Buchanan, Jeremias Sulam</p><p class="fs-2">Keywords: <em>Inverse problems, Proximal operators, Plug-and-play, Explicit regularizer, Convergent PnP, Input convex neural networks</em></p><h3 class="no_toc" id="24-provable-probabilistic-imaging-using-score-based-generative-priors"> <a href="#24-provable-probabilistic-imaging-using-score-based-generative-priors" class="anchor-heading" aria-labelledby="24-provable-probabilistic-imaging-using-score-based-generative-priors"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 24. <a href="https://openreview.net/forum?id=PTdUT7biBO">Provable Probabilistic Imaging using Score-based Generative Priors</a></h3><p>Yu Sun, Zihui Wu, Yifan Chen, Berthy Feng, Katherine Bouman</p><p class="fs-2">Keywords: <em>Diffusion models, inverse problem, image reconstruction, langevin dynamics, markov processes, plug-and-play priors, posterior sampling, regularized inversion, score-based generative models, uncertainty quantification</em></p><h3 class="no_toc" id="25-principle-component-trees-and-their-persistent-homology"> <a href="#25-principle-component-trees-and-their-persistent-homology" class="anchor-heading" aria-labelledby="25-principle-component-trees-and-their-persistent-homology"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 25. <a href="https://openreview.net/forum?id=1bfZBI3arx">Principle Component Trees and their Persistent Homology</a></h3><p>Ben Kizaric, Daniel L. Pimentel-Alarcón</p><p class="fs-2">Keywords: <em>subspace clustering, low-rank decomposition, unsupervised learning, manifold learning, dimensionality reduction, topological data analysis</em></p><h3 class="no_toc" id="26-flowdas-a-flow-based-framework-for-data-assimilation"> <a href="#26-flowdas-a-flow-based-framework-for-data-assimilation" class="anchor-heading" aria-labelledby="26-flowdas-a-flow-based-framework-for-data-assimilation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 26. <a href="https://openreview.net/forum?id=MF64kI7WKQ">FlowDAS: A Flow-Based Framework for Data Assimilation</a></h3><p>Siyi Chen, Yixuan Jia, Qing Qu, He Sun, Jeffrey A Fessler</p><p class="fs-2">Keywords: <em>Data Assimilation, Stochastic Dynamic System, Flow matching, Stochastic Interpolants, Inverse Problem</em></p><h3 class="no_toc" id="27-large-scale-multiway-clustering-with-seeded-clustering"> <a href="#27-large-scale-multiway-clustering-with-seeded-clustering" class="anchor-heading" aria-labelledby="27-large-scale-multiway-clustering-with-seeded-clustering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 27. <a href="https://openreview.net/forum?id=xfA9mu6NQL">Large-Scale Multiway Clustering with Seeded Clustering</a></h3><p>Jiaxin Hu</p><p class="fs-2">Keywords: <em>scalable algorithm, time complexity, space complexity, large-scale data, tensor clustering, seeded clustering</em></p><h3 class="no_toc" id="28-are-all-layers-created-equal-a-neural-collapse-perspective"> <a href="#28-are-all-layers-created-equal-a-neural-collapse-perspective" class="anchor-heading" aria-labelledby="28-are-all-layers-created-equal-a-neural-collapse-perspective"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 28. <a href="https://openreview.net/forum?id=5eHpefiK8W">Are all layers created equal: A neural collapse perspective</a></h3><p>Jinxin Zhou, Jiachen Jiang, Zhihui Zhu</p><p class="fs-2">Keywords: <em>Deep Learning, Neural Collapse, Robustness, Generalization, Memorization, Understanding</em></p><h3 class="no_toc" id="29-geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics"> <a href="#29-geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics" class="anchor-heading" aria-labelledby="29-geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 29. <a href="https://openreview.net/forum?id=Lt6l1woQ84">Geometry of Concepts in Next-token Prediction: Neural-Collapse Meets Semantics</a></h3><p>Yize Zhao, Christos Thrampoulidis</p><p class="fs-2">Keywords: <em>Large Language Models(LLMs), Neural Embeddings, Word Embeddings, Neural-Collapse, Interpretability, Optimization</em></p><h3 class="no_toc" id="30-deep-neural-regression-collapse"> <a href="#30-deep-neural-regression-collapse" class="anchor-heading" aria-labelledby="30-deep-neural-regression-collapse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 30. <a href="https://openreview.net/forum?id=tIyeKqy2pd">Deep Neural Regression Collapse</a></h3><p>Akshay Rangamani, Altay Unal</p><p class="fs-2">Keywords: <em>Neural Collapse, Regression, Low Rank</em></p><h3 class="no_toc" id="31-geometric-algebra-planes-convex-implicit-neural-volumes"> <a href="#31-geometric-algebra-planes-convex-implicit-neural-volumes" class="anchor-heading" aria-labelledby="31-geometric-algebra-planes-convex-implicit-neural-volumes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 31. <a href="https://openreview.net/forum?id=6VBVEF70oD">Geometric Algebra Planes: Convex Implicit Neural Volumes</a></h3><p>Irmak Sivgin, Sara Fridovich-Keil, Gordon Wetzstein, Mert Pilanci</p><p class="fs-2">Keywords: <em>Volume representation, tensor decomposition, convex optimization, geometric algebra, nerf</em></p><h3 class="no_toc" id="32-a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries"> <a href="#32-a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries" class="anchor-heading" aria-labelledby="32-a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 32. <a href="https://openreview.net/forum?id=UaFKI8cjA6">A Robust Kernel Statistical Test of Invariance: Detecting Subtle Asymmetries</a></h3><p>Ashkan Soleymani, Behrooz Tahmasebi, Stefanie Jegelka, Patrick Jaillet</p><p class="fs-2">Keywords: <em>Invariance, Hypothesis Testing, Kernel Methods</em></p><h3 class="no_toc" id="33-learning-with-exact-invariances-in-polynomial-time"> <a href="#33-learning-with-exact-invariances-in-polynomial-time" class="anchor-heading" aria-labelledby="33-learning-with-exact-invariances-in-polynomial-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 33. <a href="https://openreview.net/forum?id=lhO8pJVIjh">Learning with Exact Invariances in Polynomial Time</a></h3><p>Ashkan Soleymani, Behrooz Tahmasebi, Stefanie Jegelka, Patrick Jaillet</p><p class="fs-2">Keywords: <em>Learning with Invariances, Kernels, Spectral Theory</em></p><h3 class="no_toc" id="34-primal-dual-spectral-representation-for-off-policy-evaluation"> <a href="#34-primal-dual-spectral-representation-for-off-policy-evaluation" class="anchor-heading" aria-labelledby="34-primal-dual-spectral-representation-for-off-policy-evaluation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 34. <a href="https://openreview.net/forum?id=uNEuIgkBA3">Primal-Dual Spectral Representation for Off-policy Evaluation</a></h3><p>Yang Hu, Tianyi Chen, Na Li, Kai Wang, Bo Dai</p><p class="fs-2">Keywords: <em>reinforcement learning, off-policy evaluation, spectral representation, primal-dual representation</em></p><h3 class="no_toc" id="35-dependence-induced-representations"> <a href="#35-dependence-induced-representations" class="anchor-heading" aria-labelledby="35-dependence-induced-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 35. <a href="https://openreview.net/forum?id=n6g3dVigo9">Dependence Induced Representations</a></h3><p>Xiangxiang Xu, Lizhong Zheng</p><p class="fs-2">Keywords: <em>representation learning, statistical dependence, maximal correlation, minimal sufficiency, neural collapse</em></p><h3 class="no_toc" id="36-moxco-how-i-learned-to-stop-exploring-and-love-my-local-minima"> <a href="#36-moxco-how-i-learned-to-stop-exploring-and-love-my-local-minima" class="anchor-heading" aria-labelledby="36-moxco-how-i-learned-to-stop-exploring-and-love-my-local-minima"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 36. <a href="https://openreview.net/forum?id=ferFzBa9bM">MoXCo: How I learned to stop exploring and love my local minima?</a></h3><p>Esha Singh, Shoham Sabach, Yu-Xiang Wang</p><p class="fs-2">Keywords: <em>optimization, deep learning, adaptive methods</em></p><h2 id="coffee-break--poster-session-3"> <a href="#coffee-break--poster-session-3" class="anchor-heading" aria-labelledby="coffee-break--poster-session-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Coffee Break + Poster Session 3</h2><h4 id="time-day-4-mar-27--thursday--1100-am-to-1230-pm"> <a href="#time-day-4-mar-27--thursday--1100-am-to-1230-pm" class="anchor-heading" aria-labelledby="time-day-4-mar-27--thursday--1100-am-to-1230-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 4 (Mar 27) – Thursday – 11:00 AM to 12:30 PM</a></h4><h3 class="no_toc" id="1-improving-neuron-level-interpretability-with-white-box-language-models"> <a href="#1-improving-neuron-level-interpretability-with-white-box-language-models" class="anchor-heading" aria-labelledby="1-improving-neuron-level-interpretability-with-white-box-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=XPpbo0zC4Y">Improving Neuron-level Interpretability with White-box Language Models</a></h3><p>Hao Bai, Yi Ma</p><p class="fs-2">Keywords: <em>White-box models, deep learning architectures, neuron-level interpretation</em></p><h3 class="no_toc" id="2-vanishing-feature-diagnosing-model-merging-and-beyond"> <a href="#2-vanishing-feature-diagnosing-model-merging-and-beyond" class="anchor-heading" aria-labelledby="2-vanishing-feature-diagnosing-model-merging-and-beyond"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=KBSh5ChQIo">Vanishing Feature: Diagnosing Model Merging and Beyond</a></h3><p>Xingyu Qu, Samuel Horváth</p><p class="fs-2">Keywords: <em>Model Merging, Efficiency, Deep Learning, Efficient Deep Learning</em></p><h3 class="no_toc" id="3-a-case-study-of-low-ranked-self-expressive-structures-in-neural-network-representations"> <a href="#3-a-case-study-of-low-ranked-self-expressive-structures-in-neural-network-representations" class="anchor-heading" aria-labelledby="3-a-case-study-of-low-ranked-self-expressive-structures-in-neural-network-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=vMGYwMFRKf">A Case Study of Low Ranked Self-Expressive Structures in Neural Network Representations</a></h3><p>Uday Singh Saini, William Shiao, Yahya Sattar, Yogesh Dahiya, Samet Oymak, Evangelos E. Papalexakis</p><p class="fs-2">Keywords: <em>Subspace Clustering, Centered Kernel Alignment, Representation Similarity Measures.</em></p><h3 class="no_toc" id="4-you-only-debias-once-towards-flexible-accuracy-fairness-trade-offs-at-inference-time"> <a href="#4-you-only-debias-once-towards-flexible-accuracy-fairness-trade-offs-at-inference-time" class="anchor-heading" aria-labelledby="4-you-only-debias-once-towards-flexible-accuracy-fairness-trade-offs-at-inference-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=XYMWd1wNlf">You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at Inference Time</a></h3><p>Xiaotian Han, Tianlong Chen, Kaixiong Zhou, Zhimeng Jiang, Zhangyang Wang, Xia Hu</p><p class="fs-2">Keywords: <em>fairness, weight space, neural network subspace</em></p><h3 class="no_toc" id="5-reccrysformer-refined-protein-structural-prediction-from-3d-patterson-maps-via-recycling-training-runs"> <a href="#5-reccrysformer-refined-protein-structural-prediction-from-3d-patterson-maps-via-recycling-training-runs" class="anchor-heading" aria-labelledby="5-reccrysformer-refined-protein-structural-prediction-from-3d-patterson-maps-via-recycling-training-runs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=U9DhMKzXPT">RecCrysFormer: Refined Protein Structural Prediction from 3D Patterson Maps via Recycling Training Runs</a></h3><p>Tom Pan, Evan Dramko, Mitchell D. Miller, George N Phillips Jr., Anastasios Kyrillidis</p><p class="fs-2">Keywords: <em>Protein Structural Prediction, Transformers, Patterson Maps</em></p><h3 class="no_toc" id="6-dual-reasoning-a-gnn-llm-collaborative-framework-for-knowledge-graph-question-answering"> <a href="#6-dual-reasoning-a-gnn-llm-collaborative-framework-for-knowledge-graph-question-answering" class="anchor-heading" aria-labelledby="6-dual-reasoning-a-gnn-llm-collaborative-framework-for-knowledge-graph-question-answering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 6. <a href="https://openreview.net/forum?id=odnOkx8Qfj">Dual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph Question Answering</a></h3><p>Guangyi Liu, Yongqi Zhang, Yong Li, Quanming Yao</p><p class="fs-2">Keywords: <em>Large Language Model, Knowledge Graph, Question Answering</em></p><h3 class="no_toc" id="7-meta-controlnet-enhancing-task-adaptation-via-meta-learning"> <a href="#7-meta-controlnet-enhancing-task-adaptation-via-meta-learning" class="anchor-heading" aria-labelledby="7-meta-controlnet-enhancing-task-adaptation-via-meta-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 7. <a href="https://openreview.net/forum?id=ju63pUpq0N">Meta ControlNet: Enhancing Task Adaptation via Meta Learning</a></h3><p>Junjie Yang, Jinze Zhao, Peihao Wang, Zhangyang Wang, Yingbin Liang</p><p class="fs-2">Keywords: <em>Meta Learning, Diffusion Models, Generalization</em></p><h3 class="no_toc" id="8-bridging-domain-adaptation-and-graph-neural-networks-a-tensor-based-framework-for-effective-label-propagation"> <a href="#8-bridging-domain-adaptation-and-graph-neural-networks-a-tensor-based-framework-for-effective-label-propagation" class="anchor-heading" aria-labelledby="8-bridging-domain-adaptation-and-graph-neural-networks-a-tensor-based-framework-for-effective-label-propagation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 8. <a href="https://openreview.net/forum?id=erHR9IqQBQ">Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based Framework for Effective Label Propagation</a></h3><p>Tao Wen, Elynn Chen, Yuzhou Chen, Qi Lei</p><p class="fs-2">Keywords: <em>Graph Classification, Domain Adaptation, Label Propagation</em></p><h3 class="no_toc" id="9-concept-bottleneck-model-with-zero-performance-loss"> <a href="#9-concept-bottleneck-model-with-zero-performance-loss" class="anchor-heading" aria-labelledby="9-concept-bottleneck-model-with-zero-performance-loss"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 9. <a href="https://openreview.net/forum?id=ii2zoKgRJV">Concept Bottleneck Model with Zero Performance Loss</a></h3><p>Zhenzhen Wang, Aleksander Popel, Jeremias Sulam</p><p class="fs-2">Keywords: <em>interpretability, explainability, concept bottleneck model, concept explanations</em></p><h3 class="no_toc" id="10-enhancing-video-representation-learning-with-temporal-differentiation"> <a href="#10-enhancing-video-representation-learning-with-temporal-differentiation" class="anchor-heading" aria-labelledby="10-enhancing-video-representation-learning-with-temporal-differentiation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 10. <a href="https://openreview.net/forum?id=NgR6hYd3Xw">Enhancing Video Representation Learning with Temporal Differentiation</a></h3><p>Siyi Chen, Minkyu Choi, Zesen Zhao, Kuan Han, Qing Qu, Zhongming Liu</p><p class="fs-2">Keywords: <em>video representation learning, physics-inspired</em></p><h3 class="no_toc" id="11-explaining-and-mitigating-the-modality-gap-in-contrastive-multimodal-learning"> <a href="#11-explaining-and-mitigating-the-modality-gap-in-contrastive-multimodal-learning" class="anchor-heading" aria-labelledby="11-explaining-and-mitigating-the-modality-gap-in-contrastive-multimodal-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 11. <a href="https://openreview.net/forum?id=2sThreW73a">Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning</a></h3><p>Can Yaras, Siyi Chen, Peng Wang, Qing Qu</p><p class="fs-2">Keywords: <em>multimodal learning, modality gap, contrastive learning</em></p><h3 class="no_toc" id="12-learning-effective-dynamics-across-spatio-temporal-scales-of-complex-flows"> <a href="#12-learning-effective-dynamics-across-spatio-temporal-scales-of-complex-flows" class="anchor-heading" aria-labelledby="12-learning-effective-dynamics-across-spatio-temporal-scales-of-complex-flows"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 12. <a href="https://openreview.net/forum?id=TU9e5yChcU">Learning Effective Dynamics across Spatio-Temporal Scales of Complex Flows</a></h3><p>Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos</p><p class="fs-2">Keywords: <em>Learned Effective Dynamics, Reduced-Order Modeling, Multiscale Systems, Turbulent Flows</em></p><h3 class="no_toc" id="13-white-box-error-correction-code-transformer"> <a href="#13-white-box-error-correction-code-transformer" class="anchor-heading" aria-labelledby="13-white-box-error-correction-code-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 13. <a href="https://openreview.net/forum?id=6lfnSzJ5qE">White-box Error Correction Code Transformer</a></h3><p>Ziyan Zheng, Chin Wa Lau, Nian Guo, Xiang Shi, Shao-Lun Huang</p><p class="fs-2">Keywords: <em>Error Correction Codes, Neural Decoder, White-box Transformer, Sparse Rate Reduction, Tanner Graph</em></p><h3 class="no_toc" id="14-curse-of-attention-a-kernel-based-perspective-for-why-transformers-fail-to-generalize-on-time-series-forecasting-and-beyond"> <a href="#14-curse-of-attention-a-kernel-based-perspective-for-why-transformers-fail-to-generalize-on-time-series-forecasting-and-beyond" class="anchor-heading" aria-labelledby="14-curse-of-attention-a-kernel-based-perspective-for-why-transformers-fail-to-generalize-on-time-series-forecasting-and-beyond"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 14. <a href="https://openreview.net/forum?id=bdOmItHgU5">Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to Generalize on Time Series Forecasting and Beyond</a></h3><p>Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang</p><p class="fs-2">Keywords: <em>Time Series Forecasting, Transformer Generalization, Kernel Methods</em></p><h3 class="no_toc" id="15-active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms"> <a href="#15-active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms" class="anchor-heading" aria-labelledby="15-active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 15. <a href="https://openreview.net/forum?id=Zx6WUbE9J7">Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</a></h3><p>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei</p><p class="fs-2">Keywords: <em>attention sink, mechanistic interpretability, language models, transformers</em></p><h3 class="no_toc" id="16-diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering"> <a href="#16-diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering" class="anchor-heading" aria-labelledby="16-diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 16. <a href="https://openreview.net/forum?id=8VKSMxYKiY">Diffusion models learn low-dimensional distributions via subspace clustering</a></h3><p>Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion models, mixture of low-rank Gaussians, phase transition, subspace clustering</em></p><h3 class="no_toc" id="17-visual-prompting-reimagined-the-power-of-activation-prompts"> <a href="#17-visual-prompting-reimagined-the-power-of-activation-prompts" class="anchor-heading" aria-labelledby="17-visual-prompting-reimagined-the-power-of-activation-prompts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 17. <a href="https://openreview.net/forum?id=thmB4XdOsD">Visual Prompting Reimagined: The Power of Activation Prompts</a></h3><p>Yihua Zhang, Hongkang Li, Yuguang Yao, Aochuan Chen, Shuai Zhang, Pin-Yu Chen, Meng Wang, Sijia Liu</p><p class="fs-2">Keywords: <em>visual prompt, parameter efficient finetuning, learning theory, generalization analysis</em></p><h3 class="no_toc" id="18-understanding-diffusion-based-representation-learning-via-low-dimensional-modeling"> <a href="#18-understanding-diffusion-based-representation-learning-via-low-dimensional-modeling" class="anchor-heading" aria-labelledby="18-understanding-diffusion-based-representation-learning-via-low-dimensional-modeling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 18. <a href="https://openreview.net/forum?id=HyESKJgPv6">Understanding Diffusion-based Representation Learning via Low-Dimensional Modeling</a></h3><p>Xiao Li, Zekai Zhang, Xiang Li, Siyi Chen, Zhihui Zhu, Peng Wang, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion representation learning, representation learning, diffusion model</em></p><h3 class="no_toc" id="19-simplifying-dino-by-coding-rate-regularization"> <a href="#19-simplifying-dino-by-coding-rate-regularization" class="anchor-heading" aria-labelledby="19-simplifying-dino-by-coding-rate-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 19. <a href="https://openreview.net/forum?id=CajE48Bs4h">Simplifying DINO by Coding Rate Regularization</a></h3><p>Ziyang Wu, Jingyuan Zhang, Druv Pai, Yi Ma</p><p class="fs-2">Keywords: <em>Representation Learning, Self Supervised Learning, Coding Rate</em></p><h3 class="no_toc" id="20-closure-discovery-for-coarse-grained-partial-differential-equations-using-grid-based-reinforcement-learning"> <a href="#20-closure-discovery-for-coarse-grained-partial-differential-equations-using-grid-based-reinforcement-learning" class="anchor-heading" aria-labelledby="20-closure-discovery-for-coarse-grained-partial-differential-equations-using-grid-based-reinforcement-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 20. <a href="https://openreview.net/forum?id=Pve4Pg0A1v">Closure Discovery for Coarse-Grained Partial Differential Equations Using Grid-based Reinforcement Learning</a></h3><p>Jan-Philipp von Bassewitz, Sebastian Kaltenbach, Petros Koumoutsakos</p><p class="fs-2">Keywords: <em>Closure Discovery, Inductive Bias, Multi-Agent Reinforcement Learning</em></p><h3 class="no_toc" id="21-heterogeneous-decision-making-in-mixed-traffic-uncertainty-aware-planning-and-bounded-rationality"> <a href="#21-heterogeneous-decision-making-in-mixed-traffic-uncertainty-aware-planning-and-bounded-rationality" class="anchor-heading" aria-labelledby="21-heterogeneous-decision-making-in-mixed-traffic-uncertainty-aware-planning-and-bounded-rationality"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 21. <a href="https://openreview.net/forum?id=pxg38Rw63r">Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality</a></h3><p>Hang Wang, Qiaoyi Fang, Junshan Zhang</p><p class="fs-2">Keywords: <em>Mixed Traffic, Reinforcement Learning, Planning, Bounded Rationality</em></p><h3 class="no_toc" id="22-competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents"> <a href="#22-competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents" class="anchor-heading" aria-labelledby="22-competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 22. <a href="https://openreview.net/forum?id=rILX5xHcg4">CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents</a></h3><p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie</p><p class="fs-2">Keywords: <em>LLM-based Agent, Agent Based Modeling, Competition</em></p><h3 class="no_toc" id="23-dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks"> <a href="#23-dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks" class="anchor-heading" aria-labelledby="23-dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 23. <a href="https://openreview.net/forum?id=SRB8ricWVr">DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks</a></h3><p>Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie</p><p class="fs-2">Keywords: <em>Large Language Models, Evaluation, Data Contamination</em></p><h3 class="no_toc" id="24-knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs"> <a href="#24-knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs" class="anchor-heading" aria-labelledby="24-knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 24. <a href="https://openreview.net/forum?id=FqVEcAVRjm">Knowledge-aware Parsimony Learning: A Perspective from Relational Graphs</a></h3><p>Quanming Yao, Yongqi Zhang, Yaqing Wang, Nan Yin, James Kwok, Qiang Yang</p><p class="fs-2">Keywords: <em>scaling law, Parsimony Learning, Graph Learning</em></p><h3 class="no_toc" id="25-understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing"> <a href="#25-understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing" class="anchor-heading" aria-labelledby="25-understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 25. <a href="https://openreview.net/forum?id=6WQq5KBejG">Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</a></h3><p>Peihao Wang, Ruisi Cai, Yuehao Wang, Jiajun Zhu, Pragya Srivastava, Zhangyang Wang, Pan Li</p><p class="fs-2">Keywords: <em>State Space Models, Large Language Models, Recency, Over-smoothing</em></p><h3 class="no_toc" id="26-rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding"> <a href="#26-rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding" class="anchor-heading" aria-labelledby="26-rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 26. <a href="https://openreview.net/forum?id=6oQDDcW2gY">Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding</a></h3><p>Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang</p><p class="fs-2">Keywords: <em>Positional Encoding, Equivariant Machine Learning, Large Language Models</em></p><h3 class="no_toc" id="27-implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations"> <a href="#27-implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations" class="anchor-heading" aria-labelledby="27-implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 27. <a href="https://openreview.net/forum?id=qxMTYliGHI">Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations</a></h3><p>Yize Zhao, Tina Behnia, Vala Vakilian, Christos Thrampoulidis</p><p class="fs-2">Keywords: <em>language models, neural embeddings, optimization, implicit regularization, low-rank matrix factorization, support-vector machines</em></p><h3 class="no_toc" id="28-dynamic-rescaling-for-training-gnns"> <a href="#28-dynamic-rescaling-for-training-gnns" class="anchor-heading" aria-labelledby="28-dynamic-rescaling-for-training-gnns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 28. <a href="https://openreview.net/forum?id=cFsLHAGnyA">Dynamic Rescaling for Training GNNs</a></h3><p>Nimrah Mustafa, Rebekka Burkholz</p><p class="fs-2">Keywords: <em>graph neural network, rescale invariance, generalization, network balance</em></p><h3 class="no_toc" id="29-image-reconstruction-via-autoencoding-sequential-deep-image-prior"> <a href="#29-image-reconstruction-via-autoencoding-sequential-deep-image-prior" class="anchor-heading" aria-labelledby="29-image-reconstruction-via-autoencoding-sequential-deep-image-prior"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 29. <a href="https://openreview.net/forum?id=s2pVNpUI4b">Image Reconstruction Via Autoencoding Sequential Deep Image Prior</a></h3><p>Ismail Alkhouri, Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar</p><p class="fs-2">Keywords: <em>Image Reconstruction, Deep Image Prior, Generative Models</em></p><h3 class="no_toc" id="30-sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems"> <a href="#30-sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems" class="anchor-heading" aria-labelledby="30-sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 30. <a href="https://openreview.net/forum?id=jj2st1jBnY">SITCOM: Step-wise Triple-Consistent Diffusion Sampling for Inverse Problems</a></h3><p>Ismail Alkhouri, Shijun Liang, Cheng-Han Huang, Jimmy Dai, Qing Qu, Saiprasad Ravishankar, Rongrong Wang</p><p class="fs-2">Keywords: <em>Image Restoration, Diffusion Models, Inverse Problems</em></p><h3 class="no_toc" id="31-sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training"> <a href="#31-sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training" class="anchor-heading" aria-labelledby="31-sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 31. <a href="https://openreview.net/forum?id=d3E3LWmTar">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</a></h3><p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Sergey Levine, Yi Ma</p><p class="fs-2">Keywords: <em>foundation model post-training</em></p><h3 class="no_toc" id="32-attention-only-transformers-via-unrolled-subspace-denoising"> <a href="#32-attention-only-transformers-via-unrolled-subspace-denoising" class="anchor-heading" aria-labelledby="32-attention-only-transformers-via-unrolled-subspace-denoising"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 32. <a href="https://openreview.net/forum?id=AAaP689tAD">Attention-Only Transformers via Unrolled Subspace Denoising</a></h3><p>Peng Wang, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma</p><p class="fs-2">Keywords: <em>transformer, self-attention, unrolled optimization, subspace denoising</em></p><h3 class="no_toc" id="33-out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers"> <a href="#33-out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers" class="anchor-heading" aria-labelledby="33-out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 33. <a href="https://openreview.net/forum?id=cC68jjGq6T">Out-of-distribution generalization via composition: a lens through induction heads in Transformers</a></h3><p>Jiajun Song, Zhuoyan Xu, Yiqiao Zhong</p><p class="fs-2">Keywords: <em>out-of-distribution generalization, low-dimensional subspace, composition, large language models, emergent ability, in-context learning</em></p><h3 class="no_toc" id="34-sufficient-and-necessary-explanations-and-what-lies-in-between"> <a href="#34-sufficient-and-necessary-explanations-and-what-lies-in-between" class="anchor-heading" aria-labelledby="34-sufficient-and-necessary-explanations-and-what-lies-in-between"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 34. <a href="https://openreview.net/forum?id=H43BmpeJII">Sufficient and Necessary Explanations (and What Lies in Between)</a></h3><p>Beepul Bharti, Paul Yi, Jeremias Sulam</p><p class="fs-2">Keywords: <em>interpretability, explainability</em></p><h3 class="no_toc" id="35-generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping"> <a href="#35-generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping" class="anchor-heading" aria-labelledby="35-generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 35. <a href="https://openreview.net/forum?id=8mMqlab1pn">Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping</a></h3><p>Enming Liang, Minghua Chen</p><p class="fs-2">Keywords: <em>Non-convex Optimization, Generative Modeling, Flow, ODE</em></p><h3 class="no_toc" id="36-shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models"> <a href="#36-shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models" class="anchor-heading" aria-labelledby="36-shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 36. <a href="https://openreview.net/forum?id=U44hq1OLDG">Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models</a></h3><p>Wenda Li, Huijie Zhang, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion Model, watermark, low-dimensional subspace, consistency, robustness</em></p></main></div></div><div class="search-overlay"></div></div>
