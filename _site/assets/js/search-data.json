{"0": {
    "doc": "Home",
    "title": "Call for Papers",
    "content": "We are pleased to invite paper submissions for the first Conference on Parsimony and Learning. Please see the call for papers for details about the submission and reviewing process, as well as subject areas of interest and general policies. Stay tuned for further updates. ",
    "url": "/#call-for-papers",
    
    "relUrl": "/#call-for-papers"
  },"1": {
    "doc": "Home",
    "title": "Key Dates and Deadlines",
    "content": ". | August 28th, 2023: Submission Deadline for Proceedings Track | October 10th, 2023: Submission Deadline for Recent Spotlight Track | October 14th, 2023: 2-Week Rebuttal Stage Starts (Proceedings Track) | October 27th, 2023: Rebuttal Stage Ends, Authors-Reviewers Discussion Stage Starts (Proceedings Track) | November 5th, 2023: Authors-Reviewers Discussion Stage Ends (Proceedings Track) | November 20th, 2023: Final Decisions Released (Both Tracks) | December 5th, 2023: Camera-Ready Deadline (Both Tracks) | January 3rd-6th, 2024: Main Conference (In-Person, HKU Main Campus) | . ",
    "url": "/#key-dates-and-deadlines",
    
    "relUrl": "/#key-dates-and-deadlines"
  },"2": {
    "doc": "Home",
    "title": "Keynote Speakers",
    "content": "Additional speakers to be announced soon! . <!-- ",
    "url": "/#keynote-speakers",
    
    "relUrl": "/#keynote-speakers"
  },"3": {
    "doc": "Home",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . IST Austria / Neural Magic . SueYeon Chung . NYU / Flatiron Institute . Kostas Daniilidis . UPenn . Maryam Fazel . University of Washington . Tom Goldstein . University of Maryland . Yingbin Liang . Ohio State University . Robert D. Nowak . University of Wisconsin-Madison . Dimitris Papailiopoulos . University of Wisconsin-Madison . Jong Chul Ye . KAIST . ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "Home",
    "title": "Home",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU The Conference on Parsimony and Learning (CPAL) is an annual research conference focused on addressing the parsimonious, low dimensional structures that prevail in machine learning, signal processing, optimization, and beyond. We are interested in theories, algorithms, applications, hardware and systems, as well as scientific foundations for learning with parsimony. We describe our vision for the conference in more detail here. ",
    "url": "/",
    
    "relUrl": "/"
  },"5": {
    "doc": "Advisory Committee",
    "title": "Advisory Committee",
    "content": "Additional members of the advisory committee to be announced soon . Anima Anandkumar . Caltech / NVIDIA . Advisory Committee . Emmanuel Candès . Stanford . Advisory Committee . Alex Dimakis . UT Austin . Advisory Committee . Michael Elad . Technion . Advisory Committee . Yi Ma . UC Berkeley / HKU IDS . Advisory Committee . Peyman Milanfar . Google Research . Advisory Committee . Stefano Soatto . UCLA . Advisory Committee . Rebecca Willett . UChicago . Advisory Committee . Eric P. Xing . MBZUAI / Carnegie Mellon University . Advisory Committee . Tong Zhang . HKUST . Advisory Committee . * Ordered alphabetically . ",
    "url": "/advisory/#advisory-committee",
    
    "relUrl": "/advisory/#advisory-committee"
  },"6": {
    "doc": "Advisory Committee",
    "title": "Advisory Committee",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/advisory/",
    
    "relUrl": "/advisory/"
  },"7": {
    "doc": "Area Chairs",
    "title": "Area Chairs",
    "content": "Navid Azizan . MIT . Area Chair . Chenglong Bao . Tsinghua University . Area Chair . Babak Ehteshami Bejnordi . Qualcomm . Area Chair . Beidi Chen . Meta / Carnegie Mellon University . Area Chair . Tianlong Chen . UT Austin / MIT . Area Chair . Tianyi Chen . RPI . Area Chair . Yubei Chen . NYU / Meta . Area Chair . Yuxin Chen . UPenn . Area Chair . Sarah Dean . Cornell University . Area Chair . Ivan Dokmanić . University of Basel . Area Chair . Simon Du . University of Washington . Area Chair . Utku Evci . Google DeepMind . Area Chair . Paris Giampouras . Johns Hopkins University . Area Chair . Wei Hu . University of Michigan . Area Chair . Souvik Kundu . Intel Labs, USA . Area Chair . Qi Lei . NYU . Area Chair . Xiao Li . CUHK Shenzhen . Area Chair . Wenjing Liao . Georgia Institute of Technology . Area Chair . Jiaqi Ma . Harvard / UIUC . Area Chair . Elena Mocanu . University of Twente . Area Chair . Vidya K. Muthukumar . Georgia Institute of Technology . Area Chair . Greg Ongie . Marquette University . Area Chair . Laixi Shi . Caltech . Area Chair . Mahdi Soltanolkotabi . USC . Area Chair . Weijie Su . UPenn . Area Chair . Soledad Villar . Johns Hopkins University . Area Chair . Peng Wang . University of Michigan . Area Chair . Yu-Xiang Wang . UC Santa Barbara . Area Chair . Bihan Wen . Nanyang Technological University . Area Chair . Fanny Yang . ETH Zurich . Area Chair . Tianbao Yang . Texas A&amp;M . Area Chair . Yuqian Zhang . Rutgers University . Area Chair . * Ordered alphabetically . ",
    "url": "/area_chairs/",
    
    "relUrl": "/area_chairs/"
  },"8": {
    "doc": "Call for Papers",
    "title": "Call for Papers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/cfp/",
    
    "relUrl": "/cfp/"
  },"9": {
    "doc": "Code of Conduct",
    "title": "Code of Conduct",
    "content": "(Adapted from ICML/ICLR/NeurIPS/LoG) . We strive to hold a conference in which any person can meaningfully participate in the CPAL community through: sharing ideas, presenting their work, meeting members of the community, learning from other people’s work, and discussing ways to improve the community. This conference will work towards preventing any type of discrimination based on age, disability, ethnicity, experience in the field, gender identity, nationality, physical appearance, race, religion, sexual orientation, or other protected characteristics. We strictly prohibit any actions that may prevent the participation of any attendee, including: bullying, harassment, inappropriate media, offensive language, violence, zoom bombing, and so on. Please report any violations of the code of conduct to the conference organizers, via email, slack, or any other channels. If requested, we can handle these reports anonymously, to protect the person making the report. Violators of the code of conduct will be asked to stop their inappropriate behavior, and may be removed from their right to participate in the conference. Further disciplinary action such as bans from future iterations of the conference may be taken. ",
    "url": "/code_of_conduct/#code-of-conduct",
    
    "relUrl": "/code_of_conduct/#code-of-conduct"
  },"10": {
    "doc": "Code of Conduct",
    "title": "Code of Conduct",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/code_of_conduct/",
    
    "relUrl": "/code_of_conduct/"
  },"11": {
    "doc": "Key Dates",
    "title": "Key Dates and Deadlines",
    "content": ". | August 28th, 2023: Submission Deadline for Proceedings Track | October 10th, 2023: Submission Deadline for Recent Spotlight Track | October 14th, 2023: 2-Week Rebuttal Stage Starts (Proceedings Track) | October 27th, 2023: Rebuttal Stage Ends, Authors-Reviewers Discussion Stage Starts (Proceedings Track) | November 5th, 2023: Authors-Reviewers Discussion Stage Ends (Proceedings Track) | November 20th, 2023: Final Decisions Released (Both Tracks) | December 5th, 2023: Camera-Ready Deadline (Both Tracks) | January 3rd-6th, 2024: Main Conference (In-Person, HKU Main Campus) | . ",
    "url": "/deadlines/#key-dates-and-deadlines",
    
    "relUrl": "/deadlines/#key-dates-and-deadlines"
  },"12": {
    "doc": "Key Dates",
    "title": "Key Dates",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/deadlines/",
    
    "relUrl": "/deadlines/"
  },"13": {
    "doc": "Organization Committee",
    "title": "Organization Committee",
    "content": " ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"14": {
    "doc": "Organization Committee",
    "title": "General Chairs",
    "content": "Gitta Kutyniok . LMU Munich . General Chair . Yi Ma . UC Berkeley / HKU IDS . General Chair . Harry Shum . HKUST / IDEA . General Chair . René Vidal . UPenn . General Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"15": {
    "doc": "Organization Committee",
    "title": "Program Chairs",
    "content": "Yuejie Chi . Carnegie Mellon University . Program Chair . Gintare Karolina Dziugaite . Google DeepMind . Program Chair . Qing Qu . University of Michigan . Program Chair . Atlas Wang . UT Austin . Program Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"16": {
    "doc": "Organization Committee",
    "title": "Local Chairs",
    "content": "Yanchao Yang . HKU . Local Chair . Man-Chung Yue . HKU . Local Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"17": {
    "doc": "Organization Committee",
    "title": "Publication Chairs",
    "content": "Zhihui Zhu . Ohio State University . Publication Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"18": {
    "doc": "Organization Committee",
    "title": "Industry Liaison Chairs",
    "content": "Dan Alistarh . IST Austria / Neural Magic . Industry Liaison Chair . Dilin Wang . Meta . Industry Liaison Chair . Chong You . Google Research . Industry Liaison Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"19": {
    "doc": "Organization Committee",
    "title": "Panel Chairs",
    "content": "Sijia Liu . Michigan State University . Panel Chair . Jeremias Sulam . Johns Hopkins University . Panel Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"20": {
    "doc": "Organization Committee",
    "title": "Tutorial Chairs",
    "content": "Saiprasad Ravishankar . Michigan State University . Tutorial Chair . John Wright . Columbia University . Tutorial Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"21": {
    "doc": "Organization Committee",
    "title": "Publicity Chairs",
    "content": "Yani Ioannou . University of Calgary . Publicity Chair . William T. Redman . UCSB . Publicity Chair . Liyue Shen . University of Michigan . Publicity Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"22": {
    "doc": "Organization Committee",
    "title": "Web Chairs",
    "content": "Sam Buchanan . TTIC . Web Chair . * Ordered alphabetically . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"23": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"24": {
    "doc": "Review Guidelines",
    "title": "Reviewer Guidelines",
    "content": " ",
    "url": "/review_guidelines/#reviewer-guidelines",
    
    "relUrl": "/review_guidelines/#reviewer-guidelines"
  },"25": {
    "doc": "Review Guidelines",
    "title": "Notable Innovations in Our Review Mechanism",
    "content": "CPAL strives for providing every paper with high-quality, accountable reviews, and therefore takes the following actions in addition: . | Shepherding by an Action PC: Every paper’s final decision, after being recommended by AC, will go through the direct shepherding of all program chairs (led by one “action PC”). The action PC has two main duties: . | (before final decision released) The action PC will pay particular attention to the borderline cases and the dispute (large score variations) cases, and will be asked to write additional “meta-meta reviews” in those cases and potentially calibrate on top of AC recommendations. Final decisions will be scrutinized and made in a joint meeting by all program chairs. | (after receiving the camera-ready) Each accepted paper’s authors will be asked to submit a one-page cover letter, summarizing what revisions are made between the paper’s submitted and camera-ready versions. The action PC will ensure: (1) all “promised” changes by authors during the discussion stage are indeed implemented; (2) no change that is “too substantial” and “unsoliciated” shall be made to the paper, unless in exceptional circumstances where the action PC has to approve case-by-case. The action PC reserves the right to reject a camera-ready submission and exclude it from the conference proceedings. | . | Semi-Open Identity for Accountability (Action PC and/or AC): For every accepted paper, the names of its AC and action PC will be publicly released on its OpenReview page too. For every rejected paper (excluding withdrawals), only the name of its action PC will be displayed. This decision was not reached lightly; but we hope it would meaningfully add credibility and accountability for every paper’s final outcome. | Reviewer Rating and “Dynamic Sparse Selection”: each AC will be asked to rate every reviewer in their batch, in terms of timeliness and quality. Program chairs, who know all reviewers’ identities, will compile a list of reviewers sorted by their average ratings received. Reviewers that receive consistent low reviewer rating for multiple papers / from multiple ACs will be excluded from future review processes. | . ",
    "url": "/review_guidelines/#notable-innovations-in-our-review-mechanism",
    
    "relUrl": "/review_guidelines/#notable-innovations-in-our-review-mechanism"
  },"26": {
    "doc": "Review Guidelines",
    "title": "General Guidelines",
    "content": "We all like the Acceptance Criteria made by TMLR https://jmlr.org/tmlr/acceptance-criteria.html and would instruct our ACs and reviewers to honor the same. In particular we note: . “Crucially, it should not be used as a reason to reject work that isn’t considered “significant” or “impactful” because it isn’t achieving a new state-of-the-art on some benchmark. Nor should it form the basis for rejecting work on a method considered not “novel enough”, as novelty of the studied method is not a necessary criteria for acceptance. We explicitly avoid these terms (“significant”, “impactful”, “novel”), and focus instead on the notion of “interest”. If the authors make it clear that there is something to be learned by some researchers in their area from their work, then the criteria of interest is considered satisfied. ",
    "url": "/review_guidelines/#general-guidelines",
    
    "relUrl": "/review_guidelines/#general-guidelines"
  },"27": {
    "doc": "Review Guidelines",
    "title": "Review Guidelines",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/review_guidelines/",
    
    "relUrl": "/review_guidelines/"
  },"28": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Additional speakers to be announced soon! . <!-- ",
    "url": "/speakers/#keynote-speakers",
    
    "relUrl": "/speakers/#keynote-speakers"
  },"29": {
    "doc": "Keynote Speakers",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . IST Austria / Neural Magic . SueYeon Chung . NYU / Flatiron Institute . Kostas Daniilidis . UPenn . Maryam Fazel . University of Washington . Tom Goldstein . University of Maryland . Yingbin Liang . Ohio State University . Robert D. Nowak . University of Wisconsin-Madison . Dimitris Papailiopoulos . University of Wisconsin-Madison . Jong Chul Ye . KAIST . ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"30": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"31": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": " ",
    "url": "/subject_areas/#subject-areas",
    
    "relUrl": "/subject_areas/#subject-areas"
  },"32": {
    "doc": "Subject Areas",
    "title": "Theory &amp; Foundations",
    "content": ". | Theories for sparse coding, structured sparsity, subspace learning, low-dimensional manifolds, and general low-dimensional structures. | Dictionary learning and representation learning for low-dimensional structures and their connections to deep learning theory. | Equivariance and invariance modeling. | Theoretical neuroscience and cognitive science foundation for parsimony, and biologically inspired computational mechanisms. | . ",
    "url": "/subject_areas/#theory--foundations",
    
    "relUrl": "/subject_areas/#theory--foundations"
  },"33": {
    "doc": "Subject Areas",
    "title": "Optimization &amp; Algorithms",
    "content": ". | Optimization, robustness, and generalization methods for learning compact and structured representations. | Interpretable and efficient deep architectures (e.g., based on unrolled optimization). | Data-efficient and computation-efficient training and inference. | Adaptive and robust learning and inference algorithms. | Distributed, networked, or federated learning at scale. | Other nonlinear dimension-reduction and representation-learning methods. | . ",
    "url": "/subject_areas/#optimization--algorithms",
    
    "relUrl": "/subject_areas/#optimization--algorithms"
  },"34": {
    "doc": "Subject Areas",
    "title": "Data, Systems &amp; Applications",
    "content": ". | Domain-specific datasets, benchmarks, and evaluation metrics. | Parsimonious and structured representation learning from data. | Inverse problems that benefit from parsimonious priors. | Hardware and system co-design for parsimonious learning algorithms. | Intelligent systems that integrate perception, learning, and decision making (i.e., robots). | Applications in science, engineering, medicine, and social sciences. | . The above is intended as a high-level overview of CPAL’s focus and by no means exclusive. If you doubt that your paper fits the venue, feel free to contact the program chairs via email at pcs@cpal.cc. ",
    "url": "/subject_areas/#data-systems--applications",
    
    "relUrl": "/subject_areas/#data-systems--applications"
  },"35": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/subject_areas/",
    
    "relUrl": "/subject_areas/"
  },"36": {
    "doc": "Submission Tracks",
    "title": "Submission Tracks and Review Process",
    "content": "CPAL has two submission tracks: . | Proceedings track (archival) | “Recent spotlight” track (non-archival) | . Submissions to both tracks are to be prepared using the CPAL LaTeX style files, available as a zip archive or as an Overleaf template. CPAL OpenReview Submission Portal . ",
    "url": "/tracks/#submission-tracks-and-review-process",
    
    "relUrl": "/tracks/#submission-tracks-and-review-process"
  },"37": {
    "doc": "Submission Tracks",
    "title": "Proceedings Track  (archival)",
    "content": "The submission and review stage will be double-blind. We use OpenReview to host papers and allow for public discussions. Before the end of the Authors-Reviewers Discussion Stage, besides official reviews, anyone can post publicly visible comments; and authors can participate in the discussion as well as update their submission at any time. After that, there will be an internal discussion period amongst reviewers and ACs with the aim of summarizing the review process, after which the final decisions are made by ACs. After the notification deadline, accepted and opted-in rejected papers will be made public and open for non-anonymous public commenting. Their anonymous reviews, meta-reviews, author responses and reviewer responses will also be made public. Authors of rejected papers will have two weeks after the notification deadline to opt in to make their de-anonymized rejected papers public in OpenReview. Submissions that are substantially similar to papers previously published, or submitted in parallel to other peer-reviewed venues with proceedings or journals may not be submitted to the Proceedings Track. Papers previously presented at workshops are permitted, so long as they did not appear in a conference proceedings (e.g., CVPRW proceedings), a journal or a book. The existence of non-anonymous preprints (on arXiv or other online repositories, personal websites, social media) will not result in rejection. Authors may submit anonymized work to CPAL that is already available as a preprint (e.g., on arXiv) without citing it. Accepted papers will be published in the Proceedings for Machine Learning Research (PMLR). Full proceedings papers can have up to nine pages with unlimited pages for references and appendix. Upon acceptance of a paper, at least one of the authors must join the conference. Using Large Language Models (LLMs) . We follow the rule by NeurIPS 2023, quoted as follows: . “We welcome authors to use any tool that is suitable for preparing high-quality papers and research. However, we ask authors to keep in mind two important criteria. First, we expect papers to fully describe their methodology, and any tool that is important to that methodology, including the use of LLMs, should be described also. For example, authors should mention tools (including LLMs) that were used for data processing or filtering, visualization, facilitating or running experiments, and proving theorems. It may also be advisable to describe the use of LLMs in implementing the method (if this corresponds to an important, original, or non-standard component of the approach). Second, authors are responsible for the entire content of the paper, including all text and figures, so while authors are welcome to use any tool they wish for writing the paper, they must ensure that all text is correct and original.” . ",
    "url": "/tracks/#proceedings-track--archival",
    
    "relUrl": "/tracks/#proceedings-track--archival"
  },"38": {
    "doc": "Submission Tracks",
    "title": "“Recent Spotlight” Track (non-archival)",
    "content": "We meanwhile aim to showcase the latest research innovations at all stages of the research process, from work-in-progress to recently published papers. Concretely, we ask members of the community to submit a conference-style paper (at most nine pages, with extra pages for references allowed) describing the work. Please also upload a short (250 word) abstract to OpenReview. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail: . | A poster (in PDF form) presenting results of work-in-progress. | A link to an arXiv preprint or a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This track is non-archival and has no proceedings. We permit under-review or concurrent submissions, as well as papers officially accepted by a journal or conference within 6 months of the submission deadline for Recent Spotlight Track (Oct 10, 2023). Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions), and will be held with the same high quality bar with the Proceedings Track. ",
    "url": "/tracks/#recent-spotlight-track-non-archival",
    
    "relUrl": "/tracks/#recent-spotlight-track-non-archival"
  },"39": {
    "doc": "Submission Tracks",
    "title": "Submission Tracks",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/tracks/",
    
    "relUrl": "/tracks/"
  },"40": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "“Everything should be made as simple as possible, but not any simpler.” . – Albert Einstein . One of the most fundamental reasons for the very existence and therefore emergence of intelligence or science is that the world is not fully random, but highly structured and predictable. Hence, a fundamental purpose and function of intelligence or science is to learn parsimonious models (or laws) for such predicable structures, from massive sensed data of the world. Over the past decade, the advent of machine learning and large-scale computing has immeasurably changed the ways we process, interpret, and predict with data in engineering and science. The ‘traditional’ approach to algorithm design, based around parametric models for specific structures of signals and measurements – say sparse and low-rank models – and the associated optimization toolkit, is now significantly enriched with data-driven learning-based techniques, where large-scale networks are pre-trained and then adapted to a variety of specific tasks. Nevertheless, the successes of both modern data-driven and classic model-based paradigms rely crucially on correctly identifying the low-dimensional structures present in real-world data, to the extent that we see the roles of learning and compression of data processing algorithms – whether explicit or implicit, as with deep networks – as inextricably linked. Over the last ten or so years, several rich lines of research, including theoretical, computational, and practical, have explored the interplay between learning and compression. Some works explore the role of signal models in the era of deep learning, attempting to understand the interaction between deep networks and nonlinear, multi-modal data structures. Others have applied these insights to the principled design of deep architectures that incorporate desired structures in data into the learning process. Still others have considered generic deep networks as first-class citizens in their own right, exploring ways to compress and sparsify models for greater efficiency, often accompanied by hardware or system-aware co-designs. Across each of these settings, theoretical works rooted in low-dimensional modeling have begun to explain the foundations of deep architectures and efficient learning – from optimization to generalization – in spite of “overparameterization” and other obstructions. Most recently, the advent of foundation models has led some to posit that parsimony and compression itself are a fundamental part of the learning objective of an intelligent system, connecting to ideas from neuroscience on compression as a guiding principle for the brain representing the sensory data of the world. By and large, these lines of work have so far developed somewhat in isolation from one another, in spite of their common basis and purpose for parsimony and learning. Our intention in organizing this conference is to address this issue and go beyond: we envision the conference as a general scientific forum where researchers in machine learning, applied mathematics, signal processing, optimization, intelligent systems, and all associated science and engineering fields can gather, share insights, and ultimately work towards a common modern theoretical and computational framework for understanding intelligence and science from the perspective of parsimonious learning. ",
    "url": "/vision/#conference-vision",
    
    "relUrl": "/vision/#conference-vision"
  },"41": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/vision/",
    
    "relUrl": "/vision/"
  }
}
