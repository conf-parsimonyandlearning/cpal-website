{"0": {
    "doc": "Home",
    "title": "Register to Attend CPAL 2024",
    "content": "All CPAL attendees are required to register. The deadline to register is December 15th, 2023. Registration Link . ",
    "url": "/#register-to-attend-cpal-2024",
    
    "relUrl": "/#register-to-attend-cpal-2024"
  },"1": {
    "doc": "Home",
    "title": "Keynote Speakers",
    "content": "Information on the speakers’ planned talks is available here. <!-- ",
    "url": "/#keynote-speakers",
    
    "relUrl": "/#keynote-speakers"
  },"2": {
    "doc": "Home",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . Institute of Science and Technology Austria / Neural Magic . SueYeon Chung . New York University / Flatiron Institute . Kostas Daniilidis . University of Pennsylvania . Maryam Fazel . University of Washington . Tom Goldstein . University of Maryland . Yingbin Liang . Ohio State University . Dimitris Papailiopoulos . University of Wisconsin-Madison . Stefano Soatto . University of California, Los Angeles . Jong Chul Ye . Korea Advanced Institute of Science and Technology (KAIST) . ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Home",
    "title": "Tentative Program",
    "content": "All times below are in HKT (GMT+8). | 08:00 AM | 08:30 AM | 09:00 AM | 09:30 AM | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 01:00 PM | 01:30 PM | 02:00 PM | 02:30 PM | 03:00 PM | 03:30 PM | 04:00 PM | 04:30 PM | 05:00 PM | 05:30 PM | 06:00 PM | 06:30 PM | 07:00 PM | 07:30 PM | 08:00 PM | 08:30 PM | 09:00 PM | . | ",
    "url": "/#tentative-program",
    
    "relUrl": "/#tentative-program"
  },"4": {
    "doc": "Home",
    "title": "Day 1 (Jan 3)Wednesday",
    "content": ". | Registration 8:00 AM–9:15 AM TBA | Opening Remarks 9:15 AM–10:00 AM TBA | Stefano Soatto 10:00 AM–11:00 AM TBA \"Representation and Control of Meanings in Large Language Models and Multimodal Foundation Models\" | Coffee Break | Rising Stars Session 1 11:20 AM–12:30 PM TBA | Lunch Break | Maryam Fazel 1:30 PM–2:30 PM TBA \"Flat Minima and Generalization in Learning: The Case of Low-rank Matrix Recovery\" | Oral Session 1 2:30 PM–3:40 PM TBA | Coffee Break | Dan Alistarh 4:00 PM–5:00 PM TBA \"Accurate Model Compression at GPT Scale\" | Reception 5:00 PM–6:30 PM TBA | . | ",
    "url": "/",
    
    "relUrl": "/"
  },"5": {
    "doc": "Home",
    "title": "Day 2 (Jan 4)Thursday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Jong Chul Ye 9:00 AM–10:00 AM TBA \"Scaling Diffusion Model for Inverse Problems Beyond 2D and Known Forward Models\" | Rising Stars Session 2 10:00 AM–11:00 AM TBA | Coffee Break | Oral Session 2 11:20 AM–12:30 PM TBA | Lunch Break | Kostas Daniilidis 1:30 PM–2:30 PM TBA \"Parsimony through Equivariance\" | Oral Session 3 2:30 PM–4:00 PM TBA | Coffee Break | Panel Discussion 4:10 PM–5:00 PM TBA | Spotlight Poster Session 1 5:00 PM–6:30 PM TBA | Banquet 7:00 PM–9:00 PM TBA | . | ",
    "url": "/",
    
    "relUrl": "/"
  },"6": {
    "doc": "Home",
    "title": "Day 3 (Jan 5)Friday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Yingbin Liang 9:00 AM–10:00 AM TBA \"In-Context Convergence of Transformers\" | Rising Stars Session 3 10:00 AM–11:00 AM TBA | Coffee Break | Oral Session 4 11:20 AM–12:30 PM TBA | Lunch Break | Dimitris Papailiopoulos 1:30 PM–2:30 PM TBA \"Teaching arithmetic to small language models\" | Oral Session 5 2:30 PM–3:40 PM TBA | Coffee Break | Oral Session 6 4:00 PM–5:00 PM TBA | Spotlight Poster Session 2 5:00 PM–6:30 PM TBA | Tram Tour (Tentative) 7:00 PM–9:00 PM TBA | . | ",
    "url": "/",
    
    "relUrl": "/"
  },"7": {
    "doc": "Home",
    "title": "Day 4 (Jan 6)Saturday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Tom Goldstein 9:00 AM–10:00 AM TBA \"Statistical methods for addressing safety and security issues of generative models\" | SueYeon Chung 10:00 AM–11:00 AM TBA \"Multi-level theory of neural representations: Capacity of neural manifolds in biological and artificial neural networks\" | Coffee Break | Open-Door Roundtable (Organization Committee) 11:20 AM–12:30 PM TBA | Lunch Break | OPEN TO PUBLIC: Half-Day Tutorials, Part I(Two Parallel Tracks: Low-Dimensional Structure in Deep Networks; Inverse Problems) 1:30 PM–3:40 PM TBA | Coffee Break | OPEN TO PUBLIC: Half-Day Tutorials, Part II(Two Parallel Tracks: Low-Dimensional Structure in Deep Networks; Inverse Problems) 4:00 PM–6:30 PM TBA | . | ",
    "url": "/",
    
    "relUrl": "/"
  },"8": {
    "doc": "Home",
    "title": "Day 5 (Jan 7)Sunday",
    "content": ". | Social Event / Tour / Etc. (TBA) 10:00 AM–2:00 PM TBA | . | . ",
    "url": "/",
    
    "relUrl": "/"
  },"9": {
    "doc": "Home",
    "title": " Conference Sponsors ",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"10": {
    "doc": "Home",
    "title": "Local Host and Main Sponsor",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"11": {
    "doc": "Home",
    "title": "Platinum Sponsor",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"12": {
    "doc": "Home",
    "title": "Gold Sponsors",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"13": {
    "doc": "Home",
    "title": "Silver Sponsors",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"14": {
    "doc": "Home",
    "title": "Rising Star Award Sponsor",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"15": {
    "doc": "Home",
    "title": "Yoga Sponsor",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"16": {
    "doc": "Home",
    "title": "Home",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU The Conference on Parsimony and Learning (CPAL) is an annual research conference focused on addressing the parsimonious, low dimensional structures that prevail in machine learning, signal processing, optimization, and beyond. We are interested in theories, algorithms, applications, hardware and systems, as well as scientific foundations for learning with parsimony. CPAL 2024 will take place from January 3–6 at the University of Hong Kong! . ",
    "url": "/",
    
    "relUrl": "/"
  },"17": {
    "doc": "Advisory Committee",
    "title": "Advisory Committee",
    "content": "Anima Anandkumar . Caltech / NVIDIA . Advisory Committee . Emmanuel Candès . Stanford . Advisory Committee . Alex Dimakis . UT Austin . Advisory Committee . Michael Elad . Technion . Advisory Committee . Yi Ma . UC Berkeley / HKU IDS . Advisory Committee . Peyman Milanfar . Google Research . Advisory Committee . Stefano Soatto . UCLA . Advisory Committee . Rebecca Willett . UChicago . Advisory Committee . Eric P. Xing . MBZUAI / Carnegie Mellon University . Advisory Committee . Tong Zhang . HKUST . Advisory Committee . * Ordered alphabetically . ",
    "url": "/advisory/#advisory-committee",
    
    "relUrl": "/advisory/#advisory-committee"
  },"18": {
    "doc": "Advisory Committee",
    "title": "Advisory Committee",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/advisory/",
    
    "relUrl": "/advisory/"
  },"19": {
    "doc": "Area Chairs",
    "title": "Area Chairs",
    "content": "Navid Azizan . MIT . Area Chair . Chenglong Bao . Tsinghua University . Area Chair . Babak Ehteshami Bejnordi . Qualcomm . Area Chair . Beidi Chen . Meta / Carnegie Mellon University . Area Chair . Tianlong Chen . UT Austin / MIT . Area Chair . Tianyi Chen . RPI . Area Chair . Yubei Chen . UC Davis . Area Chair . Yuxin Chen . UPenn . Area Chair . Sarah Dean . Cornell University . Area Chair . Ivan Dokmanić . University of Basel . Area Chair . Simon Du . University of Washington . Area Chair . Utku Evci . Google DeepMind . Area Chair . Paris Giampouras . Johns Hopkins University . Area Chair . Wei Hu . University of Michigan . Area Chair . Souvik Kundu . Intel Labs, USA . Area Chair . Qi Lei . NYU . Area Chair . Xiao Li . CUHK Shenzhen . Area Chair . Wenjing Liao . Georgia Institute of Technology . Area Chair . Jiaqi Ma . Harvard / UIUC . Area Chair . Elena Mocanu . University of Twente . Area Chair . Vidya K. Muthukumar . Georgia Institute of Technology . Area Chair . Greg Ongie . Marquette University . Area Chair . Laixi Shi . Caltech . Area Chair . Mahdi Soltanolkotabi . USC . Area Chair . Weijie Su . UPenn . Area Chair . Soledad Villar . Johns Hopkins University . Area Chair . Peng Wang . University of Michigan . Area Chair . Yu-Xiang Wang . UC Santa Barbara . Area Chair . Bihan Wen . Nanyang Technological University . Area Chair . Fanny Yang . ETH Zurich . Area Chair . Tianbao Yang . Texas A&amp;M . Area Chair . Yuqian Zhang . Rutgers University . Area Chair . * Ordered alphabetically . ",
    "url": "/area_chairs/",
    
    "relUrl": "/area_chairs/"
  },"20": {
    "doc": "Call for Papers",
    "title": "Call for Papers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/cfp/",
    
    "relUrl": "/cfp/"
  },"21": {
    "doc": "Code of Conduct",
    "title": "Code of Conduct",
    "content": "(Adapted from ICML/ICLR/NeurIPS/LoG) . We strive to hold a conference in which any person can meaningfully participate in the CPAL community through: sharing ideas, presenting their work, meeting members of the community, learning from other people’s work, and discussing ways to improve the community. This conference will work towards preventing any type of discrimination based on age, disability, ethnicity, experience in the field, gender identity, nationality, physical appearance, race, religion, sexual orientation, or other protected characteristics. We strictly prohibit any actions that may prevent the participation of any attendee, including: bullying, harassment, inappropriate media, offensive language, violence, zoom bombing, and so on. Please report any violations of the code of conduct to the conference organizers, via email, slack, or any other channels. If requested, we can handle these reports anonymously, to protect the person making the report. Violators of the code of conduct will be asked to stop their inappropriate behavior, and may be removed from their right to participate in the conference. Further disciplinary action such as bans from future iterations of the conference may be taken. ",
    "url": "/code_of_conduct/#code-of-conduct",
    
    "relUrl": "/code_of_conduct/#code-of-conduct"
  },"22": {
    "doc": "Code of Conduct",
    "title": "Code of Conduct",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/code_of_conduct/",
    
    "relUrl": "/code_of_conduct/"
  },"23": {
    "doc": "Key Dates",
    "title": "Key Dates and Deadlines",
    "content": "Unless specified otherwise, all deadlines are 23:59 Anywhere-on-Earth (AOE) . ",
    "url": "/deadlines/#key-dates-and-deadlines",
    
    "relUrl": "/deadlines/#key-dates-and-deadlines"
  },"24": {
    "doc": "Key Dates",
    "title": "Conference Submission (Proceedings Track)",
    "content": "| Event | Date | Countdown | . | Submission Deadline | August 28th, 2023 | | . | Reviews Released, Rebuttal Stage Begins | October 16th, 2023 | | . | Rebuttal Stage Ends, Author-Reviewer Discussion Begins | October 27th, 2023 | | . | Author-Reviewer Discussion Ends | November 5th, 2023 | | . | Final Decisions Released | November 20th, 2023 | | . | Camera-Ready Deadline | December 5th, 2023 | | . ",
    "url": "/deadlines/#conference-submission-proceedings-track",
    
    "relUrl": "/deadlines/#conference-submission-proceedings-track"
  },"25": {
    "doc": "Key Dates",
    "title": "Conference Submission (Recent Spotlight Track)",
    "content": "| Event | Date | Countdown | . | Submission Deadline | October 10th, 2023 | | . | Final Decisions Released | November 20th, 2023 | | . | Camera-Ready Deadline | December 5th, 2023 | | . ",
    "url": "/deadlines/#conference-submission-recent-spotlight-track",
    
    "relUrl": "/deadlines/#conference-submission-recent-spotlight-track"
  },"26": {
    "doc": "Key Dates",
    "title": "Rising Stars Award",
    "content": "| Event | Date | Countdown | . | Application Deadline | October 1st, 2023 | | . | Decisions Released | October 23th, 2023 | | . ",
    "url": "/deadlines/#rising-stars-award",
    
    "relUrl": "/deadlines/#rising-stars-award"
  },"27": {
    "doc": "Key Dates",
    "title": "Main Conference",
    "content": "| Event | Date | . | Registration Deadline | December 15th, 2023 (HKT) | . | Conference Program | January 3rd - January 6th, 2024 | . ",
    "url": "/deadlines/#main-conference",
    
    "relUrl": "/deadlines/#main-conference"
  },"28": {
    "doc": "Key Dates",
    "title": "Key Dates",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/deadlines/",
    
    "relUrl": "/deadlines/"
  },"29": {
    "doc": "Travel: Hotels",
    "title": "Recommended Hotels",
    "content": "The CPAL 2024 local hosts, the University of Hong Kong Musketeers Foundation Institute of Data Science, have prepared a list of recommended hotels near to the conference site for CPAL 2024 attendees. Please reference the link below to the HKU-IDS website for more information. Accommodation Recommendations . ",
    "url": "/hotels/#recommended-hotels",
    
    "relUrl": "/hotels/#recommended-hotels"
  },"30": {
    "doc": "Travel: Hotels",
    "title": "Travel Questions",
    "content": "For further questions about local accommodations, please contact: . Conference Secretaries at the HKU Musketeers Foundation Institute of Data Science Email: cpal-info@hku.hk Tel: +852 3910 2323 Address: Room P307, Graduate House, No. 3 University Drive, The University of Hong Kong, Pokfulam, Hong Kong . ",
    "url": "/hotels/#travel-questions",
    
    "relUrl": "/hotels/#travel-questions"
  },"31": {
    "doc": "Travel: Hotels",
    "title": "Travel: Hotels",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/hotels/",
    
    "relUrl": "/hotels/"
  },"32": {
    "doc": "Organization Committee",
    "title": "Organization Committee",
    "content": " ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"33": {
    "doc": "Organization Committee",
    "title": "General Chairs",
    "content": "Gitta Kutyniok . LMU Munich . General Chair . Yi Ma . UC Berkeley / HKU IDS . General Chair . Harry Shum . HKUST / IDEA . General Chair . René Vidal . UPenn . General Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"34": {
    "doc": "Organization Committee",
    "title": "Program Chairs",
    "content": "Yuejie Chi . Carnegie Mellon University . Program Chair . Gintare Karolina Dziugaite . Google DeepMind . Program Chair . Qing Qu . University of Michigan . Program Chair . Atlas Wang . UT Austin . Program Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"35": {
    "doc": "Organization Committee",
    "title": "Local Chairs",
    "content": "Yanchao Yang . HKU . Local Chair . Man-Chung Yue . HKU . Local Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"36": {
    "doc": "Organization Committee",
    "title": "Publication Chairs",
    "content": "Zhihui Zhu . Ohio State University . Publication Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"37": {
    "doc": "Organization Committee",
    "title": "Industry Liaison Chairs",
    "content": "Dan Alistarh . IST Austria / Neural Magic . Industry Liaison Chair . Dilin Wang . Meta . Industry Liaison Chair . Chong You . Google Research . Industry Liaison Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"38": {
    "doc": "Organization Committee",
    "title": "Panel Chairs",
    "content": "Sijia Liu . Michigan State University . Panel Chair . Jeremias Sulam . Johns Hopkins University . Panel Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"39": {
    "doc": "Organization Committee",
    "title": "Tutorial Chairs",
    "content": "Saiprasad Ravishankar . Michigan State University . Tutorial Chair . John Wright . Columbia University . Tutorial Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"40": {
    "doc": "Organization Committee",
    "title": "Publicity Chairs",
    "content": "Yani Ioannou . University of Calgary . Publicity Chair . William T. Redman . UCSB . Publicity Chair . Liyue Shen . University of Michigan . Publicity Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"41": {
    "doc": "Organization Committee",
    "title": "Rising Stars Award Chairs",
    "content": "Yubei Chen . UC Davis . Rising Stars Award Chair . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"42": {
    "doc": "Organization Committee",
    "title": "Web Chairs",
    "content": "Sam Buchanan . TTIC . Web Chair . * Ordered alphabetically . ",
    "url": "/organization_committee/",
    
    "relUrl": "/organization_committee/"
  },"43": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"44": {
    "doc": "Conference Program",
    "title": "Conference Program",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/program/",
    
    "relUrl": "/program/"
  },"45": {
    "doc": "Program at a Glance",
    "title": "Program at a Glance",
    "content": "All times below are in HKT (GMT+8). | 08:00 AM | 08:30 AM | 09:00 AM | 09:30 AM | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 01:00 PM | 01:30 PM | 02:00 PM | 02:30 PM | 03:00 PM | 03:30 PM | 04:00 PM | 04:30 PM | 05:00 PM | 05:30 PM | 06:00 PM | 06:30 PM | 07:00 PM | 07:30 PM | 08:00 PM | 08:30 PM | 09:00 PM | . | ",
    "url": "/program_schedule/#program-at-a-glance",
    
    "relUrl": "/program_schedule/#program-at-a-glance"
  },"46": {
    "doc": "Program at a Glance",
    "title": "Day 1 (Jan 3)Wednesday",
    "content": ". | Registration 8:00 AM–9:15 AM TBA | Opening Remarks 9:15 AM–10:00 AM TBA | Stefano Soatto 10:00 AM–11:00 AM TBA \"Representation and Control of Meanings in Large Language Models and Multimodal Foundation Models\" | Coffee Break | Rising Stars Session 1 11:20 AM–12:30 PM TBA | Lunch Break | Maryam Fazel 1:30 PM–2:30 PM TBA \"Flat Minima and Generalization in Learning: The Case of Low-rank Matrix Recovery\" | Oral Session 1 2:30 PM–3:40 PM TBA | Coffee Break | Dan Alistarh 4:00 PM–5:00 PM TBA \"Accurate Model Compression at GPT Scale\" | Reception 5:00 PM–6:30 PM TBA | . | ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"47": {
    "doc": "Program at a Glance",
    "title": "Day 2 (Jan 4)Thursday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Jong Chul Ye 9:00 AM–10:00 AM TBA \"Scaling Diffusion Model for Inverse Problems Beyond 2D and Known Forward Models\" | Rising Stars Session 2 10:00 AM–11:00 AM TBA | Coffee Break | Oral Session 2 11:20 AM–12:30 PM TBA | Lunch Break | Kostas Daniilidis 1:30 PM–2:30 PM TBA \"Parsimony through Equivariance\" | Oral Session 3 2:30 PM–4:00 PM TBA | Coffee Break | Panel Discussion 4:10 PM–5:00 PM TBA | Spotlight Poster Session 1 5:00 PM–6:30 PM TBA | Banquet 7:00 PM–9:00 PM TBA | . | ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"48": {
    "doc": "Program at a Glance",
    "title": "Day 3 (Jan 5)Friday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Yingbin Liang 9:00 AM–10:00 AM TBA \"In-Context Convergence of Transformers\" | Rising Stars Session 3 10:00 AM–11:00 AM TBA | Coffee Break | Oral Session 4 11:20 AM–12:30 PM TBA | Lunch Break | Dimitris Papailiopoulos 1:30 PM–2:30 PM TBA \"Teaching arithmetic to small language models\" | Oral Session 5 2:30 PM–3:40 PM TBA | Coffee Break | Oral Session 6 4:00 PM–5:00 PM TBA | Spotlight Poster Session 2 5:00 PM–6:30 PM TBA | Tram Tour (Tentative) 7:00 PM–9:00 PM TBA | . | ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"49": {
    "doc": "Program at a Glance",
    "title": "Day 4 (Jan 6)Saturday",
    "content": ". | Yoga Meditation 8:30 AM–9:00 AM TBA | Tom Goldstein 9:00 AM–10:00 AM TBA \"Statistical methods for addressing safety and security issues of generative models\" | SueYeon Chung 10:00 AM–11:00 AM TBA \"Multi-level theory of neural representations: Capacity of neural manifolds in biological and artificial neural networks\" | Coffee Break | Open-Door Roundtable (Organization Committee) 11:20 AM–12:30 PM TBA | Lunch Break | OPEN TO PUBLIC: Half-Day Tutorials, Part I(Two Parallel Tracks: Low-Dimensional Structure in Deep Networks; Inverse Problems) 1:30 PM–3:40 PM TBA | Coffee Break | OPEN TO PUBLIC: Half-Day Tutorials, Part II(Two Parallel Tracks: Low-Dimensional Structure in Deep Networks; Inverse Problems) 4:00 PM–6:30 PM TBA | . | ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"50": {
    "doc": "Program at a Glance",
    "title": "Day 5 (Jan 7)Sunday",
    "content": ". | Social Event / Tour / Etc. (TBA) 10:00 AM–2:00 PM TBA | . | . ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"51": {
    "doc": "Program at a Glance",
    "title": "Program at a Glance",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/program_schedule/",
    
    "relUrl": "/program_schedule/"
  },"52": {
    "doc": "Register & Attend",
    "title": "Register & Attend",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/register_and_attend/",
    
    "relUrl": "/register_and_attend/"
  },"53": {
    "doc": "Registration",
    "title": "Registration",
    "content": "All CPAL attendees are required to register to attend the conference. Please complete your registration at the HKU website, where more details (cost, options) are available. The deadline to register is December 15th, 2023. Registration Link . ",
    "url": "/registration/#registration",
    
    "relUrl": "/registration/#registration"
  },"54": {
    "doc": "Registration",
    "title": "Registration",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/registration/",
    
    "relUrl": "/registration/"
  },"55": {
    "doc": "Review Guidelines",
    "title": "Reviewer Guidelines",
    "content": " ",
    "url": "/review_guidelines/#reviewer-guidelines",
    
    "relUrl": "/review_guidelines/#reviewer-guidelines"
  },"56": {
    "doc": "Review Guidelines",
    "title": "Notable Innovations in Our Review Mechanism",
    "content": "CPAL strives for providing every paper with high-quality, accountable reviews, and therefore takes the following actions in addition: . | Shepherding by an Action PC: Every paper’s final decision, after being recommended by AC, will go through the direct shepherding of all program chairs (led by one “action PC”). The action PC has two main duties: . | (before final decision released) The action PC will pay particular attention to the borderline cases and the dispute (large score variations) cases, and will be asked to write additional “meta-meta reviews” in those cases and potentially calibrate on top of AC recommendations. Final decisions will be scrutinized and made in a joint meeting by all program chairs. | (after receiving the camera-ready) Each accepted paper’s authors will be asked to submit a one-page cover letter, summarizing what revisions are made between the paper’s submitted and camera-ready versions. The action PC will ensure: (1) all “promised” changes by authors during the discussion stage are indeed implemented; (2) no change that is “too substantial” and “unsoliciated” shall be made to the paper, unless in exceptional circumstances where the action PC has to approve case-by-case. The action PC reserves the right to reject a camera-ready submission and exclude it from the conference proceedings. | . | Semi-Open Identity for Accountability (Action PC and/or AC): For every accepted paper, the names of its AC and action PC will be publicly released on its OpenReview page too. For every rejected paper (excluding withdrawals), only the name of its action PC will be displayed. This decision was not reached lightly; but we hope it would meaningfully add credibility and accountability for every paper’s final outcome. | Reviewer Rating and “Dynamic Sparse Selection”: each AC will be asked to rate every reviewer in their batch, in terms of timeliness and quality. Program chairs, who know all reviewers’ identities, will compile a list of reviewers sorted by their average ratings received. Reviewers that receive consistent low reviewer rating for multiple papers / from multiple ACs will be excluded from future review processes. | . ",
    "url": "/review_guidelines/#notable-innovations-in-our-review-mechanism",
    
    "relUrl": "/review_guidelines/#notable-innovations-in-our-review-mechanism"
  },"57": {
    "doc": "Review Guidelines",
    "title": "General Guidelines",
    "content": "We all like the Acceptance Criteria made by TMLR https://jmlr.org/tmlr/acceptance-criteria.html and would instruct our ACs and reviewers to honor the same. In particular we note: . “Crucially, it should not be used as a reason to reject work that isn’t considered “significant” or “impactful” because it isn’t achieving a new state-of-the-art on some benchmark. Nor should it form the basis for rejecting work on a method considered not “novel enough”, as novelty of the studied method is not a necessary criteria for acceptance. We explicitly avoid these terms (“significant”, “impactful”, “novel”), and focus instead on the notion of “interest”. If the authors make it clear that there is something to be learned by some researchers in their area from their work, then the criteria of interest is considered satisfied. ",
    "url": "/review_guidelines/#general-guidelines",
    
    "relUrl": "/review_guidelines/#general-guidelines"
  },"58": {
    "doc": "Review Guidelines",
    "title": "Review Guidelines",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/review_guidelines/",
    
    "relUrl": "/review_guidelines/"
  },"59": {
    "doc": "Rising Stars Award",
    "title": "Rising Stars Award",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/rising_stars/",
    
    "relUrl": "/rising_stars/"
  },"60": {
    "doc": "Awardees",
    "title": "CPAL Rising Stars Awardees",
    "content": "CPAL is excited to announce the CPAL Rising Stars awardees for 2024. The awardees showcase expertise in fields such as machine learning, applied mathematics, signal processing, optimization, systems, and more interdisciplinary fields. Together, they exemplify outstanding research potential for the CPAL community. CPAL Rising Stars will attend the CPAL conference in person and present a short overview of their research. Awardees are listed below, along with their planned presentation information. ",
    "url": "/rising_stars_awardees/#cpal-rising-stars-awardees",
    
    "relUrl": "/rising_stars_awardees/#cpal-rising-stars-awardees"
  },"61": {
    "doc": "Awardees",
    "title": "List of Awardees",
    "content": "Lijun Ding . University of Wisconsin / University of Washington . IFDS Postdoctoral Researcher . Title: Optimization for statistical learning with low dimensional structure: regularity and conditioning . Abstract: Many statistical machine learning problems, where one aims to recover an underlying low-dimensional signal, are based on optimization. Existing work often overlooked the computational complexity in solving the optimization problem, or required case-specific algorithm and analysis – especially for nonconvex problems. This talk addresses the above two issues from a unified perspective of conditioning. In particular, we show that once the sample size exceeds the intrinsic dimension, (1) a broad class of convex and nonsmooth nonconvex problems are well-conditioned, (2) well conditioning in turn ensures the efficiency of out-of-box optimization methods and inspires new algorithms. Lastly, we show that a conditioning notion called flatness leads to accurate recovery in overparametrized models. Ningyuan Huang . Johns Hopkins University . Ph.D. Student . Title: Approximately Equivariant Graph Networks . Abstract: Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to active symmetries, whereas the permutation equivariance of GNNs corresponds to passive symmetries. In this talk, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We propose approximately equivariant graph networks to implement these symmetries and investigate the symmetry model selection problem. We theoretically and empirically show a bias-variance tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. Daniel Paul Kunin . Stanford University . Ph.D. Student . Title: Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks . Abstract: In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler (sparse or low-rank) subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape’s curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization. Daniel LeJeune . Stanford University . Postdoctoral Researcher . Title: Emergent properties of heuristics in machine learning . Abstract: Successful methods in modern machine learning practice are built on solid intuition and theoretical insight by their designers, but are often ultimately heuristic and exhibit unintended emergent behaviors. Sometimes these emergent behaviors are detrimental, but surprisingly, many provide unexpected desirable benefits. By theoretically characterizing these emergent behaviors, we can develop a more robust methods development process, where more and more of these desirable behaviors can be included by design and leveraged in powerful ways. I will discuss several examples of heuristics and emergent behavior: subsampling and sketching in linear regression and their equivalence to ridge regularization; empirical risk minimization and the universality of relative performances under distribution shifts; and adaptivity in dropout and feature learning models which are equivalent to parsimony-promoting sparse or low-rank regularization. Shuang Li . Iowa State University . Assistant Professor . Title: The Future Geometric Analysis of Optimization Problems in Signal Processing and Machine Learning . Abstract: High-dimensional data analysis and estimation appear in many signal processing and machine learning applications. The underlying low-dimensional structure in these high-dimensional data inspires us to develop optimality guarantees as well as optimization-based techniques for the fundamental problems in signal processing and machine learning. In recent years, non-convex optimization widely appears in engineering and is solved by many heuristic local algorithms, but lacks global guarantees. The recent geometric/landscape analysis provides a way to determine whether an iterative algorithm can reach global optimality. The landscape of empirical risk has been widely studied in a series of machine learning problems, including low-rank matrix factorization, matrix sensing, matrix completion, and phase retrieval. A favorable geometry guarantees that many algorithms can avoid saddle points and converge to local minima. In this presentation, I will discuss potential directions for the future geometric analysis of optimization problems in signal processing and machine learning. Shiwei Liu . University of Texas at Austin / Eindhoven University of Technology / University of Oxford . IFML Postdoctoral Researcher . Title: Sparsity in Neural Networks: Science and Practice . Abstract: Sparsity has demonstrated its remarkable performance in the realm of model compression through the selectively eliminating a large portion of model parameters. Nevertheless, conventional methods to discover strong sparse neural networks often necessitate the training of an over-parameterized dense model, followed by iterative cycles of pruning and re-training. As the size of modern neural networks exponentially increases, the costs of dense pre-training and updates have become increasingly prohibitive. In this talk, I will introduce an approach that enables the training of sparse neural networks from scratch, without the need for any pre-training steps or dense updates. By achieving the property of over-parameterization in time, our approach demonstrates the capacity to achieve performance levels equivalent to fully dense networks while utilizing only a very small fraction of weights. Beyond the advantages in model compression, I will also elucidate a broader spectrum of benefits of sparsity in neural networks including scalability, robustness, and fairness, and great potentials build large-scale responsible AI. Yiping Lu . New York University . Courant Instructor . Title: Simulation-Calibrated Scientific Machine Learning . Abstract: Machine learning (ML) has achieved great success in a variety of applications suggesting a new way to build flexible, universal, and efficient approximators for complex high-dimensional data. These successes have inspired many researchers to apply ML to other scientific applications such as industrial engineering, scientific computing, and operational research, where similar challenges often occur. However, the luminous success of ML is overshadowed by persistent concerns that the mathematical theory of large-scale machine learning, especially deep learning, is still lacking and the trained ML predictor is always biased. In this talk, I’ll introduce a novel framework of (S)imulation-(Ca)librated (S)cientific (M)achine (L)earning (SCaSML), which can leverage the structure of physical models to achieve the following goals: 1) make unbiased predictions even based on biased machine learning predictors; 2) beat the curse of dimensionality with an estimator suffers from it. The SCASML paradigm combines a (possibly) biased machine learning algorithm with a de-biasing step design using rigorous numerical analysis and stochastic simulation. Theoretically, I’ll try to understand whether the SCaSML algorithms are optimal and what factors (e.g., smoothness, dimension, and boundness) determine the improvement of the convergence rate. Empirically, I’ll introduce different estimators that enable unbiased and trustworthy estimation for physical quantities with a biased machine learning estimator. Applications include but are not limited to estimating the moment of a function, simulating high-dimensional stochastic processes, uncertainty quantification using bootstrap methods, and randomized linear algebra. Omar Montasser . University of California, Berkeley . FODSI-Simons Postdoctoral Researcher . Title: Theoretical Foundations of Adversarially Robust Learning . Abstract: Despite extraordinary progress, current machine learning systems have been shown to be brittle against adversarial examples: seemingly innocuous but carefully crafted perturbations of test examples that cause machine learning predictors to misclassify. Can we learn predictors robust to adversarial examples? and how? There has been much empirical interest in this major challenge in machine learning, and in this talk, we will present a theoretical perspective. We will illustrate the need to go beyond traditional approaches and principles, such as empirical (robust) risk minimization, and present new algorithmic ideas with stronger robust learning guarantees. Ramchandran Muthukumar . Johns Hopkins University . Ph.D. Student . Title: Sparsity-aware generalization theory for deep neural networks . Abstract: Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models. Ambar Pal . Johns Hopkins University . Ph.D. Student . Title: The Role of Parsimonious Structures in Data for Trustworthy Machine Learning . Abstract: This talk overviews recent theoretical results in the geometric foundations of adversarially robust machine learning. Modern ML classifiers can fail spectacularly when subject to specially crafted input-perturbations, called adversarial examples. On the other hand, we humans are quite robust for several tasks involving vision. Motivated by this disconnect, in the first part of this talk we will take a deeper dive into the question of when exactly we can avoid adversarial examples. We will see that a key geometric property of the data-distribution — concentration on small-volume subsets of the input space — characterizes whether any robust classifier exists. In particular, this suggests that natural image distributions are concentrated. In the second part of this talk, we will empirically instantiate these results for a few concentrated data-distributions, and discover that utilizing such structure in data leads to classifiers that enjoy better provable robustness guarantees in several regimes. This talk is based on work at NeurIPS ’23, ’20 and TMLR ’23. Rahul Parhi . École Polytechnique Fédérale de Lausanne . Postdoctoral Researcher . Title: On the Sparsity-Promoting Effect of Weight Decay in Deep Learning . Abstract: Deep learning has been wildly successful in practice and most state-of-the-art artificial intelligence systems are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this talk, I present a new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of trained neural networks through the lens of sparsity. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory. This framework explains the effect of weight decay regularization in neural network training, the importance of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networks can perform well in high-dimensional problems. Bahareh Tolooshams . California Institute of Technology . Postdoctoral Researcher . Title: Deep Interpretable Generative Learning for Science and Engineering . Abstract: Discriminative and generative AI are two deep learning paradigms that revolutionized prediction and generation of high-quality images from text prompts. Nonetheless, discriminative learning is unable to generate data, and deep generative models struggle with decoding capabilities. Moreover, both approaches are data-hungry and have low interpretability. These drawbacks have posed significant barriers to the adoption of deep learning in applications where a) acquiring supervised data is expensive or infeasible, and b) goals extend beyond data fitting to attain scientific insights. Furthermore, deep learning applications are fairly unexplored in fields with rich mathematical and optimization frameworks such as inverse problems, or those in which interpretability matters. This talk discusses the theory and applications of deep learning in data-limited or unsupervised inverse problems. These include applications in radar sensing, Poisson image denoising, and computational neuroscience. Hongyi Wang . Carnegie Mellon University . Senior Project Scientist . Title: Speeding up Large-Scale Machine Learning Model Development Using Low-Rank Models and Gradients . Abstract: Large-scale machine learning (ML) models, such as GPT-4 and Llama2, are at the forefront of advances in the field of AI. Nonetheless, developing these large-scale ML models demands substantial computational resources and a deep understanding of distributed ML and systems. In this presentation, I will introduce three frameworks, namely ATOMO, Pufferfish, and Cuttlefish, which use low-rank approximations on model gradients and model weights to significantly expedite ML model training. ATOMO is a general compression framework that has experimentally established that using low-rank gradients, as opposed to sparse ones, can lead to substantially faster distributed training. Pufferfish further bypasses the cost of compression by directly training low-rank models. However, directly training low-rank models usually leads to a loss in accuracy. Pufferfish mitigates this issue by training a full-rank model and then converting to a low-rank model early in the training process. Nonetheless, Pufferfish necessitates extra hyperparameter tuning, such as determining the optimal transition time from full-rank to low-rank. Cuttlefish addresses this issue by automatically estimating and adjusting these hyperparameters during training. I will present extensive experimental results on the distributed training of large-scale ML models, including LLMs, to demonstrate the efficacy of these frameworks. Peng Wang . University of Michigan . Postdoctoral Researcher . Title: Understanding Hierarchical Representations in Deep Networks via Intermediate Features . Abstract: Over the past decade, deep learning has proven to be a highly effective method for learning meaningful features from raw data. This work attempts to unveil the mystery of hierarchical feature learning in deep networks. Specifically, in the context of multi-class classification problems, we explore how deep networks transform input data by investigating the output (i.e., features) of each layer after training. Towards this goal, we first define metrics for within-class compression and between-class discrimination of intermediate features, respectively. Through an analysis of these two metrics, we show that the evolution of features follows a simple and quantitative law from shallow to deep layers: Each layer of linear networks progressively compresses within-class features at a linear rate and discriminates between-class features at a sublinear rate. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep networks. Moreover, our extensive experiments validate our theoretical findings numerically. Yaodong Yu . University of California, Berkeley . Ph.D. Student . Title: White-Box Transformers via Sparse Rate Reduction . Abstract: In this talk, I will present the white-box transformer — CRATE (i.e., Coding RAte reduction TransformEr). We contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing lossy coding rate. This leads to a family of white-box transformer architectures which are mathematically interpretable. Our experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers (ViTs). I will also present some recent theoretical and empirical results of CRATE on emergence behavior, language modeling, and auto-encoding. Ravid Shwartz Ziv . New York University . CDS Faculty Fellow . Title: Decoding the Information Bottleneck in Self-Supervised Learning: Pathway to Optimal Representation . Abstract: Deep Neural Networks (DNNs) have excelled in many fields, largely due to their proficiency in supervised learning tasks. However, the dependence on vast labeled data becomes a constraint when such data is scarce. Self-Supervised Learning (SSL), a promising approach, harnesses unlabeled data to derive meaningful representations. Yet, how SSL filters irrelevant information without explicit labels remains unclear. In this talk, we aim to unravel the enigma of SSL using the lens of Information Theory, with a spotlight on the Information Bottleneck principle. This principle, while providing a sound understanding of the balance between compressing and preserving relevant features in supervised learning, presents a puzzle when applied to SSL due to the absence of labels during training. We will delve into the concept of ‘optimal representation’ in SSL, its relationship with data augmentations, optimization methods, and downstream tasks, and how SSL training learns and achieves optimal representations. Our discussion unveils our pioneering discoveries, demonstrating how SSL training naturally leads to the creation of optimal, compact representations that correlate with semantic labels. Remarkably, SSL seems to orchestrate an alignment of learned representations with semantic classes across multiple hierarchical levels, an alignment that intensifies during training and grows more defined deeper into the network. Considering these insights and their implications for class set performance, we conclude our talk by applying our analysis to devise more robust SSL-based information algorithms. These enhancements in transfer learning could lead to more efficient learning systems, particularly in data-scarce environments. ",
    "url": "/rising_stars_awardees/#list-of-awardees",
    
    "relUrl": "/rising_stars_awardees/#list-of-awardees"
  },"62": {
    "doc": "Awardees",
    "title": "Awardees",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/rising_stars_awardees/",
    
    "relUrl": "/rising_stars_awardees/"
  },"63": {
    "doc": "Application",
    "title": "CPAL Rising Stars Award",
    "content": "The Conference on Parsimony and Learning (CPAL) launches the Rising Stars Award program to highlight exceptional junior researchers at a critical inflection and starting point in their career: last-year PhD students, postdoctoral scholars, first-year tenure track faculty, or industry researcher within two years of graduation. CPAL is an annual research conference focused on addressing the parsimonious, low dimensional structures that prevail in machine learning, signal processing, optimization, and beyond. In its first year, CPAL Rising Stars Award program will provide PhD students, postdocs, junior faculties and industry researchers the opportunity to plug into these networks, platforms, and opportunities. The program also aims to increase representation and diversity in this area by providing a platform and a supportive mentoring network to navigate academic careers. All graduate students and postdocs, including those who belong to groups underrepresented, are encouraged to apply, including but not limited to people of all racial, ethnic, geographic, and socioeconomic backgrounds, sexual orientations, genders, and persons with disabilities. The application deadline for the CPAL Rising Stars award has now passed. ",
    "url": "/rising_stars_guidelines/#cpal-rising-stars-award",
    
    "relUrl": "/rising_stars_guidelines/#cpal-rising-stars-award"
  },"64": {
    "doc": "Application",
    "title": "Key Dates",
    "content": ". | Applications Due: October 1st, 2023 | Notification Deadline: October 23rd, 2023 | Conference: January 3-6, 2024 at The University of Hong Kong (HKU) (in person) | . ",
    "url": "/rising_stars_guidelines/#key-dates",
    
    "relUrl": "/rising_stars_guidelines/#key-dates"
  },"65": {
    "doc": "Application",
    "title": "Program Format",
    "content": ". | Dedicated poster session for selected awardees | Panels (career development) | Roundtable dinners or 1-1 meetings with senior researchers | Selected awardees need to confirm in-person attendance and will be supported with travel funding to attend CPAL 2023 conference at HKU | . ",
    "url": "/rising_stars_guidelines/#program-format",
    
    "relUrl": "/rising_stars_guidelines/#program-format"
  },"66": {
    "doc": "Application",
    "title": "Application Requirements",
    "content": ". | Name &amp; Contact | Resume/CV | Tentative Poster Title | Research statement outlining research goals, potential projects of interest, and long-term career goals, and commitment to creating a more diverse and inclusive scientific community (1 page, standard font at a size 11 or larger) | List names of 1-2 references with emails | . ",
    "url": "/rising_stars_guidelines/#application-requirements",
    
    "relUrl": "/rising_stars_guidelines/#application-requirements"
  },"67": {
    "doc": "Application",
    "title": "Eligibility and Guidelines",
    "content": ". | Applicants must be full time graduate students in their last year of obtaining a PhD, or current postdoctoral scholars/fellows, first-year tenure track faculty, or industry researchers within two years of graduation | We welcome applicants from a wide variety of fields and backgrounds: any eligible PhD or postdoc or junior faculty or junior industry researchers, who are engaging in addressing the parsimonious, low dimensional structures that prevail in machine learning, signal processing, optimization, systems, interdisciplinary applications and beyond are encouraged to apply. | Applicants from all institutions worldwide are encouraged to apply. | An applicant may only submit one application. | . ",
    "url": "/rising_stars_guidelines/#eligibility-and-guidelines",
    
    "relUrl": "/rising_stars_guidelines/#eligibility-and-guidelines"
  },"68": {
    "doc": "Application",
    "title": "Review Criteria",
    "content": "Proposals will be reviewed by the CPAL Rising Stars Program Committee (chaired by Dr. Yubei Chen) based on research impact, academic progress (if applicable), career potential, and commitment to broadening participation. ",
    "url": "/rising_stars_guidelines/#review-criteria",
    
    "relUrl": "/rising_stars_guidelines/#review-criteria"
  },"69": {
    "doc": "Application",
    "title": "Contact",
    "content": "Please email ybchen@ucdavis.edu with questions. ",
    "url": "/rising_stars_guidelines/#contact",
    
    "relUrl": "/rising_stars_guidelines/#contact"
  },"70": {
    "doc": "Application",
    "title": "Application",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/rising_stars_guidelines/",
    
    "relUrl": "/rising_stars_guidelines/"
  },"71": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Clicking a speaker’s photo will jump to their talk information below. <!-- ",
    "url": "/speakers/#keynote-speakers",
    
    "relUrl": "/speakers/#keynote-speakers"
  },"72": {
    "doc": "Keynote Speakers",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . Institute of Science and Technology Austria / Neural Magic . SueYeon Chung . New York University / Flatiron Institute . Kostas Daniilidis . University of Pennsylvania . Maryam Fazel . University of Washington . Tom Goldstein . University of Maryland . Yingbin Liang . Ohio State University . Dimitris Papailiopoulos . University of Wisconsin-Madison . Stefano Soatto . University of California, Los Angeles . Jong Chul Ye . Korea Advanced Institute of Science and Technology (KAIST) . ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"73": {
    "doc": "Keynote Speakers",
    "title": "Talk Details",
    "content": "Dan Alistarh . Institute of Science and Technology Austria / Neural Magic . Title: Accurate Model Compression at GPT Scale . Time and Location: Day 1, 4:00 PM HKT, TBA . Abstract . A key barrier to the wide deployment of highly-accurate machine learning models, whether for language or vision, is their high computational and memory overhead. Although we possess the mathematical tools for highly-accurate compression of such models, these theoretically-elegant techniques require second-order information of the model’s loss function, which is hard to even approximate efficiently at the scale of billion-parameter models.In this talk, I will describe our work on bridging this computational divide, which enables the accurate second-order pruning and quantization of models at truly massive scale. Compressed using our techniques, models with billions and even trillions of parameters can be executed efficiently on a few GPUs, with significant speedups, and negligible accuracy loss. Based in part on our work, the community has been able to run accurate billion or even trillion-parameter models on computationally-limited devices. Bio . Dan Alistarh is a Professor at IST Austria, in Vienna. Previously, he was a Researcher with Microsoft, a Postdoc at MIT CSAIL, and received his PhD from the EPFL. His research is on algorithms for efficient machine learning and high-performance computing, with a focus on scalable DNN inference and training, for which he was awarded an ERC Starting Grant in 2018. In his spare time, he works with the ML research team at Neural Magic, a startup based in Boston, on making compression faster, more accurate and accessible to practitioners. SueYeon Chung . New York University / Flatiron Institute . Title: Multi-level theory of neural representations: Capacity of neural manifolds in biological and artificial neural networks . Time and Location: Day 4, 10:00 AM HKT, TBA . Abstract . A central goal in neuroscience is to understand how orchestrated computations in the brain arise from the properties of single neurons and networks of such neurons. Answering this question requires theoretical advances that shine a light on the ‘black box’ of representations in neural circuits. In this talk, we will demonstrate theoretical approaches that help describe how cognitive task implementations emerge from the structure in neural populations and from biologically plausible neural networks. We will introduce a new theory that connects geometric structures that arise from neural population responses (i.e., neural manifolds) to the neural representation’s efficiency in implementing a task. In particular, this theory describes how many neural manifolds can be represented (or ‘packed’) in the neural activity space while they can be linearly decoded by a downstream readout neuron. The intuition from this theory is remarkably simple: like a sphere packing problem in physical space, we can encode many “neural manifolds” into the neural activity space if these manifolds are small and low-dimensional, and vice versa. Next, we will describe how such an approach can, in fact, open the ‘black box’ of distributed neuronal circuits in a range of settings, such as experimental neural datasets and artificial neural networks. In particular, our method overcomes the limitations of traditional dimensionality reduction techniques, as it operates directly on the high-dimensional representations. Furthermore, this method allows for simultaneous multi-level analysis, by measuring geometric properties in neural population data and estimating the amount of task information embedded in the same population. Finally, we will discuss our recent efforts to fully extend this multi-level description of neural populations by (1) understanding how task-implementing neural manifolds emerge across brain regions and during learning, (2) investigating how neural tuning properties shape the representation geometry in early sensory areas, and (3) demonstrating the impressive task performance and neural predictivity achieved by optimizing a deep network to maximize the capacity of neural manifolds. By expanding our mathematical toolkit for analyzing representations underlying complex neuronal networks, we hope to contribute to the long-term challenge of understanding the neuronal basis of tasks and behaviors. Bio . SueYeon Chung is an Assistant Professor in the Center for Neural Science at NYU, with a joint appointment in the Center for Computational Neuroscience at the Flatiron Institute, an internal research division of the Simons Foundation. She is also an affiliated faculty member at the Center for Data Science and Cognition and Perception Program at NYU. Prior to joining NYU, she was a Postdoctoral Fellow in the Center for Theoretical Neuroscience at Columbia University, and BCS Fellow in Computation at MIT. Before that, she received a Ph.D. in applied physics at Harvard University, and a B.A. in mathematics and physics at Cornell University. She received the Klingenstein-Simons Fellowship Award in Neuroscience in 2023. Her main research interests lie at the intersection between computational neuroscience and deep learning, with a particular focus on understanding and interpreting neural computation in biological and artificial neural networks by employing methods from neural network theory, statistical physics, and high-dimensional statistics. Kostas Daniilidis . University of Pennsylvania . Title: Parsimony through Equivariance . Time and Location: Day 2, 1:30 PM HKT, TBA . Abstract . Equivariant representations are crucial in various scientific and engineering domains because they encode the inherent symmetries present in physical and biological systems, thereby providing a more natural and efficient way to model them. In the context of machine learning and perception, equivariant representations ensure that the output of a model changes in a predictable way in response to transformations of its input, such as 2D or 3D rotation or scaling. In this talk, we will show a systematic way of how to achieve equivariance by design and how such an approach can yield parsimony in training data and model capacity. Bio . Kostas Daniilidis is the Ruth Yalom Stone Professor of Computer and Information Science at the University of Pennsylvania where he has been faculty since 1998. He is an IEEE Fellow. He was the director of the GRASP laboratory from 2008 to 2013, Associate Dean for Graduate Education from 2012-2016, and Faculty Director of Online Learning from 2013- 2017. He obtained his undergraduate degree in Electrical Engineering from the National Technical University of Athens, 1986, and his PhD (Dr.rer.nat.) in Computer Science from the University of Karlsruhe, 1992, under the supervision of Hans-Hellmut Nagel. He received the Best Conference Paper Award at ICRA 2017. He co-chaired ECCV 2010 and 3DPVT 2006. His most cited works have been on event-based vision, equivariant learning, 3D human pose, and hand-eye calibration. Maryam Fazel . University of Washington . Title: Flat Minima and Generalization in Learning: The Case of Low-rank Matrix Recovery . Time and Location: Day 1, 1:30 PM HKT, TBA . Abstract . Many behaviors observed in deep neural networks still lack satisfactory explanation; e.g., how does an overparameterized neural network avoid overfitting and generalize to unseen data? Empirical evidence suggests that generalization depends on which zero-loss local minimum is attained during training. The shape of the training loss around a local minimum affects the model’s performance: “Flat” minima—around which the loss grows slowly—appear to generalize well. Clarifying this phenomenon helps explain generalization properties, which still largely remain a mystery. In this talk we focus on a simple class of overparameterized nonlinear models, those arising in low-rank matrix recovery. We study several key models: matrix sensing, phase retrieval, robust Principal Component Analysis, covariance matrix estimation, and single hidden layer neural networks with quadratic activation. We prove that in these models, flat minima (measured by average curvature) exactly recover the ground truth under standard statistical assumptions, and we prove weak recovery for matrix completion. These results suggest (i) a theoretical basis for favoring methods that bias iterates towards flat solutions, (ii) use of Hessian trace as a good regularizer. Since the landscape properties we prove are algorithm-agnostic, a future direction is to pair these findings with the analysis of common training algorithms to better understand the interplay between the loss landscape and algorithmic implicit bias. Bio . Maryam Fazel is the Moorthy Family Professor of Electrical and Computer Engineering at the University of Washington, with adjunct appointments in Computer Science and Engineering, Mathematics, and Statistics. Maryam received her MS and PhD from Stanford University, her BS from Sharif University of Technology in Iran, and was a postdoctoral scholar at Caltech before joining UW. She is a recipient of the NSF Career Award, UWEE Outstanding Teaching Award, and UAI conference Best Student Paper Award with her student. She directs the Institute for Foundations of Data Science (IFDS), a multi-site NSF TRIPODS Institute. She serves on the Editorial board of the MOS-SIAM Book Series on Optimization, is an Associate Editor of the SIAM Journal on Mathematics of Data Science and an Action Editor of Journal of Machine Learning Research. Her current research interests are in the area of optimization in machine learning and control. Tom Goldstein . University of Maryland . Title: Statistical methods for addressing safety and security issues of generative models . Time and Location: Day 4, 9:00 AM HKT, TBA . Abstract . This talk will have two parts. In the first part, I’ll talk about mathematical perspectives on how to watermark generative models to prevent parameter theft, ways to watermark generative model outputs to enable detection, and ways to perform post-hoc detection of language models without relying on watermarks. I’ll emphasize the important idea of using statistical hypothesis testing and p-values to provide rigorous control of the false-positive rate of detection. In the second part of the talk, I’ll present methods for constructing neural networks that exhibit “slow” thinking abilities akin to human logical reasoning. Rather than learning simple pattern matching rules, these networks have the ability to synthesize algorithmic reasoning processes and solve difficult discrete search and planning problems that cannot be solved by conventional AI systems. Interestingly, these reasoning systems naturally exhibit error correction and robustness properties that make them more difficult to break than their fast thinking counterparts. Bio . Tom Goldstein is the Volpi-Cupal Professor of Computer Science at the University of Maryland, and director of the Maryland Center for Machine Learning. His research lies at the intersection of machine learning and optimization, and targets applications in computer vision and signal processing. Professor Goldstein has been the recipient of several awards, including SIAM’s DiPrima Prize, a DARPA Young Faculty Award, a JP Morgan Faculty award, an Amazon Research Award, and a Sloan Fellowship. Yingbin Liang . Ohio State University . Title: In-Context Convergence of Transformers . Time and Location: Day 3, 9:00 AM HKT, TBA . Abstract . Transformers have recently revolutionized many machine learning domains and one salient discovery is their remarkable in-context learning capability, where models can capture an unseen task by utilizing task-specific prompts without further parameters fine-tuning. In this talk, I will present our recent work that aims at understanding the in-context learning mechanism of transformers. Our focus is on the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. I will first present our characterization of the training convergence of in-context learning for data with balanced and imbalanced features, respectively. I will then discuss the insights that we obtain about attention models and training processes. I will also talk about the analysis techniques that we develop which may be useful for a broader set of problems. I will finally conclude my talk with comments on a few future directions.This is a joint work with Yu Huang (UPenn) and Yuan Cheng (NUS). Bio . Dr. Yingbin Liang is currently a Professor at the Department of Electrical and Computer Engineering at the Ohio State University (OSU), and a core faculty of the Ohio State Translational Data Analytics Institute (TDAI). She also serves as the Deputy Director of the AI-EDGE Institute at OSU. Dr. Liang received the Ph.D. degree in Electrical Engineering from the University of Illinois at Urbana-Champaign in 2005, and served on the faculty of University of Hawaii and Syracuse University before she joined OSU. Dr. Liang’s research interests include machine learning, optimization, information theory, and statistical signal processing. Dr. Liang received the National Science Foundation CAREER Award and the State of Hawaii Governor Innovation Award in 2009. She also received EURASIP Best Paper Award in 2014. She is an IEEE fellow. Dimitris Papailiopoulos . University of Wisconsin-Madison . Title: Teaching arithmetic to small language models . Time and Location: Day 3, 1:30 PM HKT, TBA . Abstract . Can a language model truly “understand” arithmetic? We explore this by trying to teach small transformers from scratch to perform elementary arithmetic operations, using the next-token prediction objective. We first demonstrate that conventional training data (i.e., “A+B=C”) is not effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions which, in some cases, can be explained through connections to low-rank matrix completion. We then train these small models on chain-of-thought data that includes intermediate steps. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We finally discuss the issue of length generalization: can a model trained on n digits add n+1 digit numbers? Humans don’t need to be taught every digit length of addition to be able to perform it. It turns out that language models aren’t great at length generalization, but we catch glimpses of it in “unstable” scenarios. Surprisingly, the infamous U-shaped overfitting curve makes an appearance! . Bio . Dimitris Papailiopoulos is the Jay &amp; Cynthia Ihlenfeld Associate Professor of Electrical and Computer Engineering at the University of Wisconsin-Madison. His research interests span machine learning, information theory, and distributed systems, with a current focus on understanding the intricacies of large-language models. Before coming to Madison, Dimitris was a postdoctoral researcher at UC Berkeley and a member of the AMPLab. He earned his Ph.D. in ECE from UT Austin, under the supervision of Alex Dimakis. He received his ECE Diploma M.Sc. degree from the Technical University of Crete, in Greece. Dimitris is a recipient of the NSF CAREER Award (2019), three years of Sony Faculty Innovation Awards (2018, 2019 and 2020), a joint IEEE ComSoc/ITSoc Best Paper Award (2020), an IEEE Signal Processing Society, Young Author Best Paper Award (2015), the Vilas Associate Award (2021), the Emil Steiger Distinguished Teaching Award (2021), and the Benjamin Smith Reynolds Award for Excellence in Teaching (2019). In 2018, he co-founded MLSys, a new conference that targets research at the intersection of machine learning and systems. Stefano Soatto . University of California, Los Angeles . Title: Representation and Control of Meanings in Large Language Models and Multimodal Foundation Models . Time and Location: Day 1, 10:00 AM HKT, TBA . Abstract . Large Language Models and Multimodal Foundation Models, despite the simple predictive learning criterion and absence of explicit complexity bias, have shown the ability to capture the structure and “meaning” of data. I will introduce a notion of “meaning” for large language models as equivalence classes of sentences, and describe methods to establish a geometry and topology in the space of meanings, as well as an algebra so meanings can be composed and asymmetric relations such as entailment and implication can be quantified. Meanings as equivalence classes of sentences determined by the trained embedings can be defined, computed and quantified for pre-trained models, without the need for instruction tuning, reinforcement learning, or prompt engineering. Meanings as trajectories can be shown to align with human assessment through manually annotated benchmarks and can, as the outputs of dynamical systems, be controlled. I will show illustrative examples using both text and imaging modalities. Bio . Professor Soatto received his Ph.D. in Control and Dynamical Systems from the California Institute of Technology in 1996; he joined UCLA in 2000 after being Assistant and then Associate Professor of Electrical and Biomedical Engineering at Washington University, and Research Associate in Applied Sciences at Harvard University. Between 1995 and 1998 he was also Ricercatore in the Department of Mathematics and Computer Science at the University of Udine - Italy. He received his D.Ing. degree (highest honors) from the University of Padova- Italy in 1992. His general research interests are in Computer Vision and Nonlinear Estimation and Control Theory. In particular, he is interested in ways for computers to use sensory information (e.g. vision, sound, touch) to interact with humans and the environment. Dr. Soatto is the recipient of the David Marr Prize (with Y. Ma, J. Kosecka and S. Sastry of U.C. Berkeley) for work on Euclidean reconstruction and reprojection up to subgroups. He also received the Siemens Prize with the Outstanding Paper Award from the IEEE Computer Society for his work on optimal structure from motion (with R. Brockett of Harvard). He received the National Science Foundation Career Award and the Okawa Foundation Grant. He is Associate Editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) and a Member of the Editorial Board of the International Journal of Computer Vision (IJCV) and Foundations and Trends in Computer Graphics and Vision. Jong Chul Ye . Korea Advanced Institute of Science and Technology (KAIST) . Title: Scaling Diffusion Model for Inverse Problems Beyond 2D and Known Forward Models . Time and Location: Day 2, 9:00 AM HKT, TBA . Abstract . Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. Moreover, most of the approaches assume the known forward model, while many inverse problems are involved with blind reconstruction without prior knowledge of the forward model. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem, and blind diffusion models. Our experimental results demonstrate that our method is highly effective for MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT, blind deconvolution. Bio . Jong Chul Ye is a Professor at the Kim Jaechul Graduate School of Artificial Intelligence (AI) of Korea Advanced Institute of Science and Technology (KAIST), Korea. He received his B.Sc. and M.Sc. degrees from Seoul National University, Korea, and his PhD from Purdue University. Before joining KAIST, he worked at Philips Research and GE Global Research in New York. He has served as an associate editor of IEEE Trans. on Image Processing and an editorial board member for Magnetic Resonance in Medicine. He is currently an associate editor for IEEE Trans. on Medical Imaging and a Senior Editor of IEEE Signal Processing Magazine. He is an IEEE Fellow, was the Chair of IEEE SPS Computational Imaging TC, and IEEE EMBS Distinguished Lecturer. He was a General co-chair (with Mathews Jacob) for IEEE Symposium on Biomedical Imaging (ISBI) 2020. His research interest is in machine learning for biomedical imaging and computer vision. ",
    "url": "/speakers/#talk-details",
    
    "relUrl": "/speakers/#talk-details"
  },"74": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"75": {
    "doc": "Sponsors",
    "title": " Conference Sponsors ",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"76": {
    "doc": "Sponsors",
    "title": "Local Host and Main Sponsor",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"77": {
    "doc": "Sponsors",
    "title": "Platinum Sponsor",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"78": {
    "doc": "Sponsors",
    "title": "Gold Sponsors",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"79": {
    "doc": "Sponsors",
    "title": "Silver Sponsors",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"80": {
    "doc": "Sponsors",
    "title": "Rising Star Award Sponsor",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"81": {
    "doc": "Sponsors",
    "title": "Yoga Sponsor",
    "content": " ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"82": {
    "doc": "Sponsors",
    "title": "Sponsors",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"83": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": " ",
    "url": "/subject_areas/#subject-areas",
    
    "relUrl": "/subject_areas/#subject-areas"
  },"84": {
    "doc": "Subject Areas",
    "title": "Theory &amp; Foundations",
    "content": ". | Theories for sparse coding, structured sparsity, subspace learning, low-dimensional manifolds, and general low-dimensional structures. | Dictionary learning and representation learning for low-dimensional structures and their connections to deep learning theory. | Equivariance and invariance modeling. | Theoretical neuroscience and cognitive science foundation for parsimony, and biologically inspired computational mechanisms. | . ",
    "url": "/subject_areas/#theory--foundations",
    
    "relUrl": "/subject_areas/#theory--foundations"
  },"85": {
    "doc": "Subject Areas",
    "title": "Optimization &amp; Algorithms",
    "content": ". | Optimization, robustness, and generalization methods for learning compact and structured representations. | Interpretable and efficient deep architectures (e.g., based on unrolled optimization). | Data-efficient and computation-efficient training and inference. | Adaptive and robust learning and inference algorithms. | Distributed, networked, or federated learning at scale. | Other nonlinear dimension-reduction and representation-learning methods. | . ",
    "url": "/subject_areas/#optimization--algorithms",
    
    "relUrl": "/subject_areas/#optimization--algorithms"
  },"86": {
    "doc": "Subject Areas",
    "title": "Data, Systems &amp; Applications",
    "content": ". | Domain-specific datasets, benchmarks, and evaluation metrics. | Parsimonious and structured representation learning from data. | Inverse problems that benefit from parsimonious priors. | Hardware and system co-design for parsimonious learning algorithms. | Parsimonious learning in intelligent systems that integrate perception-action cycles. | Applications in science, engineering, medicine, and social sciences. | . The above is intended as a high-level overview of CPAL’s focus and by no means exclusive. If you doubt that your paper fits the venue, feel free to contact the program chairs via email at pcs@cpal.cc. ",
    "url": "/subject_areas/#data-systems--applications",
    
    "relUrl": "/subject_areas/#data-systems--applications"
  },"87": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/subject_areas/",
    
    "relUrl": "/subject_areas/"
  },"88": {
    "doc": "Submission Tracks",
    "title": "Submission Tracks and Review Process",
    "content": "CPAL has two submission tracks: . | Proceedings track (archival) | “Recent spotlight” track (non-archival) | . Submissions to both tracks are to be prepared using the CPAL LaTeX style files, available as a zip archive or as an Overleaf template. CPAL OpenReview Submission Portal . ",
    "url": "/tracks/#submission-tracks-and-review-process",
    
    "relUrl": "/tracks/#submission-tracks-and-review-process"
  },"89": {
    "doc": "Submission Tracks",
    "title": "Proceedings Track  (archival)",
    "content": "The submission and review stage will be double-blind. We use OpenReview to host papers and record discussions between authors and reviewers. Before the end of the Authors-Reviewers Discussion Stage, authors can participate in the discussion as well as update their submission at any time. After that, there will be an internal discussion period amongst reviewers and ACs with the aim of summarizing the review process, after which the final decisions are made by ACs. After the notification deadline, accepted and opted-in rejected papers will be made public and open for non-anonymous public commenting. Their anonymous reviews, meta-reviews, author responses and reviewer responses will also be made public. Authors of rejected papers will have two weeks after the notification deadline to opt in to make their de-anonymized rejected papers public in OpenReview. Submissions that are substantially similar to papers previously published, or submitted in parallel to other peer-reviewed venues with proceedings or journals may not be submitted to the Proceedings Track. Papers previously presented at workshops are permitted, so long as they did not appear in a conference proceedings (e.g., CVPRW proceedings), a journal or a book. The existence of non-anonymous preprints (on arXiv or other online repositories, personal websites, social media) will not result in rejection. Authors may submit anonymized work to CPAL that is already available as a preprint (e.g., on arXiv) without citing it. Accepted papers will be published in the Proceedings for Machine Learning Research (PMLR). Full proceedings papers can have up to nine pages with unlimited pages for references and appendix. Upon acceptance of a paper, at least one of the authors must join the conference. Using Large Language Models (LLMs) . We follow the rule by NeurIPS 2023, quoted as follows: . “We welcome authors to use any tool that is suitable for preparing high-quality papers and research. However, we ask authors to keep in mind two important criteria. First, we expect papers to fully describe their methodology, and any tool that is important to that methodology, including the use of LLMs, should be described also. For example, authors should mention tools (including LLMs) that were used for data processing or filtering, visualization, facilitating or running experiments, and proving theorems. It may also be advisable to describe the use of LLMs in implementing the method (if this corresponds to an important, original, or non-standard component of the approach). Second, authors are responsible for the entire content of the paper, including all text and figures, so while authors are welcome to use any tool they wish for writing the paper, they must ensure that all text is correct and original.” . ",
    "url": "/tracks/#proceedings-track--archival",
    
    "relUrl": "/tracks/#proceedings-track--archival"
  },"90": {
    "doc": "Submission Tracks",
    "title": "“Recent Spotlight” Track (non-archival)",
    "content": "We meanwhile aim to showcase the latest research innovations at all stages of the research process, from work-in-progress to recently published papers. Concretely, we ask members of the community to submit to OpenReview either: . | A conference-style submission describing the work, which may be prepared using the CPAL style files, but need not conform to any specific formatting requirements (e.g., page limits); | A poster (in PDF form) presenting results of work-in-progress; | The camera-ready version of work that has been published prior (e.g., conferences, journals). | . Please also upload a short (250 word) abstract to OpenReview. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail: . | A link to a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This track is non-archival and has no proceedings. We permit under-review or concurrent submissions, as well as papers officially accepted by a journal or conference within 12 months of the submission deadline for the Recent Spotlight Track. Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions), and will be held with the same high quality bar with the Proceedings Track. ",
    "url": "/tracks/#recent-spotlight-track-non-archival",
    
    "relUrl": "/tracks/#recent-spotlight-track-non-archival"
  },"91": {
    "doc": "Submission Tracks",
    "title": "Submission Tracks",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/tracks/",
    
    "relUrl": "/tracks/"
  },"92": {
    "doc": "Travel: Visa Information",
    "title": "Visa Information",
    "content": " ",
    "url": "/visa/#visa-information",
    
    "relUrl": "/visa/#visa-information"
  },"93": {
    "doc": "Travel: Visa Information",
    "title": "Visa-Free Stay Eligibility",
    "content": "Holders of passports of many countries (including US, Canada, UK, and many other European and Asian countries) enjoy a visa-free stay in HK for a certain number of days. Please visit the link https://www.immd.gov.hk/eng/services/visas/visit-transit/visit-visa-entry-permit.html to check whether your passport enjoys a visa-free period and its length. ",
    "url": "/visa/#visa-free-stay-eligibility",
    
    "relUrl": "/visa/#visa-free-stay-eligibility"
  },"94": {
    "doc": "Travel: Visa Information",
    "title": "If You Require a Visa",
    "content": "For those who need a visa to enter HK, if you need an invitation letter for your visa application, please contact us and we are happy issue the letter. ",
    "url": "/visa/#if-you-require-a-visa",
    
    "relUrl": "/visa/#if-you-require-a-visa"
  },"95": {
    "doc": "Travel: Visa Information",
    "title": "For PRC Passport Holders",
    "content": "If a Mainland Resident . Mainland residents who wish to attend the conference are required to obtain the “Exit-entry Permit for Travelling to and from Hong Kong and Macao” (通行證) and “endorsement for individual visit” (個人旅遊簽註) from the relevant Public Security Bureau Office. Please see https://s.nia.gov.cn/mps/bszy/wlgaot/sqgowl/201903/t20190313_1002.html#d0. If In-Transit . Holders of People’s Republic of China (PRC) passports who are in transit through Hong Kong to and from another country or territory may be granted a stay of seven days on each landing without the prior need to obtain an entry permit provided that normal immigration requirements are met, including possession of valid entry facilities for the destination and confirmed onward booking for the overseas journey. Please see https://www.immd.gov.hk/eng/services/visas/overseas-chinese-entry-arrangement.html. ",
    "url": "/visa/#for-prc-passport-holders",
    
    "relUrl": "/visa/#for-prc-passport-holders"
  },"96": {
    "doc": "Travel: Visa Information",
    "title": "Questions",
    "content": "If you have any questions, please feel free to contact the local organizers: Dr. Man-Chung Yue (mcyue@hku.hk) and Dr. Yanchao Yang (yanchaoy@hku.hk). ",
    "url": "/visa/#questions",
    
    "relUrl": "/visa/#questions"
  },"97": {
    "doc": "Travel: Visa Information",
    "title": "Travel: Visa Information",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/visa/",
    
    "relUrl": "/visa/"
  },"98": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "“Everything should be made as simple as possible, but not any simpler.” . – Albert Einstein . One of the most fundamental reasons for the very existence and therefore emergence of intelligence or science is that the world is not fully random, but highly structured and predictable. Hence, a fundamental purpose and function of intelligence or science is to learn parsimonious models (or laws) for such predicable structures, from massive sensed data of the world. Over the past decade, the advent of machine learning and large-scale computing has immeasurably changed the ways we process, interpret, and predict with data in engineering and science. The ‘traditional’ approach to algorithm design, based around parametric models for specific structures of signals and measurements – say sparse and low-rank models – and the associated optimization toolkit, is now significantly enriched with data-driven learning-based techniques, where large-scale networks are pre-trained and then adapted to a variety of specific tasks. Nevertheless, the successes of both modern data-driven and classic model-based paradigms rely crucially on correctly identifying the low-dimensional structures present in real-world data, to the extent that we see the roles of learning and compression of data processing algorithms – whether explicit or implicit, as with deep networks – as inextricably linked. Over the last ten or so years, several rich lines of research, including theoretical, computational, and practical, have explored the interplay between learning and compression. Some works explore the role of signal models in the era of deep learning, attempting to understand the interaction between deep networks and nonlinear, multi-modal data structures. Others have applied these insights to the principled design of deep architectures that incorporate desired structures in data into the learning process. Still others have considered generic deep networks as first-class citizens in their own right, exploring ways to compress and sparsify models for greater efficiency, often accompanied by hardware or system-aware co-designs. Across each of these settings, theoretical works rooted in low-dimensional modeling have begun to explain the foundations of deep architectures and efficient learning – from optimization to generalization – in spite of “overparameterization” and other obstructions. Most recently, the advent of foundation models has led some to posit that parsimony and compression itself are a fundamental part of the learning objective of an intelligent system, connecting to ideas from neuroscience on compression as a guiding principle for the brain representing the sensory data of the world. By and large, these lines of work have so far developed somewhat in isolation from one another, in spite of their common basis and purpose for parsimony and learning. Our intention in organizing this conference is to address this issue and go beyond: we envision the conference as a general scientific forum where researchers in machine learning, applied mathematics, signal processing, optimization, intelligent systems, and all associated science and engineering fields can gather, share insights, and ultimately work towards a common modern theoretical and computational framework for understanding intelligence and science from the perspective of parsimonious learning. ",
    "url": "/vision/#conference-vision",
    
    "relUrl": "/vision/#conference-vision"
  },"99": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/vision/",
    
    "relUrl": "/vision/"
  }
}
