{"0": {
    "doc": "Home",
    "title": "Call for Papers",
    "content": "We are pleased to invite paper submissions for the first Conference on Parsimony and Learning. Please see . Call for Papers . for details about the submission and reviewing process, as well as subject areas of interest and general policies. Stay tuned for further updates. ",
    "url": "/#call-for-papers",
    
    "relUrl": "/#call-for-papers"
  },"1": {
    "doc": "Home",
    "title": "Key Dates and Deadlines",
    "content": ". | August 28th, 2023: Submission Deadline | October 14th, 2023: 2-Week Rebuttal Stage Starts | October 27th, 2023: Rebuttal Stage Ends, Authors-Reviewers Discussion Stage Starts | November 5th, 2023: Authors-Reviewers Discussion Stage Ends | November 20th, 2023: Final Decisions Released | November 30th, 2023: Camera-Ready Deadline | January 3rd-6th, 2024: Main Conference (In-Person, HKU Main Campus) | . ",
    "url": "/#key-dates-and-deadlines",
    
    "relUrl": "/#key-dates-and-deadlines"
  },"2": {
    "doc": "Home",
    "title": "Keynote Speakers",
    "content": "Additional speakers to be announced soon! . <!-- ",
    "url": "/#keynote-speakers",
    
    "relUrl": "/#keynote-speakers"
  },"3": {
    "doc": "Home",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . IST Austria/Neural Magic . Tom Goldstein . University of Maryland . Robert D. Nowak . University of Wisconsin-Madison . Dimitris Papailiopoulos . University of Wisconsin-Madison . Jong Chul Ye . KAIST . ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "Home",
    "title": "Home",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU The Conference on Parsimony and Learning (CPAL) is an annual research conference focused on addressing the parsimonious, low dimensional structures that prevail in machine learning, signal processing, optimization, and beyond. We are interested in theories, algorithms, applications, hardware and systems, as well as scientific foundations for learning with parsimony. We describe our vision for the conference in more detail here. ",
    "url": "/",
    
    "relUrl": "/"
  },"5": {
    "doc": "Call for Papers",
    "title": "Subject Areas",
    "content": "We invite submissions relevant to the following broad areas of interest, all connected with parsimony and learning in modern data science: . | Models and Algorithms | Data | Theory | Hardware and Systems | Applications and Science | . Please reference the detailed listing here. ",
    "url": "/cfp/#subject-areas",
    
    "relUrl": "/cfp/#subject-areas"
  },"6": {
    "doc": "Call for Papers",
    "title": "Submission Tracks and Review Process",
    "content": "We will have a main proceeding track (archival), and a “recent spotlight” track (non-archival). ",
    "url": "/cfp/#submission-tracks-and-review-process",
    
    "relUrl": "/cfp/#submission-tracks-and-review-process"
  },"7": {
    "doc": "Call for Papers",
    "title": "Proceeding Track  (archival)",
    "content": "The submission and review stage will be double-blind. We use OpenReview to host papers and allow for public discussions. Before the end of the Authors-Reviewers Discussion Stage, besides official reviews, anyone can post publicly visible comments; and authors can participate in the discussion as well as update their submission at any time. After that, there will be an internal discussion period amongst reviewers and ACs with the aim of summarizing the review process, after which the final decisions are made by ACs. After the notification deadline, accepted and opted-in rejected papers will be made public and open for non-anonymous public commenting. Their anonymous reviews, meta-reviews, author responses and reviewer responses will also be made public. Authors of rejected papers will have two weeks after the notification deadline to opt in to make their de-anonymized rejected papers public in OpenReview. Submissions that are substantially similar to papers previously published, or submitted in parallel to other peer-reviewed venues with proceedings or journals may not be submitted to the Proceeding Track. Papers previously presented at workshops are permitted, so long as they did not appear in a conference proceedings (e.g., CVPRW proceedings), a journal or a book. The existence of non-anonymous preprints (on arXiv or other online repositories, personal websites, social media) will not result in rejection. Authors may submit anonymized work to CPAL that is already available as a preprint (e.g., on arXiv) without citing it. Accepted papers will be published in the Proceedings for Machine Learning Research (PMLR). Full proceedings papers can have up to 9 pages with unlimited pages for references and appendix. Upon acceptance of a paper, at least one of the authors must join the conference. Using Large Language Models (LLMs) . We follow the rule by NeurIPS 2023, quoted as follows: . “We welcome authors to use any tool that is suitable for preparing high-quality papers and research. However, we ask authors to keep in mind two important criteria. First, we expect papers to fully describe their methodology, and any tool that is important to that methodology, including the use of LLMs, should be described also. For example, authors should mention tools (including LLMs) that were used for data processing or filtering, visualization, facilitating or running experiments, and proving theorems. It may also be advisable to describe the use of LLMs in implementing the method (if this corresponds to an important, original, or non-standard component of the approach). Second, authors are responsible for the entire content of the paper, including all text and figures, so while authors are welcome to use any tool they wish for writing the paper, they must ensure that all text is correct and original.” . ",
    "url": "/cfp/#proceeding-track--archival",
    
    "relUrl": "/cfp/#proceeding-track--archival"
  },"8": {
    "doc": "Call for Papers",
    "title": "“Recent Spotlight” Track (non-archival)",
    "content": "We meanwhile aim to showcase the latest research innovations at all stages of the research process, from work-in-progress to recently published papers. Concretely, we ask members of the community to submit a conference-style paper (from four to eight pages, with extra pages for references) describing the work. Please also upload a short (250 word) abstract to OpenReview. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail: . | A poster (in PDF form) presenting results of work-in-progress. | A link to an arXiv preprint or a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This track is non-archival and has no proceedings. We permit under-review or concurrent submissions. Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions), and will be held with the same high quality bar with the Proceeding Track. ",
    "url": "/cfp/#recent-spotlight-track-non-archival",
    
    "relUrl": "/cfp/#recent-spotlight-track-non-archival"
  },"9": {
    "doc": "Call for Papers",
    "title": "Reviewer Guidelines",
    "content": " ",
    "url": "/cfp/#reviewer-guidelines",
    
    "relUrl": "/cfp/#reviewer-guidelines"
  },"10": {
    "doc": "Call for Papers",
    "title": "Notable Innovations in Our Review Mechanism",
    "content": "CPAL strives for providing every paper with high-quality, accountable reviews, and therefore takes the following actions in addition: . | Shepherding by an Action PC: Every paper’s final decision, after being recommended by AC, will go through the direct shepherding of all program chairs (led by one “action PC”). The action PC has two main duties: . | (before final decision released) The action PC will pay particular attention to the borderline cases and the dispute (large score variations) cases, and will be asked to write additional “meta-meta reviews” in those cases and potentially calibrate on top of AC recommendations. Final decisions will be scrutinized and made in a joint meeting by all program chairs. | (after receiving the camera-ready) Each accepted paper’s authors will be asked to submit a one-page cover letter, summarizing what revisions are made between the paper’s submitted and camera-ready versions. The action PC will ensure: (1) all “promised” changes by authors during the discussion stage are indeed implemented; (2) no change that is “too substantial” and “unsoliciated” shall be made to the paper, unless in exceptional circumstances where the action PC has to approve case-by-case. The action PC reserves the right to reject a camera-ready submission and exclude it from the conference proceedings. | . | Semi-Open Identity for Accountability (Action PC and/or AC): For every accepted paper, the names of its AC and action PC will be publicly released on its OpenReview page too. For every rejected paper (excluding withdrawals), only the name of its action PC will be displayed. This decision was not reached lightly; but we hope it would meaningfully add credibility and accountability for every paper’s final outcome. | Reviewer Rating and “Dynamic Sparse Selection”: each AC will be asked to rate every reviewer in their batch, in terms of timeliness and quality. Program chairs, who know all reviewers’ identities, will compile a list of reviewers sorted by their average ratings received. Reviewers that receive consistent low reviewer rating for multiple papers / from multiple ACs will be excluded from future review processes. | . ",
    "url": "/cfp/#notable-innovations-in-our-review-mechanism",
    
    "relUrl": "/cfp/#notable-innovations-in-our-review-mechanism"
  },"11": {
    "doc": "Call for Papers",
    "title": "General Guidelines",
    "content": "We all like the Acceptance Criteria made by TMLR https://jmlr.org/tmlr/acceptance-criteria.html and would instruct our ACs and reviewers to honor the same. In particular we note: . “Crucially, it should not be used as a reason to reject work that isn’t considered “significant” or “impactful” because it isn’t achieving a new state-of-the-art on some benchmark. Nor should it form the basis for rejecting work on a method considered not “novel enough”, as novelty of the studied method is not a necessary criteria for acceptance. We explicitly avoid these terms (“significant”, “impactful”, “novel”), and focus instead on the notion of “interest”. If the authors make it clear that there is something to be learned by some researchers in their area from their work, then the criteria of interest is considered satisfied. ",
    "url": "/cfp/#general-guidelines",
    
    "relUrl": "/cfp/#general-guidelines"
  },"12": {
    "doc": "Call for Papers",
    "title": "Code of Conduct",
    "content": "(Adapted from ICML/ICLR/NeurIPS/LoG) . We strive to hold a conference in which any person can meaningfully participate in the CPAL community through: sharing ideas, presenting their work, meeting members of the community, learning from other people’s work, and discussing ways to improve the community. This conference will work towards preventing any type of discrimination based on age, disability, ethnicity, experience in the field, gender identity, nationality, physical appearance, race, religion, sexual orientation, or other protected characteristics. We strictly prohibit any actions that may prevent the participation of any attendee, including: bullying, harassment, inappropriate media, offensive language, violence, zoom bombing, and so on. Please report any violations of the code of conduct to the conference organizers, via email, slack, or any other channels. If requested, we can handle these reports anonymously, to protect the person making the report. Violators of the code of conduct will be asked to stop their inappropriate behavior, and may be removed from their right to participate in the conference. Further disciplinary action such as bans from future iterations of the conference may be taken. ",
    "url": "/cfp/#code-of-conduct",
    
    "relUrl": "/cfp/#code-of-conduct"
  },"13": {
    "doc": "Call for Papers",
    "title": "Call for Papers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/cfp/",
    
    "relUrl": "/cfp/"
  },"14": {
    "doc": "Key Dates",
    "title": "Key Dates and Deadlines",
    "content": ". | August 28th, 2023: Submission Deadline | October 14th, 2023: 2-Week Rebuttal Stage Starts | October 27th, 2023: Rebuttal Stage Ends, Authors-Reviewers Discussion Stage Starts | November 5th, 2023: Authors-Reviewers Discussion Stage Ends | November 20th, 2023: Final Decisions Released | November 30th, 2023: Camera-Ready Deadline | January 3rd-6th, 2024: Main Conference (In-Person, HKU Main Campus) | . ",
    "url": "/deadlines/#key-dates-and-deadlines",
    
    "relUrl": "/deadlines/#key-dates-and-deadlines"
  },"15": {
    "doc": "Key Dates",
    "title": "Key Dates",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/deadlines/",
    
    "relUrl": "/deadlines/"
  },"16": {
    "doc": "Organizers",
    "title": "Program Committee",
    "content": " ",
    "url": "/organizers/#program-committee",
    
    "relUrl": "/organizers/#program-committee"
  },"17": {
    "doc": "Organizers",
    "title": "General Chairs",
    "content": "Gitta Kutyniok . LMU Munich . General Chair . Yi Ma . UC Berkeley . General Chair . Harry Shum . HKUST/IDEA . General Chair . René Vidal . UPenn . General Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"18": {
    "doc": "Organizers",
    "title": "Program Chairs",
    "content": "Yuejie Chi . Carnegie Mellon University . Program Chair . Gintare Karolina Dziugaite . Google DeepMind . Program Chair . Qing Qu . University of Michigan . Program Chair . Atlas Wang . UT Austin . Program Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"19": {
    "doc": "Organizers",
    "title": "Local Chairs",
    "content": "Yanchao Yang . HKU . Local Chair . Man-Chung Yue . HKU . Local Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"20": {
    "doc": "Organizers",
    "title": "Publication Chairs",
    "content": "Zhihui Zhu . Ohio State University . Publication Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"21": {
    "doc": "Organizers",
    "title": "Industry Liaison Chairs",
    "content": "Dan Alistarh . IST Austria/Neural Magic . Industry Liaison Chair . Dilin Wang . Meta . Industry Liaison Chair . Chong You . Google Research . Industry Liaison Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"22": {
    "doc": "Organizers",
    "title": "Panel Chairs",
    "content": "Sijia Liu . Michigan State University . Panel Chair . Jeremias Sulam . Johns Hopkins University . Panel Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"23": {
    "doc": "Organizers",
    "title": "Tutorial Chairs",
    "content": "Saiprasad Ravishankar . Michigan State University . Tutorial Chair . John Wright . Columbia University . Tutorial Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"24": {
    "doc": "Organizers",
    "title": "Publicity Chairs",
    "content": "Yani Ioannou . University of Calgary . Publicity Chair . William T. Redman . UCSB . Publicity Chair . Liyue Shen . University of Michigan . Publicity Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"25": {
    "doc": "Organizers",
    "title": "Web Chairs",
    "content": "Sam Buchanan . TTIC . Web Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"26": {
    "doc": "Organizers",
    "title": "Area Chairs",
    "content": "Chenglong Bao . Tsinghua University . Area Chair . Babak Ehteshami Bejnordi . Qualcomm . Area Chair . Beidi Chen . Meta/Carnegie Mellon University . Area Chair . Tianlong Chen . UT Austin/MIT . Area Chair . Yubei Chen . NYU/Meta . Area Chair . Yuxin Chen . UPenn . Area Chair . Ivan Dokmanić . University of Basel . Area Chair . Simon Du . University of Washington . Area Chair . Utku Evci . Google DeepMind . Area Chair . Souvik Kundu . Intel AI Labs . Area Chair . Qi Lei . NYU . Area Chair . Xiao Li . CUHK Shenzhen . Area Chair . Elena Mocanu . University of Twente . Area Chair . Greg Ongie . Marquette University . Area Chair . Mahdi Soltanolkotabi . USC . Area Chair . Peng Wang . University of Michigan . Area Chair . Yu-Xiang Wang . UC Santa Barbara . Area Chair . Bihan Wen . Nanyang Technological University . Area Chair . Fanny Yang . ETH Zurich . Area Chair . Yuqian Zhang . Rutgers University . Area Chair . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"27": {
    "doc": "Organizers",
    "title": "Advisory Committee",
    "content": "Alex Dimakis . UT Austin . Advisory Committee . Michael Elad . Technion . Advisory Committee . Yi Ma . UC Berkeley . Advisory Committee . Stefano Soatto . UCLA . Advisory Committee . Rebecca Willett . UChicago . Advisory Committee . Eric P. Xing . MBZUAI/Carnegie Mellon University . Advisory Committee . Tong Zhang . HKUST . Advisory Committee . ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"28": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/organizers/",
    
    "relUrl": "/organizers/"
  },"29": {
    "doc": "Policies",
    "title": "TODO for this page:",
    "content": ". | DEI? | Code of conduct? | Review/author guide? | . ",
    "url": "/policies/#todo-for-this-page",
    
    "relUrl": "/policies/#todo-for-this-page"
  },"30": {
    "doc": "Policies",
    "title": "Policies",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/policies/",
    
    "relUrl": "/policies/"
  },"31": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Additional speakers to be announced soon! . <!-- ",
    "url": "/speakers/#keynote-speakers",
    
    "relUrl": "/speakers/#keynote-speakers"
  },"32": {
    "doc": "Keynote Speakers",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Dan Alistarh . IST Austria/Neural Magic . Tom Goldstein . University of Maryland . Robert D. Nowak . University of Wisconsin-Madison . Dimitris Papailiopoulos . University of Wisconsin-Madison . Jong Chul Ye . KAIST . ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"33": {
    "doc": "Keynote Speakers",
    "title": "Keynote Speakers",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/speakers/",
    
    "relUrl": "/speakers/"
  },"34": {
    "doc": "Sponsors",
    "title": "TODO for this page:",
    "content": ". | Can include it on the homepage too | . ",
    "url": "/sponsors/#todo-for-this-page",
    
    "relUrl": "/sponsors/#todo-for-this-page"
  },"35": {
    "doc": "Sponsors",
    "title": "Sponsors",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/sponsors/",
    
    "relUrl": "/sponsors/"
  },"36": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": " ",
    "url": "/subject_areas/#subject-areas",
    
    "relUrl": "/subject_areas/#subject-areas"
  },"37": {
    "doc": "Subject Areas",
    "title": "Models and Algorithms",
    "content": ". | Parsimonious training and inference algorithms for deep networks, including but not limited to: . | Pruning, sparse training, lottery ticket hypothesis | Low-rank training, quantization, distillation, retrieval-augmented models | Adaptive/conditional computation, including mixture of experts | Parsimonious transfer learning methods, such as sparse tuning or LoRA | . | Compact and efficient neural network architectures by design | Model-based architectures as inspired by structured models (such as unrolling) | Robust/stable/invariant models or training/inference algorithms, guided by parsimony principles | Other nonlinear dimension reduction methods not pertaining to deep networks (such as autoencoders, subspace learning, manifold learning, etc) | Interpretability induced by parsimonious modeling (such as feature selection, model visualization) | Generative models guided by parsimony principles | Distributed, federated, and communicated-efficient training or inference, that leverage model parsimony | Efficient neural scaling, and next-generation parsimonious architectures beyond common in-use models | . ",
    "url": "/subject_areas/#models-and-algorithms",
    
    "relUrl": "/subject_areas/#models-and-algorithms"
  },"38": {
    "doc": "Subject Areas",
    "title": "Data",
    "content": ". | Modern signal models: probabilistic, geometric (manifolds, graphical), visual/3D, language, dynamical, hierarchical structures | Dataset parsimony (such as data filtering, coreset selection), and sparse data formats in non-Euclidean domains such as graph | Empirical and theoretical studies of representation learning with structured data | . ",
    "url": "/subject_areas/#data",
    
    "relUrl": "/subject_areas/#data"
  },"39": {
    "doc": "Subject Areas",
    "title": "Theory",
    "content": ". | Generalization, optimization, robustness, and approximation in deep learning, rigorously relating to its implicit parsimony | Theories for classical sparse coding, dictionary learning, structured sparsity, subspace learning, etc, and their connections to neural network sparsity | Forgetting owing to sparsity, including fairness, privacy and bias concerns | . ",
    "url": "/subject_areas/#theory",
    
    "relUrl": "/subject_areas/#theory"
  },"40": {
    "doc": "Subject Areas",
    "title": "Hardware and Systems",
    "content": ". | Libraries, kernels, and compilers for accelerating sparse computation | Hardware with customized support for sparse computation | Resource-efficient learning and co-design applications at the edge or the cloud | . ",
    "url": "/subject_areas/#hardware-and-systems",
    
    "relUrl": "/subject_areas/#hardware-and-systems"
  },"41": {
    "doc": "Subject Areas",
    "title": "Applications and Science",
    "content": ". | Parsimonious AI for science and engineering applications, such as various inverse problems that benefit from parsimonious priors | Theoretical neuroscience and cognitive science foundations for parsimony, and biologically inspired algorithms | Other application fields crossing disciplinary boundaries and suggesting further collaborations under the theme of parsimony: computer vision, robotics, reinforcement learning, and more | . The above is intended as a high-level overview of CPAL’s focus and by no means exclusive. If you doubt that your paper fits the venue, feel free to contact the program chairs via email at pcs@cpal.cc. ",
    "url": "/subject_areas/#applications-and-science",
    
    "relUrl": "/subject_areas/#applications-and-science"
  },"42": {
    "doc": "Subject Areas",
    "title": "Subject Areas",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/subject_areas/",
    
    "relUrl": "/subject_areas/"
  },"43": {
    "doc": "Submission Guidelines",
    "title": "TODO FOR THIS PAGE",
    "content": ". | List of ACs (single blind policy) | Subject areas | Procedures for the CfP… . | guidelines for submitting (see next header too) | formatting/style | . | OpenReview | Award info (committee?) | Information about presenting the papers (in-person; poster/spotlight/awards; …) | . ",
    "url": "/submission/#todo-for-this-page",
    
    "relUrl": "/submission/#todo-for-this-page"
  },"44": {
    "doc": "Submission Guidelines",
    "title": "How to Submit Your Work",
    "content": "We will use OpenReview to manage submissions. Submit your work here: . https://openreview.net/group?id=mbzuai.ac.ae/SLowDNN/2023/Workshop . See the submission guidelines and topics of interest. ",
    "url": "/submission/#how-to-submit-your-work",
    
    "relUrl": "/submission/#how-to-submit-your-work"
  },"45": {
    "doc": "Submission Guidelines",
    "title": "Logistics for Accepted Papers",
    "content": "Accepted works will be expected to present a poster describing the work in-person at the workshop. Travel grants are available to support authors of accepted papers: see the travel page for details. A small subset of the top accepted papers will be recommended for inclusion in a future special issue of the IEEE Journal of Selected Topics in Signal Processing. ",
    "url": "/submission/#logistics-for-accepted-papers",
    
    "relUrl": "/submission/#logistics-for-accepted-papers"
  },"46": {
    "doc": "Submission Guidelines",
    "title": "Topics of Interest",
    "content": "Topics of interest include, but are not limited to, connections between low-dimensional models and the theory, architectures, algorithms, and applications of deep neural networks: . | Theory: approximation, generalization, robustness, compact and structured representations | Optimization: Benign non-convex optimization, implicit bias analysis, convergence guarantees | Architectures: compact/model-based/neuro-inspired/invariant neural networks | Algorithms: pruning, sparse training, robust training | Applications: generative models, resource/data-efficient learning, inverse problems | . ",
    "url": "/submission/#topics-of-interest",
    
    "relUrl": "/submission/#topics-of-interest"
  },"47": {
    "doc": "Submission Guidelines",
    "title": "Submission Guidelines",
    "content": "We aim to showcase: . | The latest research innovations at all stages of the research process, from work-in-progress to recently published papers | Position or survey papers on any topics relevant to this workshop (see the call above) | . Concretely, we ask members of the community to submit a conference-style paper (from four to eight pages, with extra pages for references) describing the work. Please also upload a short (250 word) abstract to OpenReview. Do not anonymize submissions. Papers should be written using the NeurIPS 2022 style files, available here. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail. | A poster (in PDF form) presenting results of work-in-progress. | A link to a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This workshop is non-archival, and it will not have proceedings. We permit under-review or concurrent submissions. Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions). ",
    "url": "/submission/#submission-guidelines",
    
    "relUrl": "/submission/#submission-guidelines"
  },"48": {
    "doc": "Submission Guidelines",
    "title": "Submission Guidelines",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/submission/",
    
    "relUrl": "/submission/"
  },"49": {
    "doc": "Tutorials",
    "title": "TODO for this page:",
    "content": ". | Do we want to offer our existing course? | Do we want a call? | . ",
    "url": "/tutorials/#todo-for-this-page",
    
    "relUrl": "/tutorials/#todo-for-this-page"
  },"50": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "The first day of the workshop features tutorial presentations from a subset of the organizers. These tutorials present an up-to-date account of the intersection between low-dimensional modeling and deep learning in an accessible format. The tutorials are summarized below. See the schedule for the precise times of each tutorial. Some of the tutorials draw on material from an ICASSP 2022 short course. Introduction to Low-Dimensional Models . John Wright . Columbia University . The first session will introduce fundamental properties and theoretical results for sensing, processing, analyzing, and learning low-dimensional structures from high-dimensional data. We will first discuss classical low-dimensional models, such as sparse recovery and low-rank matrix sensing, and motivate these models by applications in medical imaging, collaborative filtering, face recognition, and beyond. Based on convex relaxation, we will characterize the conditions, in terms of sample/data complexity, under which the inverse problems of recovering such low-dimensional structures become tractable and can be solved efficiently, with guaranteed correctness or accuracy. Nonconvex Optimization of Low-Dimensional Models . Yuqian Zhang . Rutgers University . We will transit from sensing to learning low-dimensional structures, such as dictionary learning, sparse blind deconvolution, and dual principal component analysis. Problems associated with learning low-dimensional models from sample data are often nonconvex: either they do not have tractable convex relaxations or the nonconvex formulation is preferred due to physical or computational constraints (such as limited memory). To deal with these challenges, we will introduce a systematic approach of analyzing the corresponding nonconvex landscapes from a geometry and symmetry perspective. The resulting approach leads to provable globally convergent nonconvex optimization methods. Learning Low-Dimensional Structure via Deep Networks . Sam Buchanan . TTIC . We will discuss the contemporary topic of using deep models for computing with nonlinear data, introducing strong conceptual connections between low-dimensional structures in data and deep models. We will then consider a mathematical model problem that attempts to capture these aspects of practice, and show how low-dimensional structure in data and tasks influences the resources (statistical, architectural) required to achieve a given performance level. Our discussion will revolve around basic tradeoffs between these resources and theoretical guarantees of performance. Learned Representations and Low-Dimensional Structures . Zhihui Zhu . Ohio State University . Continuing our exploration of deep models for nonlinear data, we will begin to delve into learned representations, network architectures, regularizations, and beyond. We will see how the tools for nonconvexity developed previously shed light on the learned representations produced by deep networks, through connections to matrix factorization. We will observe how algorithms that interact with data will expose additional connections to low-dimensional models, through implicit regularization of the network parameters. Design Deep Networks for Pursuing Low-Dimensional Structures . Yi Ma . UC Berkeley . Based upon the previous discussion on the connection between low-dimensional structures and deep models, in this section, we will discuss principles for designing deep networks through the lens of learning good low-dimensional representation for (potentially nonlinear) low-dimensional structures. We will see how unrolling iterative optimization algorithms for low-dimensional problems (such as the sparsifying algorithms) naturally lead to deep neural networks. We will then show how modern deep layered architectures, linear (convolution) operators, and nonlinear activations, and even all parameters can be derived from the principle of learning a compact linear discriminative representation for nonlinear low-dimensional structures within the data. We will show how so learned representations can bring tremendous benefits in tasks such as learning generative models, noise stability, and incremental learning. Sparsity Neural Networks – Practice and Theory . Atlas Wang . UT Austin . We discuss the role of sparsity in general neural network architectures, and shed light on how sparsity interacts with deep learning under the overparameterization regime, for both practitioners and theorists. A sparse neural network (NN) has most of its parameters set to zero and is traditionally considered as the product of NN compression (i.e., pruning). Yet recently, sparsity has exposed itself as an important bridge for modeling the underlying low dimensionality of NNs, for understanding their generalization, optimization dynamics, implicit regularization, expressivity, and robustness. Deep NNs learned with sparsity-aware priors have also demonstrated significantly improved performances through a full stack of applied work on algorithms, systems, and hardware. In this talk, I plan to cover recent progress on the practical, theoretical, and scientific aspects of sparse NNs. I will try scratching the surface of three aspects – (1) practically, why one should love a sparse NN, beyond just a post-training NN compression tool; (2) theoretically, what are some guarantees that one can expect from sparse NNs; and (3) what is future prospect of exploiting sparsity in NNs. Advancing Machine Learning for Imaging – Regularization and Robustness . Saiprasad Ravishankar . Michigan State University . In this talk, we present our work on improving machine learning for image reconstruction on three fronts – i) learning regularizers, ii) learning with no training data, and iii) ensuring robustness to perturbations in learning-based schemes. First, we present an approach for supervised learning of sparsity-promoting regularizers, where the parameters of the regularizer are learned to minimize reconstruction error on a paired training set. Training involves a challenging bilevel optimization problem with a nonsmooth lower-level objective. We derive an expression for the gradient of the training loss using the implicit closed-form solution of the lower-level variational problem, and provide an accompanying exact gradient descent algorithm (dubbed BLORC). Our experiments show that the gradient computation is efficient and BLORC learns meaningful operators for effective denoising. Second, we investigate the deep image prior (DIP) scheme that recovers an image by fitting an overparameterized neural network directly to the image’s corrupted measurements. To address DIP’s overfitting and performance issues, recent work proposed using a reference image as the network input. However, obtaining the reference often requires supervision. Hence, we propose a self-guided scheme that uses only undersampled measurements to estimate both the network weights and input image. We exploit regularization requiring the network to be a powerful denoiser. Our self-guided method gives significantly improved reconstructions for MRI with limited measurements compared to recent schemes, while using no training data. Finally, recent studies have shown that trained deep reconstruction models could be over-sensitive to tiny input perturbations, which cause unstable, low-quality reconstructed images. To address this issue, we propose Smoothed Unrolling (SMUG), which incorporates a randomized smoothing-based robust learning operation into a deep unrolling architecture and improves the robustness of MRI reconstruction with respect to diverse perturbations. ",
    "url": "/tutorials/#tutorials",
    
    "relUrl": "/tutorials/#tutorials"
  },"51": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/tutorials/",
    
    "relUrl": "/tutorials/"
  },"52": {
    "doc": "Venue",
    "title": "TODO for this page:",
    "content": ". | Thinking this can be made a general travel/logistics page | HKU info | Can put it later | . ",
    "url": "/venue/#todo-for-this-page",
    
    "relUrl": "/venue/#todo-for-this-page"
  },"53": {
    "doc": "Venue",
    "title": "Venue",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/venue/",
    
    "relUrl": "/venue/"
  },"54": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "“Entities should not be multiplied unnecessarily.” . – William of Ockham . The ways we process, interpret, and predict with data in engineering and scientific applications have changed immeasurably since the advent of the deep learning era. The ‘traditional’ approach to algorithm design, based around parametric models for structured signals and measurements – say sparse and low-rank models – and the associated optimization toolkit, is now regularly supplemented with data-driven learning-based techniques, where large-scale deep networks are pretrained and then adapted to a variety of tasks. The successes of both paradigms have depended crucially on the low-dimensional structure present in real-world data, to the extent that we view the roles of parsimony of data processing algorithms – whether explicit or implicit, as with deep networks – and learning as inextricably linked. Over the last ten or so years, several rich lines of research, both applied and theoretical, have explored the interplay between these two principles. Some works explore the role of signal models in the era of deep learning, attempting to understand the interaction between deep networks and nonlinear, multimodal data. Others have applied these insights to the principled design of deep architectures that incorporate known structures in data into the learning process. Still others have considered the parameters of generic deep networks as first-class citizens in their own right, exploring ways to compress and sparsify models for greater efficiency, often accompanied with hardware or system-aware co-designs. Across each of these settings, theoretical works have begun to explain the foundations of efficient learning – from optimization to generalization – in spite of “overparameterization” and other obstructions. And most recently, the advent of foundation models has led some to posit that parsimony itself is a fundamental part of the learning objective of an intelligent system, connecting to ideas from neuroscience on compression as a guiding principle for the brain representing the sensory data of the world. By and large, these lines of work have developed in isolation from one another, in spite of their common basis in parsimony and learning. Our intention in organizing this conference is to address this: we envision the conference as a general forum where researchers in machine learning, applied mathematics, signal processing, optimization, hardware &amp; systems, and all associated science and engineering applications can gather, share insights, and ultimately work towards a common data-centric understanding of modern parsimonious learning frameworks. ",
    "url": "/vision/#conference-vision",
    
    "relUrl": "/vision/#conference-vision"
  },"55": {
    "doc": "Conference Vision",
    "title": "Conference Vision",
    "content": "Conference on Parsimony and Learning (CPAL) January 2024,&nbsp;HKU ",
    "url": "/vision/",
    
    "relUrl": "/vision/"
  }
}
