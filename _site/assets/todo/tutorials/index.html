<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Tutorials | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Tutorials" /><meta property="og:locale" content="en_US" /><meta name="description" content="A listing of the tutorials that will be presented at the conference." /><meta property="og:description" content="A listing of the tutorials that will be presented at the conference." /><link rel="canonical" href="https://cpal.cc/assets/todo/tutorials/" /><meta property="og:url" content="https://cpal.cc/assets/todo/tutorials/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Tutorials" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A listing of the tutorials that will be presented at the conference.","headline":"Tutorials","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/assets/todo/tutorials/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">Venue</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/program_highlights/" class="nav-list-link">Program Highlights</a><li class="nav-list-item "> <a href="/oral_and_spotlight_presentations/" class="nav-list-link">Orals and Recent Spotlights</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a><li class="nav-list-item "> <a href="/tutorials/" class="nav-list-link">Tutorials</a><li class="nav-list-item "> <a href="/meditation/" class="nav-list-link">Morning Yoga Sessions</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal-registration" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="todo-for-this-page"> <a href="#todo-for-this-page" class="anchor-heading" aria-labelledby="todo-for-this-page"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> TODO for this page:</h1><ul><li>Do we want to offer our existing course?<li>Do we want a call?</ul><h1 id="tutorials"> <a href="#tutorials" class="anchor-heading" aria-labelledby="tutorials"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Tutorials</h1><p>The first day of the workshop features tutorial presentations from a subset of the organizers. These tutorials present an up-to-date account of the intersection between low-dimensional modeling and deep learning in an accessible format.</p><p>The tutorials are summarized below. See <a href="/schedule">the schedule</a> for the precise times of each tutorial. Some of the tutorials draw on material from <a href="https://highdimdata-lowdimmodels-tutorial.github.io/">an ICASSP 2022 short course</a>.</p><h3 id="redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction"> <a href="#redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction" class="anchor-heading" aria-labelledby="redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ma.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a></h3><p>UC Berkeley / HKU IDS</p><p class="organizer-meta">Professor</p></div></div><p>To begin, we will focus on the special yet highly useful case of learning the data distribution and transforming it to an <em>linear discriminative representation</em> (LDR). We will discuss the information theoretic and statistical principles behind such a representation, and design a loss function, called the <em>coding rate reduction</em>, which is optimized at such a representation. By unrolling the gradient ascent on the coding rate reduction, we will construct a deep network architecture, called the ReduNet, where each operator in the network has a mathematically precise (hence white-box and interpretable) function in the transformation of the data distribution towards an LDR. Also, the ReduNet may be constructed layer-wise in a forward-propagation manner, that is, without <em>any</em> back-propagation required.</p><h3 id="sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-i"> <a href="#sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-i" class="anchor-heading" aria-labelledby="sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-i"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sparse Modeling for Image Reconstruction and Restoration &amp; Intro to Deep Learning for Imaging (Part I)</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/sr.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a></h3><p>Michigan State University</p><p class="organizer-meta">Assistant Professor</p></div></div><p>Topics covered:<br />• Introduction to image reconstruction and to sparse modeling.<br />• Dictionary learning based image reconstruction. Combination with other models and variants (e.g., convolutional dictionary learning).<br />• Transform learning methods. Application to image reconstruction. Multi-layer sparse models.</p><h3 id="white-box-transformers-via-sparse-rate-reduction"> <a href="#white-box-transformers-via-sparse-rate-reduction" class="anchor-heading" aria-labelledby="white-box-transformers-via-sparse-rate-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> White-Box Transformers via Sparse Rate Reduction</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/sdb.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sdbuchanan.com">Sam Buchanan</a></h3><p>TTIC</p><p class="organizer-meta">Research Assistant Professor</p></div></div><p>We demonstrate how combining sparse coding and rate reduction yields sparse linear discriminative representations using an objective named “sparse rate reduction”. We develop CRATE, a deep network architecture, by unrolling the optimization of this objective and parameterizing feature distribution in each layer. CRATE’s operators are mathematically interpretable, with each layer representing an optimization step, making the network a transparent “white box”. Although CRATE’s design significantly differs from ReduNet, both aim for a similar goal, showcasing the versatility of the unrolled optimization approach. Remarkably, CRATE closely resembles the transformer architecture, suggesting that the interpretability gains from such networks might also improve our understanding of current, practical deep architectures.</p><h3 id="sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-ii"> <a href="#sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-ii" class="anchor-heading" aria-labelledby="sparse-modeling-for-image-reconstruction-and-restoration--intro-to-deep-learning-for-imaging-part-ii"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sparse Modeling for Image Reconstruction and Restoration &amp; Intro to Deep Learning for Imaging (Part II)</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/wen.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen</a></h3><p>Nanyang Technological University</p><p class="organizer-meta">Nanyang Assistant Professor</p></div></div><p>Topics covered:<br />• Non-local image modeling, e.g., group sparsity, low-rank, etc.<br />• From model-based to deep learning.<br />• Advantages of model scalability.<br />• Example of deep learning methods for image restoration (Session 2 will cover deep learning methods in more detail for imaging).<br />• How deep learning can be combined with model-based approaches.</p><h3 id="deep-learning-for-imaging"> <a href="#deep-learning-for-imaging" class="anchor-heading" aria-labelledby="deep-learning-for-imaging"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Deep Learning for Imaging</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/sr.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a></h3><p>Michigan State University</p><p class="organizer-meta">Assistant Professor</p></div></div><p>Topics covered:<br />• Recap and introduction to deep learning methods for image reconstruction. Focus on physics-based deep learning methods.<br />• Unsupervised deep learning methods: plug and play (PnP) priors, consensus equilibrium, regularization by denoising.<br />• Supervised learning methods: deep unrolling, deep equilibrium methods, bilevel optimization methods.<br />• Generative models for image reconstruction: deep image prior, GANs, diffusion models.<br />• Other trends: deep reinforcement learning for imaging, unified deep learning and sparse modeling, ensuring robustness of deep learning based image reconstruction to various perturbations (e.g., via randomized smoothing, diffusion models, etc.), learning sparse neural networks, joint training of sensing and image reconstruction.<br />• Conclusions and future directions in the field.</p><h3 id="understanding-deep-representation-learning-via-neural-collapse"> <a href="#understanding-deep-representation-learning-via-neural-collapse" class="anchor-heading" aria-labelledby="understanding-deep-representation-learning-via-neural-collapse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding Deep Representation Learning via Neural Collapse</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/zz.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://zhihuizhu.github.io">Zhihui Zhu</a></h3><p>Ohio State University</p><p class="organizer-meta">Assistant Professor</p></div></div><p>The session focuses on the strong conceptual connections between low-dimensional structures and deep models in terms of learned representation. We start with the introduction of an intriguing Neural Collapse phenomenon in the last-layer representation and its universality in deep network, and lays out the mathematical foundations of understanding its cause by studying its optimization landscapes. We then generalize and explain this phenomenon and its implications under data imbalanceness. Furthermore, we demonstrate the practical algorithmic implications of Neural Collapse on training deep neural networks.</p><h3 id="invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks"> <a href="#invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks" class="anchor-heading" aria-labelledby="invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Networks</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/qq.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://qingqu.engin.umich.edu/">Qing Qu</a></h3><p>University of Michigan</p><p class="organizer-meta">Assistant Professor</p></div></div><p>To conclude, we show that low-dimensional structures also emerge in training dynamics of deep networks. Specifically, we show that the evolution of gradient descent only affects a minimal portion of singular vector spaces across all weight matrices. The analysis enables us to considerably improve training efficiency by taking advantage of the low-dimensional structure in learning dynamics. We can construct smaller, equivalent deep linear networks without sacrificing the benefits associated with the wider counterparts. Moreover, it allows us to better understand deep representation learning by elucidating the progressive feature compression and discrimination from shallow to deep layers.</p></main></div></div><div class="search-overlay"></div></div>
