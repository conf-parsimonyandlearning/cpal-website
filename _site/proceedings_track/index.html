<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Proceedings Track | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Proceedings Track" /><meta property="og:locale" content="en_US" /><meta name="description" content="Accepted papers for CPAL 2024 Proceedings Track" /><meta property="og:description" content="Accepted papers for CPAL 2024 Proceedings Track" /><link rel="canonical" href="https://cpal.cc/proceedings_track/" /><meta property="og:url" content="https://cpal.cc/proceedings_track/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Proceedings Track" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Accepted papers for CPAL 2024 Proceedings Track","headline":"Proceedings Track","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/proceedings_track/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">Venue</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings/" class="nav-list-link">Conference Proceedings</a><li class="nav-list-item active"> <a href="/proceedings_track/" class="nav-list-link active">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/program_highlights/" class="nav-list-link">Program Highlights</a><li class="nav-list-item "> <a href="/oral_and_spotlight_presentations/" class="nav-list-link">Orals and Recent Spotlights</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a><li class="nav-list-item "> <a href="/tutorials/" class="nav-list-link">Tutorials</a><li class="nav-list-item "> <a href="/wellness/" class="nav-list-link">Tailored Wellness Sessions</a><li class="nav-list-item "> <a href="/social/" class="nav-list-link">Social Events</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://proceedings.mlr.press/v234/" class="site-button" > CPAL Proceedings </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/accepted_papers/">Accepted Papers</a><li class="breadcrumb-nav-list-item"><span>Proceedings Track</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="proceedings-track-accepted-papers"> <a href="#proceedings-track-accepted-papers" class="anchor-heading" aria-labelledby="proceedings-track-accepted-papers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proceedings Track: Accepted Papers</h1><h2 id="presentation-format"> <a href="#presentation-format" class="anchor-heading" aria-labelledby="presentation-format"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presentation Format</h2><p>Accepted papers will be presented in one of six oral sessions during the conference.</p><p>Presentations are ten minutes in duration, with two minutes for Q&amp;A.</p><p>The ordering of session numbers matches their chronological ordering, and presentations will be delivered in the order they are listed. See the <a href="/program_schedule/">full program</a> for the precise time and location of each oral session.</p><h2 id="oral-session-1"> <a href="#oral-session-1" class="anchor-heading" aria-labelledby="oral-session-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 1</h2><h4 id="time-day-1-jan-3--wednesday--230-pm-to-330-pm"> <a href="#time-day-1-jan-3--wednesday--230-pm-to-330-pm" class="anchor-heading" aria-labelledby="time-day-1-jan-3--wednesday--230-pm-to-330-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 1 (Jan 3) – Wednesday – 2:30 PM to 3:30 PM</a></h4><h3 id="1-emergence-of-segmentation-with-minimalistic-white-box-transformers"> <a href="#1-emergence-of-segmentation-with-minimalistic-white-box-transformers" class="anchor-heading" aria-labelledby="1-emergence-of-segmentation-with-minimalistic-white-box-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=kmzH8kT9TE">Emergence of Segmentation with Minimalistic White-Box Transformers</a></h3><p>Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, Yi Ma</p><p class="fs-2">Keywords: <em>white-box transformer, emergence of segmentation properties</em></p><h3 id="2-neuromixgdp-a-neural-collapse-inspired-random-mixup-for-private-data-release"> <a href="#2-neuromixgdp-a-neural-collapse-inspired-random-mixup-for-private-data-release" class="anchor-heading" aria-labelledby="2-neuromixgdp-a-neural-collapse-inspired-random-mixup-for-private-data-release"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=HyOziZFh5x">NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release</a></h3><p>Donghao Li, Yang Cao, Yuan Yao</p><p class="fs-2">Keywords: <em>Neural Collapse, Differential privacy, Private data publishing, Mixup</em></p><h3 id="3-hrbp-hardware-friendly-regrouping-towards-block-based-pruning-for-sparse-cnn-training"> <a href="#3-hrbp-hardware-friendly-regrouping-towards-block-based-pruning-for-sparse-cnn-training" class="anchor-heading" aria-labelledby="3-hrbp-hardware-friendly-regrouping-towards-block-based-pruning-for-sparse-cnn-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=VP1Xrdz0Bp">HRBP: Hardware-friendly Regrouping towards Block-based Pruning for Sparse CNN Training</a></h3><p>Haoyu Ma, Chengming Zhang, lizhi xiang, Xiaolong Ma, Geng Yuan, Wenkai Zhang, Shiwei Liu, Tianlong Chen, Dingwen Tao, Yanzhi Wang, Zhangyang Wang, Xiaohui Xie</p><p class="fs-2">Keywords: <em>efficient training, sparse training, fine-grained structured sparsity, regrouping algorithm</em></p><h3 id="4-jaxpruner-a-concise-library-for-sparsity-research"> <a href="#4-jaxpruner-a-concise-library-for-sparsity-research" class="anchor-heading" aria-labelledby="4-jaxpruner-a-concise-library-for-sparsity-research"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=H2rCZCfXkS">Jaxpruner: A Concise Library for Sparsity Research</a></h3><p>Joo Hyung Lee, Wonpyo Park, Nicole Elyse Mitchell, Jonathan Pilault, Johan Samir Obando Ceron, Han-Byul Kim, Namhoon Lee, Elias Frantar, Yun Long, Amir Yazdanbakhsh, Woohyun Han, Shivani Agrawal, Suvinay Subramanian, Xin Wang, Sheng-Chun Kao, Xingyao Zhang, Trevor Gale, Aart J.C. Bik, Milen Ferev, Zhonglin Han, Hong-Seok Kim, Yann Dauphin, Gintare Karolina Dziugaite, Pablo Samuel Castro, Utku Evci</p><p class="fs-2">Keywords: <em>jax, sparsity, pruning, quantization, sparse training, efficiency, library, software</em></p><h3 id="5-how-to-prune-your-language-model-recovering-accuracy-on-the-sparsity-may-cry-benchmark"> <a href="#5-how-to-prune-your-language-model-recovering-accuracy-on-the-sparsity-may-cry-benchmark" class="anchor-heading" aria-labelledby="5-how-to-prune-your-language-model-recovering-accuracy-on-the-sparsity-may-cry-benchmark"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=FG8b2I2AkF">How to Prune Your Language Model: Recovering Accuracy on the ``Sparsity May Cry’’ Benchmark</a></h3><p>Eldar Kurtic, Torsten Hoefler, Dan Alistarh</p><p class="fs-2">Keywords: <em>pruning, deep learning, benchmarking</em></p><h2 id="oral-session-2"> <a href="#oral-session-2" class="anchor-heading" aria-labelledby="oral-session-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 2</h2><h4 id="time-day-2-jan-4--thursday--1120-am-to-1220-pm"> <a href="#time-day-2-jan-4--thursday--1120-am-to-1220-pm" class="anchor-heading" aria-labelledby="time-day-2-jan-4--thursday--1120-am-to-1220-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 2 (Jan 4) – Thursday – 11:20 AM to 12:20 PM</a></h4><h3 id="1-efficiently-disentangle-causal-representations"> <a href="#1-efficiently-disentangle-causal-representations" class="anchor-heading" aria-labelledby="1-efficiently-disentangle-causal-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=p8WpFhcKsK">Efficiently Disentangle Causal Representations</a></h3><p>Yuanpeng Li, Joel Hestness, Mohamed Elhoseiny, Liang Zhao, Kenneth Church</p><p class="fs-2">Keywords: <em>causal representation learning</em></p><h3 id="2-unsupervised-learning-of-structured-representation-via-closed-loop-transcription"> <a href="#2-unsupervised-learning-of-structured-representation-via-closed-loop-transcription" class="anchor-heading" aria-labelledby="2-unsupervised-learning-of-structured-representation-via-closed-loop-transcription"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=MF35ZwzpY5">Unsupervised Learning of Structured Representation via Closed-Loop Transcription</a></h3><p>Shengbang Tong, Xili Dai, Yubei Chen, Mingyang Li, ZENGYI LI, Brent Yi, Yann LeCun, Yi Ma</p><p class="fs-2">Keywords: <em>Unsupervised/Self-supervised Learning, Closed-Loop Transcription</em></p><h3 id="3-an-adaptive-tangent-feature-perspective-of-neural-networks"> <a href="#3-an-adaptive-tangent-feature-perspective-of-neural-networks" class="anchor-heading" aria-labelledby="3-an-adaptive-tangent-feature-perspective-of-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=NFqCYfoLw6">An Adaptive Tangent Feature Perspective of Neural Networks</a></h3><p>Daniel LeJeune, Sina Alemohammad</p><p class="fs-2">Keywords: <em>adaptive, kernel learning, tangent kernel, neural networks, low rank</em></p><h3 id="4-sparse-activations-with-correlated-weights-in-cortex-inspired-neural-networks"> <a href="#4-sparse-activations-with-correlated-weights-in-cortex-inspired-neural-networks" class="anchor-heading" aria-labelledby="4-sparse-activations-with-correlated-weights-in-cortex-inspired-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=cyMsUO5J7U">Sparse Activations with Correlated Weights in Cortex-Inspired Neural Networks</a></h3><p>Chanwoo Chun, Daniel Lee</p><p class="fs-2">Keywords: <em>Correlated weights, Biological neural network, Cortex, Neural network gaussian process, Sparse neural network, Bayesian neural network, Generalization theory, Kernel ridge regression, Deep neural network, Random neural network</em></p><h3 id="5-exploring-minimally-sufficient-representation-in-active-learning-through-label-irrelevant-patch-augmentation"> <a href="#5-exploring-minimally-sufficient-representation-in-active-learning-through-label-irrelevant-patch-augmentation" class="anchor-heading" aria-labelledby="5-exploring-minimally-sufficient-representation-in-active-learning-through-label-irrelevant-patch-augmentation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=MlgnGWdqWl">Exploring Minimally Sufficient Representation in Active Learning through Label-Irrelevant Patch Augmentation</a></h3><p>Zhiyu Xue, Yinlong Dai, Qi Lei</p><p class="fs-2">Keywords: <em>Active Learning, Data Augmentation, Minimally Sufficient Representation</em></p><h2 id="oral-session-3"> <a href="#oral-session-3" class="anchor-heading" aria-labelledby="oral-session-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 3</h2><h4 id="time-day-2-jan-4--thursday--230-pm-to-330-pm"> <a href="#time-day-2-jan-4--thursday--230-pm-to-330-pm" class="anchor-heading" aria-labelledby="time-day-2-jan-4--thursday--230-pm-to-330-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 2 (Jan 4) – Thursday – 2:30 PM to 3:30 PM</a></h4><h3 id="1-investigating-the-catastrophic-forgetting-in-multimodal-large-language-model-fine-tuning"> <a href="#1-investigating-the-catastrophic-forgetting-in-multimodal-large-language-model-fine-tuning" class="anchor-heading" aria-labelledby="1-investigating-the-catastrophic-forgetting-in-multimodal-large-language-model-fine-tuning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=g7rMSiNtmA">Investigating the Catastrophic Forgetting in Multimodal Large Language Model Fine-Tuning</a></h3><p>Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma</p><p class="fs-2">Keywords: <em>Multimodal LLM, Supervised Fine-Tuning, Catastrophic Forgetting</em></p><h3 id="2-ws-ifsd-weakly-supervised-incremental-few-shot-object-detection-without-forgetting"> <a href="#2-ws-ifsd-weakly-supervised-incremental-few-shot-object-detection-without-forgetting" class="anchor-heading" aria-labelledby="2-ws-ifsd-weakly-supervised-incremental-few-shot-object-detection-without-forgetting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=y2ozeixGaU">WS-iFSD: Weakly Supervised Incremental Few-shot Object Detection Without Forgetting</a></h3><p>Xinyu Gong, Li Yin, Juan-Manuel Perez-Rua, Zhangyang Wang, Zhicheng Yan</p><p class="fs-2">Keywords: <em>few-shot object detection</em></p><h3 id="3-continual-learning-with-dynamic-sparse-training-exploring-algorithms-for-effective-model-updates"> <a href="#3-continual-learning-with-dynamic-sparse-training-exploring-algorithms-for-effective-model-updates" class="anchor-heading" aria-labelledby="3-continual-learning-with-dynamic-sparse-training-exploring-algorithms-for-effective-model-updates"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=kkz4BbquBy">Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates</a></h3><p>Murat Onur Yildirim, Elif Ceren Gok, Ghada Sokar, Decebal Constantin Mocanu, Joaquin Vanschoren</p><p class="fs-2">Keywords: <em>continual learning, sparse neural networks, dynamic sparse training</em></p><h3 id="4-fixed-frustratingly-easy-domain-generalization-with-mixup"> <a href="#4-fixed-frustratingly-easy-domain-generalization-with-mixup" class="anchor-heading" aria-labelledby="4-fixed-frustratingly-easy-domain-generalization-with-mixup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=iW26qcPlui">FIXED: Frustratingly Easy Domain Generalization with Mixup</a></h3><p>Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, Xing Xie</p><p class="fs-2">Keywords: <em>Domain generalization, Data Augmentation, Out-of-distribution generalization</em></p><h3 id="5-domain-generalization-via-nuclear-norm-regularization"> <a href="#5-domain-generalization-via-nuclear-norm-regularization" class="anchor-heading" aria-labelledby="5-domain-generalization-via-nuclear-norm-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=hJd66ZzXEZ">Domain Generalization via Nuclear Norm Regularization</a></h3><p>Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang</p><p class="fs-2">Keywords: <em>Domain Generalization, Nuclear Norm, Deep Learning</em></p><h2 id="oral-session-4"> <a href="#oral-session-4" class="anchor-heading" aria-labelledby="oral-session-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 4</h2><h4 id="time-day-3-jan-5--friday--1120-am-to-1220-pm"> <a href="#time-day-3-jan-5--friday--1120-am-to-1220-pm" class="anchor-heading" aria-labelledby="time-day-3-jan-5--friday--1120-am-to-1220-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Jan 5) – Friday – 11:20 AM to 12:20 PM</a></h4><h3 id="1-balance-is-essence-accelerating-sparse-training-via-adaptive-gradient-correction"> <a href="#1-balance-is-essence-accelerating-sparse-training-via-adaptive-gradient-correction" class="anchor-heading" aria-labelledby="1-balance-is-essence-accelerating-sparse-training-via-adaptive-gradient-correction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=Pby7pqtBYN">Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction</a></h3><p>Bowen Lei, Dongkuan Xu, Ruqi Zhang, Shuren He, Bani Mallick</p><p class="fs-2">Keywords: <em>Sparse Training, Space-time Co-efficiency, Acceleration, Stability, Gradient Correction</em></p><h3 id="2-probing-biological-and-artificial-neural-networks-with-task-dependent-neural-manifolds"> <a href="#2-probing-biological-and-artificial-neural-networks-with-task-dependent-neural-manifolds" class="anchor-heading" aria-labelledby="2-probing-biological-and-artificial-neural-networks-with-task-dependent-neural-manifolds"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=MxBS6aw5Gd">Probing Biological and Artificial Neural Networks with Task-dependent Neural Manifolds</a></h3><p>Michael Kuoch, Chi-Ning Chou, Nikhil Parthasarathy, Joel Dapello, James J. DiCarlo, Haim Sompolinsky, SueYeon Chung</p><p class="fs-2">Keywords: <em>Computational Neuroscience, Neural Manifolds, Neural Geometry, Representational Geometry, Biologically inspired vision models, Neuro-AI</em></p><h3 id="3-decoding-micromotion-in-low-dimensional-latent-spaces-from-stylegan"> <a href="#3-decoding-micromotion-in-low-dimensional-latent-spaces-from-stylegan" class="anchor-heading" aria-labelledby="3-decoding-micromotion-in-low-dimensional-latent-spaces-from-stylegan"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=jRVS6C3Wia">Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN</a></h3><p>Qiucheng Wu, Yifan Jiang, Junru Wu, Kai Wang, Eric Zhang, Humphrey Shi, Zhangyang Wang, Shiyu Chang</p><p class="fs-2">Keywords: <em>generative model, low-rank decomposition</em></p><h3 id="4-sparse-fréchet-sufficient-dimension-reduction-via-nonconvex-optimization"> <a href="#4-sparse-fréchet-sufficient-dimension-reduction-via-nonconvex-optimization" class="anchor-heading" aria-labelledby="4-sparse-fréchet-sufficient-dimension-reduction-via-nonconvex-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=uE1C3im4wF">Sparse Fréchet sufficient dimension reduction via nonconvex optimization</a></h3><p>Jiaying Weng, Chenlu Ke, Pei Wang</p><p class="fs-2">Keywords: <em>Fréchet regression; minimax concave penalty; multitask regression; sufficient dimension reduction; sufficient variable selection.</em></p><h3 id="5-less-is-more--towards-parsimonious-multi-task-models-using-structured-sparsity"> <a href="#5-less-is-more--towards-parsimonious-multi-task-models-using-structured-sparsity" class="anchor-heading" aria-labelledby="5-less-is-more--towards-parsimonious-multi-task-models-using-structured-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=0VU6Vlh0zy">Less is More – Towards parsimonious multi-task models using structured sparsity</a></h3><p>Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</p><p class="fs-2">Keywords: <em>Multi-task learning, structured sparsity, group sparsity, parameter pruning, semantic segmentation, depth estimation, surface normal estimation</em></p><h2 id="oral-session-5"> <a href="#oral-session-5" class="anchor-heading" aria-labelledby="oral-session-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 5</h2><h4 id="time-day-3-jan-5--friday--230-pm-to-330-pm"> <a href="#time-day-3-jan-5--friday--230-pm-to-330-pm" class="anchor-heading" aria-labelledby="time-day-3-jan-5--friday--230-pm-to-330-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Jan 5) – Friday – 2:30 PM to 3:30 PM</a></h4><h3 id="1-deep-self-expressive-learning"> <a href="#1-deep-self-expressive-learning" class="anchor-heading" aria-labelledby="1-deep-self-expressive-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=g2amnrrMP0">Deep Self-expressive Learning</a></h3><p>Chen Zhao, Chun-Guang Li, Wei He, Chong You</p><p class="fs-2">Keywords: <em>Self-Expressive Model; Subspace Clustering; Manifold Clustering</em></p><h3 id="2-pc-x-profound-clustering-via-slow-exemplars"> <a href="#2-pc-x-profound-clustering-via-slow-exemplars" class="anchor-heading" aria-labelledby="2-pc-x-profound-clustering-via-slow-exemplars"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=yhGKPtRoOx">PC-X: Profound Clustering via Slow Exemplars</a></h3><p>Yuangang Pan, Yinghua Yao, Ivor Tsang</p><p class="fs-2">Keywords: <em>Deep clustering, interpretable machine learning, Optimization</em></p><h3 id="3-piecewise-linear-manifolds-for-deep-metric-learning"> <a href="#3-piecewise-linear-manifolds-for-deep-metric-learning" class="anchor-heading" aria-labelledby="3-piecewise-linear-manifolds-for-deep-metric-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=Z0Fk5MyxkY">Piecewise-Linear Manifolds for Deep Metric Learning</a></h3><p>Shubhang Bhatnagar, Narendra Ahuja</p><p class="fs-2">Keywords: <em>Deep metric learning, Unsupervised representation learning</em></p><h3 id="4-algorithm-design-for-online-meta-learning-with-task-boundary-detection"> <a href="#4-algorithm-design-for-online-meta-learning-with-task-boundary-detection" class="anchor-heading" aria-labelledby="4-algorithm-design-for-online-meta-learning-with-task-boundary-detection"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=LzhpSfqSXC">Algorithm Design for Online Meta-Learning with Task Boundary Detection</a></h3><p>Daouda Sow, Sen Lin, Yingbin Liang, Junshan Zhang</p><p class="fs-2">Keywords: <em>online meta-learning, task boundary detection, domain shift, dynamic regret, out of distribution detection</em></p><h3 id="5-hard-hyperplane-arrangement-descent"> <a href="#5-hard-hyperplane-arrangement-descent" class="anchor-heading" aria-labelledby="5-hard-hyperplane-arrangement-descent"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=jMUDBpnZPa">HARD: Hyperplane ARrangement Descent</a></h3><p>Tianjiao Ding, Liangzu Peng, Rene Vidal</p><p class="fs-2">Keywords: <em>hyperplane clustering, subspace clustering, generalized principal component analysis</em></p><h2 id="oral-session-6"> <a href="#oral-session-6" class="anchor-heading" aria-labelledby="oral-session-6"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Oral Session 6</h2><h4 id="time-day-3-jan-5--friday--400-pm-to-500-pm"> <a href="#time-day-3-jan-5--friday--400-pm-to-500-pm" class="anchor-heading" aria-labelledby="time-day-3-jan-5--friday--400-pm-to-500-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Jan 5) – Friday – 4:00 PM to 5:00 PM</a></h4><h3 id="1-closed-loop-transcription-via-convolutional-sparse-coding"> <a href="#1-closed-loop-transcription-via-convolutional-sparse-coding" class="anchor-heading" aria-labelledby="1-closed-loop-transcription-via-convolutional-sparse-coding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. <a href="https://openreview.net/forum?id=1AEJSYO6GX">Closed-Loop Transcription via Convolutional Sparse Coding</a></h3><p>Xili Dai, Ke Chen, Shengbang Tong, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv Pai, Yuexiang Zhai, Xiaojun Yuan, Heung-Yeung Shum, Lionel Ni, Yi Ma</p><p class="fs-2">Keywords: <em>Convolutional Sparse Coding, Inverse Problem, Closed-Loop Transcription</em></p><h3 id="2-leveraging-sparse-input-and-sparse-models-efficient-distributed-learning-in-resource-constrained-environments"> <a href="#2-leveraging-sparse-input-and-sparse-models-efficient-distributed-learning-in-resource-constrained-environments" class="anchor-heading" aria-labelledby="2-leveraging-sparse-input-and-sparse-models-efficient-distributed-learning-in-resource-constrained-environments"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. <a href="https://openreview.net/forum?id=D9ggc3l0wi">Leveraging Sparse Input and Sparse Models: Efficient Distributed Learning in Resource-Constrained Environments</a></h3><p>Emmanouil Kariotakis, Grigorios Tsagkatakis, Panagiotis Tsakalides, Anastasios Kyrillidis</p><p class="fs-2">Keywords: <em>sparse neural network training, efficient training</em></p><h3 id="3-cross-quality-few-shot-transfer-for-alloy-yield-strength-prediction-a-new-materials-science-benchmark-and-a-sparsity-oriented-optimization-framework"> <a href="#3-cross-quality-few-shot-transfer-for-alloy-yield-strength-prediction-a-new-materials-science-benchmark-and-a-sparsity-oriented-optimization-framework" class="anchor-heading" aria-labelledby="3-cross-quality-few-shot-transfer-for-alloy-yield-strength-prediction-a-new-materials-science-benchmark-and-a-sparsity-oriented-optimization-framework"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. <a href="https://openreview.net/forum?id=V7mcfiSjIT">Cross-Quality Few-Shot Transfer for Alloy Yield Strength Prediction: A New Materials Science Benchmark and A Sparsity-Oriented Optimization Framework</a></h3><p>Xuxi Chen, Tianlong Chen, Everardo Yeriel Olivares, Kate Elder, Scott McCall, Aurelien Perron, Joseph McKeown, Bhavya Kailkhura, Zhangyang Wang, Brian Gallagher</p><p class="fs-2">Keywords: <em>AI4Science, sparsity, bi-level optimization</em></p><h3 id="4-deep-leakage-from-model-in-federated-learning"> <a href="#4-deep-leakage-from-model-in-federated-learning" class="anchor-heading" aria-labelledby="4-deep-leakage-from-model-in-federated-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. <a href="https://openreview.net/forum?id=S8MPHInGqj">Deep Leakage from Model in Federated Learning</a></h3><p>Zihao Zhao, Mengen Luo, Wenbo Ding</p><p class="fs-2">Keywords: <em>Federated learning, distributed learning, privacy leakage</em></p><h3 id="5-image-quality-assessment-integrating-model-centric-and-data-centric-approaches"> <a href="#5-image-quality-assessment-integrating-model-centric-and-data-centric-approaches" class="anchor-heading" aria-labelledby="5-image-quality-assessment-integrating-model-centric-and-data-centric-approaches"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. <a href="https://openreview.net/forum?id=GLQJH92m1m">Image Quality Assessment: Integrating Model-centric and Data-centric Approaches</a></h3><p>Peibei Cao, Dingquan Li, Kede Ma</p><p class="fs-2">Keywords: <em>Learning-based IQA, model-centric IQA, data-centric IQA, sampling-worthiness.</em></p></main></div></div><div class="search-overlay"></div></div>
