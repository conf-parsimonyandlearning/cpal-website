<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>List of Tutorials | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="List of Tutorials" /><meta property="og:locale" content="en_US" /><meta name="description" content="Information about first-day tutorials at CPAL 2025" /><meta property="og:description" content="Information about first-day tutorials at CPAL 2025" /><link rel="canonical" href="https://cpal.cc/tutorial_info/" /><meta property="og:url" content="https://cpal.cc/tutorial_info/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="List of Tutorials" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Information about first-day tutorials at CPAL 2025","headline":"List of Tutorials","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/tutorial_info/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/orals/" class="nav-list-link">Oral Presentations</a><li class="nav-list-item "> <a href="/posters/" class="nav-list-link">Poster Presentations</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="true"> Tutorials </button><ul class="nav-list"><li class="nav-list-item active"> <a href="/tutorial_info/" class="nav-list-link active">List of Tutorials</a><li class="nav-list-item "> <a href="/tutorial_call/" class="nav-list-link">Call for Tutorials</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Call for Applications</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> Conference Sponsors </button><ul class="nav-list"><li class="nav-list-item "> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item "> <a href="/sponsorship_opportunities/" class="nav-list-link">Sponsorship Opportunities</a></ul><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a><li class="nav-list-item"> <a href="/other_years/" class="nav-list-link">Past CPAL Websites</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://cvent.me/X5aaar" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2025" class="site-button" > CPAL OpenReview </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/tutorials/">Tutorials</a><li class="breadcrumb-nav-list-item"><span>List of Tutorials</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/stanford.jpg" alt="Splash photo of Stanford" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> March 2025,&nbsp;Stanford</div></div><h1 id="general-information"> <a href="#general-information" class="anchor-heading" aria-labelledby="general-information"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> General Information</h1><p class="highlight fs-5">Attendees must <a href="/registration">register</a> for the tutorial package to attend the tutorials.</p><p>The first day of CPAL 2025 features six tutorial presentations, arranged into two parallel tracks across three sessions throughout the day. Each tutorial consists of 2.5 hours of lectures from leading experts in the intersection between low-dimensional modeling and deep learning, with topics ranging from theory to practice and presentations in an accessible format. See the <a href="/program_schedule">schedule</a> for the precise times of each tutorial.</p><p>The content of the six tutorials is summarized below. Check back soon for a more detailed account of lectures, as well as materials from the tutorials after the conference.</p><h1 id="list-of-tutorials"> <a href="#list-of-tutorials" class="anchor-heading" aria-labelledby="list-of-tutorials"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> List of Tutorials</h1><h2 id="t1-deep-representation-learning-from-knowledge-to-intelligence"> <a href="#t1-deep-representation-learning-from-knowledge-to-intelligence" class="anchor-heading" aria-labelledby="t1-deep-representation-learning-from-knowledge-to-intelligence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T1: Deep Representation Learning: from Knowledge to Intelligence</h2><h4 id="presenters-sam-buchanan-ttic-yi-ma-hku-uc-berkeley-druv-pai-uc-berkeley-peng-wang-university-of-michigan"> <a href="#presenters-sam-buchanan-ttic-yi-ma-hku-uc-berkeley-druv-pai-uc-berkeley-peng-wang-university-of-michigan" class="anchor-heading" aria-labelledby="presenters-sam-buchanan-ttic-yi-ma-hku-uc-berkeley-druv-pai-uc-berkeley-peng-wang-university-of-michigan"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Sam Buchanan (TTIC), Yi Ma (HKU, UC Berkeley), Druv Pai (UC Berkeley), Peng Wang (University of Michigan)</h4><h4 id="abstract"> <a href="#abstract" class="anchor-heading" aria-labelledby="abstract"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>Modern generative AI systems, powered by deep learning, are currently revolutionizing science and engineering. At the same time, a fundamental lack of transparency in the way these systems learn from data and make decisions presents safety concerns of ever-increasing severity. This tutorial aims to provide a rigorous and systematic overview of the mathematical and computational principles of deep learning. The tutorial will commence with a broad overview of the history and current state of the field of AI, and then focus in on an important common problem: how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structured representation, referred to as a memory. The tutorial will show, starting from classical linear models and building up to general data distributions, how a general framework for learning structured memories arises from a universal computational principle: compression. Particularly, popular approaches for representation learning (such as denoising score-matching) as well as modern network architectures (such as residual networks and transformers) can be understood and improved as (unrolled) optimization algorithms to achieve better compression and representation. To chart a path beyond the current practices of representation learning towards developing truly autonomous and intelligent systems, the tutorial will conclude with a discussion of effective autoencoding architectures centered around the powerful framework of closed-loop transcription, enabling networks to self-improve via closed-loop feedback. Concepts will be illustrated throughout with experiments on supervised and unsupervised learning of diverse data modalities.</p><h2 id="t2-foundations-on-interpretable-ai"> <a href="#t2-foundations-on-interpretable-ai" class="anchor-heading" aria-labelledby="t2-foundations-on-interpretable-ai"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T2: Foundations on Interpretable AI</h2><h4 id="presenters-rene-vidal-university-of-pennsylvania-jeremias-sulam-johns-hopkins-university-aditya-chattopadhyay-amazon"> <a href="#presenters-rene-vidal-university-of-pennsylvania-jeremias-sulam-johns-hopkins-university-aditya-chattopadhyay-amazon" class="anchor-heading" aria-labelledby="presenters-rene-vidal-university-of-pennsylvania-jeremias-sulam-johns-hopkins-university-aditya-chattopadhyay-amazon"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Rene Vidal (University of Pennsylvania), Jeremias Sulam (Johns Hopkins University), Aditya Chattopadhyay (Amazon)</h4><h4 id="abstract-1"> <a href="#abstract-1" class="anchor-heading" aria-labelledby="abstract-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>In recent years, interpretability has emerged as a significant barrier to the widespread adoption of deep learning techniques, particularly in domains where AI decisions can have consequential impacts on human lives, such as healthcare and finance. Recent attempts at interpreting the decisions made by a deep network can be broadly classified in two categories, (i) methods that seek to explain existing models (post-hoc explainability), and (ii) methods that seek to build models that are explainable by design. This tutorial aims to provide a comprehensive overview of both approaches along with a discussion on their limitations.</p><h2 id="t3-methods-analysis-and-insights-from-multimodal-llm-pre-training-and-post-training"> <a href="#t3-methods-analysis-and-insights-from-multimodal-llm-pre-training-and-post-training" class="anchor-heading" aria-labelledby="t3-methods-analysis-and-insights-from-multimodal-llm-pre-training-and-post-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T3: Methods, Analysis, and Insights from Multimodal LLM Pre-training and Post-training</h2><h4 id="presenters-zhe-gan-apple-haotian-zhang-apple"> <a href="#presenters-zhe-gan-apple-haotian-zhang-apple" class="anchor-heading" aria-labelledby="presenters-zhe-gan-apple-haotian-zhang-apple"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Zhe Gan (Apple), Haotian Zhang (Apple)</h4><h4 id="abstract-2"> <a href="#abstract-2" class="anchor-heading" aria-labelledby="abstract-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>Multimodal Large Language Models (LLMs) have become an increasing hot research topic. In this tutorial, we will present how to build performant multimodal LLMs, along several fronts: (1) multimodal LLM pre-training, with focus on crucial design lessons for model architectures and pre-training data choices; (2) multimodal LLM post-training, with focus on text-rich image understanding, visual referring and grounding, and multi-image reasoning; (3) visual encoder pre-training, with focus on training objective design and pre-training data curation; and (4) generalist UI agents, with focus on how to adapt multimodal LLMs into generalist UI agents capable of control digital devices to complete human tasks.</p><h2 id="t4-harnessing-low-dimensionality-in-diffusion-models-from-theory-to-practice"> <a href="#t4-harnessing-low-dimensionality-in-diffusion-models-from-theory-to-practice" class="anchor-heading" aria-labelledby="t4-harnessing-low-dimensionality-in-diffusion-models-from-theory-to-practice"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T4: Harnessing Low Dimensionality in Diffusion Models: From Theory to Practice</h2><h4 id="presenters-yuxin-chen-university-of-pennsylvania-wharton-qing-qu-university-of-michigan-liyue-shen-university-of-michigan"> <a href="#presenters-yuxin-chen-university-of-pennsylvania-wharton-qing-qu-university-of-michigan-liyue-shen-university-of-michigan" class="anchor-heading" aria-labelledby="presenters-yuxin-chen-university-of-pennsylvania-wharton-qing-qu-university-of-michigan-liyue-shen-university-of-michigan"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Yuxin Chen (University of Pennsylvania, Wharton), Qing Qu (University of Michigan), Liyue Shen (University of Michigan)</h4><h4 id="abstract-3"> <a href="#abstract-3" class="anchor-heading" aria-labelledby="abstract-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>Diffusion models have recently gained attention as a powerful class of deep generative models, achieving state-of-the-art results in data generation tasks. In a nutshell, they are designed to learn an unknown data distribution starting from Gaussian noise, mimicking the process of non-equilibrium thermodynamic diffusion. Despite their outstanding empirical successes, the mathematical and algorithmic foundations of diffusion models remain far from mature. For instance: (i) Generalization: it remains unclear how diffusion models, trained on finite samples, can generate new and meaningful data that differ from the training set; (ii) Efficiency: due to the enormous model capacity and the requirement of many sampling steps, they often suffer from slow training and sampling speeds; (iii) Controllability: it remains computationally challenging to guide and control diffusion models for solving inverse problems across many scientific imaging applications, due to the necessity of data consistency and the limitations imposed by limited and noisy measurements. This tutorial will introduce a mathematical framework for understanding the generalization and improving the efficiency of diffusion models, through exploring the low-dimensional structures in both the data and model. We show how to overcome fundamental barriers to improve the generalization, efficiency, and controllability in developing diffusion models, by exploring how these models adaptively learn underlying data distributions, how to achieve faster convergence at the sampling stage, and unveiling the intrinsic properties of the learned denoiser. Leveraging the theoretical studies, we will show how to effectively employ diffusion models for solving inverse problems in scientific imaging applications.</p><h2 id="t5-a-function-space-tour-of-data-science"> <a href="#t5-a-function-space-tour-of-data-science" class="anchor-heading" aria-labelledby="t5-a-function-space-tour-of-data-science"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T5: A Function-Space Tour of Data Science</h2><h4 id="presenters-rahul-parhi-uc-san-diego-greg-ongie-marquette-university"> <a href="#presenters-rahul-parhi-uc-san-diego-greg-ongie-marquette-university" class="anchor-heading" aria-labelledby="presenters-rahul-parhi-uc-san-diego-greg-ongie-marquette-university"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Rahul Parhi (UC San Diego), Greg Ongie (Marquette University)</h4><h4 id="abstract-4"> <a href="#abstract-4" class="anchor-heading" aria-labelledby="abstract-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>Parametric methods aim to explain data with a finite number of learnable parameters. These models are typically applied in settings where the number of data is greater than the number of parameters. Nonparametric methods, on the other hand, model data using infinite-dimensional function spaces and/or allow the number of parameters to grow beyond the number of data (a.k.a. the overparameterized regime). Many classical methods in data science fit into this latter framework, including kernel methods and wavelet methods. Furthermore, modern methods based on overparameterized neural networks also fit into this framework. The common theme being that these methods aim to minimize a quantity in function space. This tutorial will provide a tour of nonparametric methods in data science through the lens of function spaces starting with classical methods such as kernel methods (reproducing kernel Hilbert spaces) and wavelet methods (bounded variation spaces, Besov spaces) and ending with modern, high-dimensional methods such as overparameterized neural networks (variation spaces, Barron spaces). Remarkably, all of these methods can be viewed through the lens of abstract representer theorems (beyond Hilbert spaces). A particular emphasis will be made on the difference between $\ell_2$ regularization (kernel methods) and sparsity-promoting $\ell_1$-regularization (wavelet methods, neural networks) through the concept of adaptivity. For each method/function space, topics such as generalization bounds, metric entropy, and minimax rates will be covered.</p><h2 id="t6-sparsity-and-mixture-of-experts-in-the-era-of-llms-a-new-odyssey"> <a href="#t6-sparsity-and-mixture-of-experts-in-the-era-of-llms-a-new-odyssey" class="anchor-heading" aria-labelledby="t6-sparsity-and-mixture-of-experts-in-the-era-of-llms-a-new-odyssey"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> T6: Sparsity and Mixture-of-Experts in the Era of LLMs: A New Odyssey</h2><h4 id="presenters-shiwei-liu-oxford-tianlong-chen-university-of-north-carolina-chapel-hill-yu-cheng-chinese-university-of-hong-kong"> <a href="#presenters-shiwei-liu-oxford-tianlong-chen-university-of-north-carolina-chapel-hill-yu-cheng-chinese-university-of-hong-kong" class="anchor-heading" aria-labelledby="presenters-shiwei-liu-oxford-tianlong-chen-university-of-north-carolina-chapel-hill-yu-cheng-chinese-university-of-hong-kong"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presenters: Shiwei Liu (Oxford), Tianlong Chen (University of North Carolina, Chapel Hill), Yu Cheng (Chinese University of Hong Kong)</h4><h4 id="abstract-5"> <a href="#abstract-5" class="anchor-heading" aria-labelledby="abstract-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract:</h4><p>Recently, Large Language Models (LLMs) have showcased remarkable generalization capabilities across a plethora of tasks, yielding notable successes. The scale of these models stands out as a pivotal determinant in enhancing LLM performance. However, the escalation in model size significantly amplifies the costs associated with both pre-training and fine-tuning, while simultaneously constraining inference speed. Consequently, there has been a surge in exploration aimed at devising novel techniques for model scaling. Among these, the sparse Mixture-of-Experts (MoE) has garnered considerable attention due to its ability to expedite pre-training and enhance inference speed, especially when compared to dense models with equivalent parameter counts. This tutorial endeavors to offer a comprehensive overview of MoE within the context of LLMs. The discussion commences by revisiting extant research on MoE, elucidating critical challenges encountered within this domain. Subsequent exploration delves into the intricate relationship between MoE and LLMs, encompassing sparse scaling of pre-training models and the conversion of existing dense models into sparse MoE counterparts. Moreover, the tutorial elucidates the broader advantages conferred by MoE beyond mere efficiency. Overall, this tutorial delineates the evolutionary trajectory of MoE within the landscape of LLMs, underscoring its pivotal role in the era of LLMs.</p></main></div></div><div class="search-overlay"></div></div>
