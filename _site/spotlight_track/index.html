<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Spotlight Track | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Spotlight Track" /><meta property="og:locale" content="en_US" /><meta name="description" content="Accepted papers for CPAL 2024 Spotlight Track" /><meta property="og:description" content="Accepted papers for CPAL 2024 Spotlight Track" /><link rel="canonical" href="https://cpal.cc/spotlight_track/" /><meta property="og:url" content="https://cpal.cc/spotlight_track/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Spotlight Track" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Accepted papers for CPAL 2024 Spotlight Track","headline":"Spotlight Track","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/spotlight_track/"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script> <script defer src="/assets/js/mathtex-script-type.js"> </script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body, { globalGroup: true, trust: true, strict: false, throwOnError: false, });"></script><style> .katex { font-size: 1em; }</style><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">CPAL Logistics and Venue</a><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item active"> <a href="/spotlight_track/" class="nav-list-link active">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/orals/" class="nav-list-link">Oral Presentations</a><li class="nav-list-item "> <a href="/posters/" class="nav-list-link">Poster Presentations</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a><li class="nav-list-item "> <a href="/wellness/" class="nav-list-link">Tailored Wellness Sessions</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Call for Applications</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Tutorials category" aria-pressed="false"> Tutorials </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tutorial_info/" class="nav-list-link">List of Tutorials</a><li class="nav-list-item "> <a href="/tutorial_call/" class="nav-list-link">Call for Tutorials</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Sponsors category" aria-pressed="false"> Conference Sponsors </button><ul class="nav-list"><li class="nav-list-item "> <a href="/sponsors/" class="nav-list-link">Sponsors</a></ul><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a><li class="nav-list-item"> <a href="/other_years/" class="nav-list-link">Past CPAL Websites</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://cvent.me/X5aaar" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2025" class="site-button" > CPAL OpenReview </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/accepted_papers/">Accepted Papers</a><li class="breadcrumb-nav-list-item"><span>Spotlight Track</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/stanford.jpg" alt="Splash photo of Stanford" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> March 2025,&nbsp;Stanford</div></div><h1 id="spotlight-track-accepted-papers"> <a href="#spotlight-track-accepted-papers" class="anchor-heading" aria-labelledby="spotlight-track-accepted-papers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spotlight Track: Accepted Papers</h1><p>Accepted Spotlight Track papers are presented as <a href="/posters">posters</a> at CPAL 2025. See the <a href="/program_schedule/">full program</a> for the precise time and location of each poster session.</p><h3 id="stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes"> <a href="#stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes" class="anchor-heading" aria-labelledby="stable-minima-cannot-overfit-in-univariate-relu-networks-generalization-by-large-step-sizes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=0q9E8QiEuu">Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes</a></h3><p>Dan Qiao, Kaiqi Zhang, Esha Singh, Daniel Soudry, Yu-Xiang Wang</p><p class="fs-2">Keywords: <em>Minima Stability, Edge-of-Stability, Generalization, Flat Local Minima, Curvature</em></p><h3 id="principle-component-trees-and-their-persistent-homology"> <a href="#principle-component-trees-and-their-persistent-homology" class="anchor-heading" aria-labelledby="principle-component-trees-and-their-persistent-homology"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=1bfZBI3arx">Principle Component Trees and their Persistent Homology</a></h3><p>Ben Kizaric, Daniel L. Pimentel-Alarcón</p><p class="fs-2">Keywords: <em>subspace clustering, low-rank decomposition, unsupervised learning, manifold learning, dimensionality reduction, topological data analysis</em></p><h3 id="mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln"> <a href="#mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln" class="anchor-heading" aria-labelledby="mix-ln-unleashing-the-power-of-deeper-layers-by-combining-pre-ln-and-post-ln"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=6Urduk1GNx">Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN</a></h3><p>Pengxiang Li, Lu Yin, Shiwei Liu</p><p class="fs-2">Keywords: <em>LayerNorm, LLM, Transformer</em></p><h3 id="geometric-algebra-planes-convex-implicit-neural-volumes"> <a href="#geometric-algebra-planes-convex-implicit-neural-volumes" class="anchor-heading" aria-labelledby="geometric-algebra-planes-convex-implicit-neural-volumes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=6VBVEF70oD">Geometric Algebra Planes: Convex Implicit Neural Volumes</a></h3><p>Irmak Sivgin, Sara Fridovich-Keil, Gordon Wetzstein, Mert Pilanci</p><p class="fs-2">Keywords: <em>Volume representation, tensor decomposition, convex optimization, geometric algebra, nerf</em></p><h3 id="understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing"> <a href="#understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing" class="anchor-heading" aria-labelledby="understanding-and-mitigating-bottlenecks-of-state-space-models-through-the-lens-of-recency-and-over-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=6WQq5KBejG">Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</a></h3><p>Peihao Wang, Ruisi Cai, Yuehao Wang, Jiajun Zhu, Pragya Srivastava, Zhangyang Wang, Pan Li</p><p class="fs-2">Keywords: <em>State Space Models, Large Language Models, Recency, Over-smoothing</em></p><h3 id="rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding"> <a href="#rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding" class="anchor-heading" aria-labelledby="rethinking-addressing-in-language-models-via-contextualized-equivariant-positional-encoding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=6oQDDcW2gY">Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding</a></h3><p>Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang</p><p class="fs-2">Keywords: <em>Positional Encoding, Equivariant Machine Learning, Large Language Models</em></p><h3 id="whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity"> <a href="#whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity" class="anchor-heading" aria-labelledby="whomp-optimizing-randomized-controlled-trials-via-wasserstein-homogeneity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=76H1lRBgup">WHOMP: Optimizing Randomized Controlled Trials via Wasserstein Homogeneity</a></h3><p>Shizhou Xu, Thomas Strohmer</p><p class="fs-2">Keywords: <em>randomized controlled trial, Wasserstein homogeneity, anti-clustering, diverse K-means, control/test group splitting, cross-validation</em></p><h3 id="diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering"> <a href="#diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering" class="anchor-heading" aria-labelledby="diffusion-models-learn-low-dimensional-distributions-via-subspace-clustering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=8VKSMxYKiY">Diffusion models learn low-dimensional distributions via subspace clustering</a></h3><p>Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion models, mixture of low-rank Gaussians, phase transition, subspace clustering</em></p><h3 id="generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping"> <a href="#generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping" class="anchor-heading" aria-labelledby="generative-learning-for-solving-non-convex-problem-with-multi-valued-input-solution-mapping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=8mMqlab1pn">Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping</a></h3><p>Enming Liang, Minghua Chen</p><p class="fs-2">Keywords: <em>Non-convex Optimization, Generative Modeling, Flow, ODE</em></p><h3 id="attention-only-transformers-via-unrolled-subspace-denoising"> <a href="#attention-only-transformers-via-unrolled-subspace-denoising" class="anchor-heading" aria-labelledby="attention-only-transformers-via-unrolled-subspace-denoising"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=AAaP689tAD">Attention-Only Transformers via Unrolled Subspace Denoising</a></h3><p>Peng Wang, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma</p><p class="fs-2">Keywords: <em>transformer, self-attention, unrolled optimization, subspace denoising</em></p><h3 id="learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence"> <a href="#learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence" class="anchor-heading" aria-labelledby="learning-gaussian-multi-index-models-with-gradient-flow-time-complexity-and-directional-convergence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=AvZr0c0Kog">Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence</a></h3><p>Berfin Simsek, Amire Bendjeddou, Daniel Hsu</p><p class="fs-2">Keywords: <em>time complexity, gradient flow dynamics, hardness</em></p><h3 id="on-generalization-bounds-for-neural-networks-with-low-rank-layers"> <a href="#on-generalization-bounds-for-neural-networks-with-low-rank-layers" class="anchor-heading" aria-labelledby="on-generalization-bounds-for-neural-networks-with-low-rank-layers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=BoT0ecevAT">On Generalization Bounds for Neural Networks with Low Rank Layers</a></h3><p>Andrea Pinto, Akshay Rangamani, Tomaso A Poggio</p><p class="fs-2">Keywords: <em>Gaussian Complexity, Low Rank, Neural Collapse</em></p><h3 id="simplifying-dino-by-coding-rate-regularization"> <a href="#simplifying-dino-by-coding-rate-regularization" class="anchor-heading" aria-labelledby="simplifying-dino-by-coding-rate-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=CajE48Bs4h">Simplifying DINO by Coding Rate Regularization</a></h3><p>Ziyang Wu, Jingyuan Zhang, Druv Pai, Yi Ma</p><p class="fs-2">Keywords: <em>Representation Learning, Self Supervised Learning, Coding Rate</em></p><h3 id="knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs"> <a href="#knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs" class="anchor-heading" aria-labelledby="knowledge-aware-parsimony-learning-a-perspective-from-relational-graphs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=FqVEcAVRjm">Knowledge-aware Parsimony Learning: A Perspective from Relational Graphs</a></h3><p>Quanming Yao, Yongqi Zhang, Yaqing Wang, Nan Yin, James Kwok, Qiang Yang</p><p class="fs-2">Keywords: <em>scaling law, Parsimony Learning, Graph Learning</em></p><h3 id="understanding-diffusion-based-representation-learning-via-low-dimensional-modeling"> <a href="#understanding-diffusion-based-representation-learning-via-low-dimensional-modeling" class="anchor-heading" aria-labelledby="understanding-diffusion-based-representation-learning-via-low-dimensional-modeling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=HyESKJgPv6">Understanding Diffusion-based Representation Learning via Low-Dimensional Modeling</a></h3><p>Xiao Li, Zekai Zhang, Xiang Li, Siyi Chen, Zhihui Zhu, Peng Wang, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion representation learning, representation learning, diffusion model</em></p><h3 id="geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics"> <a href="#geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics" class="anchor-heading" aria-labelledby="geometry-of-concepts-in-next-token-prediction-neural-collapse-meets-semantics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Lt6l1woQ84">Geometry of Concepts in Next-token Prediction: Neural-Collapse Meets Semantics</a></h3><p>Yize Zhao, Christos Thrampoulidis</p><p class="fs-2">Keywords: <em>Large Language Models(LLMs), Neural Embeddings, Word Embeddings, Neural-Collapse, Interpretability, Optimization</em></p><h3 id="flowdas-a-flow-based-framework-for-data-assimilation"> <a href="#flowdas-a-flow-based-framework-for-data-assimilation" class="anchor-heading" aria-labelledby="flowdas-a-flow-based-framework-for-data-assimilation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=MF64kI7WKQ">FlowDAS: A Flow-Based Framework for Data Assimilation</a></h3><p>Siyi Chen, Yixuan Jia, Qing Qu, He Sun, Jeffrey A Fessler</p><p class="fs-2">Keywords: <em>Data Assimilation, Stochastic Dynamic System, Flow matching, Stochastic Interpolants, Inverse Problem</em></p><h3 id="whats-in-a-prior-learned-proximal-networks-for-inverse-problems"> <a href="#whats-in-a-prior-learned-proximal-networks-for-inverse-problems" class="anchor-heading" aria-labelledby="whats-in-a-prior-learned-proximal-networks-for-inverse-problems"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=MGiamJl0YD">What’s in a Prior? Learned Proximal Networks for Inverse Problems</a></h3><p>Zhenghan Fang, Sam Buchanan, Jeremias Sulam</p><p class="fs-2">Keywords: <em>Inverse problems, Proximal operators, Plug-and-play, Explicit regularizer, Convergent PnP, Input convex neural networks</em></p><h3 id="pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge"> <a href="#pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge" class="anchor-heading" aria-labelledby="pruning-neural-network-models-for-gene-regulatory-dynamics-using-data-and-domain-knowledge"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=N03O7qEPWI">Pruning neural network models for gene regulatory dynamics using data and domain knowledge</a></h3><p>Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush</p><p class="fs-2">Keywords: <em>sparsification, pruning, lottery tickets, explainability, gene regulation, domain knowledge, neural architecture design, NeuralODEs</em></p><h3 id="certified-robustness-against-sparse-adversarial-perturbations-via-data-localization"> <a href="#certified-robustness-against-sparse-adversarial-perturbations-via-data-localization" class="anchor-heading" aria-labelledby="certified-robustness-against-sparse-adversarial-perturbations-via-data-localization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=O7dYb0VAQw">Certified Robustness against Sparse Adversarial Perturbations via Data Localization</a></h3><p>Ambar Pal, Rene Vidal, Jeremias Sulam</p><p class="fs-2">Keywords: <em>Adversarial Robustness, Certified Robustness, Sparse Perturbations, Data Localization</em></p><h3 id="mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models"> <a href="#mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models" class="anchor-heading" aria-labelledby="mixture-of-transformers-a-sparse-and-scalable-architecture-for-multi-modal-foundation-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=OutjGuJnNk">Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models</a></h3><p>Weixin Liang, LILI YU, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin</p><p class="fs-2">Keywords: <em>Sparse architecture, Efficient deep architecture, Multi-modal foundation models, Mixture-of-Experts, Transformer</em></p><h3 id="provable-probabilistic-imaging-using-score-based-generative-priors"> <a href="#provable-probabilistic-imaging-using-score-based-generative-priors" class="anchor-heading" aria-labelledby="provable-probabilistic-imaging-using-score-based-generative-priors"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=PTdUT7biBO">Provable Probabilistic Imaging using Score-based Generative Priors</a></h3><p>Yu Sun, Zihui Wu, Yifan Chen, Berthy Feng, Katherine Bouman</p><p class="fs-2">Keywords: <em>Diffusion models, inverse problem, image reconstruction, langevin dynamics, markov processes, plug-and-play priors, posterior sampling, regularized inversion, score-based generative models, uncertainty quantification</em></p><h3 id="dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks"> <a href="#dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks" class="anchor-heading" aria-labelledby="dyval-dynamic-evaluation-of-large-language-models-for-reasoning-tasks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=SRB8ricWVr">DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks</a></h3><p>Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie</p><p class="fs-2">Keywords: <em>Large Language Models, Evaluation, Data Contamination</em></p><h3 id="sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry"> <a href="#sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry" class="anchor-heading" aria-labelledby="sparse-training-from-random-initialization-aligning-lottery-ticket-masks-using-weight-symmetry"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=SXxu6BxEGD">Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry</a></h3><p>Mohammed Adnan, Rohan Jain, Ekansh Sharma, Yani Ioannou</p><p class="fs-2">Keywords: <em>Lottery Ticket Hypothesis, sparse training, linear mode connectivity, weight symmetry, deep learning, deep neural networks, random initialization, git re-basin, optimization</em></p><h3 id="shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models"> <a href="#shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models" class="anchor-heading" aria-labelledby="shallow-diffuse-robust-and-invisible-watermarking-through-low-dimensional-subspaces-in-diffusion-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=U44hq1OLDG">Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models</a></h3><p>Wenda Li, Huijie Zhang, Qing Qu</p><p class="fs-2">Keywords: <em>diffusion Model, watermark, low-dimensional subspace, consistency, robustness</em></p><h3 id="a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries"> <a href="#a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries" class="anchor-heading" aria-labelledby="a-robust-kernel-statistical-test-of-invariance-detecting-subtle-asymmetries"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=UaFKI8cjA6">A Robust Kernel Statistical Test of Invariance: Detecting Subtle Asymmetries</a></h3><p>Ashkan Soleymani, Behrooz Tahmasebi, Stefanie Jegelka, Patrick Jaillet</p><p class="fs-2">Keywords: <em>Invariance, Hypothesis Testing, Kernel Methods</em></p><h3 id="wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models"> <a href="#wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models" class="anchor-heading" aria-labelledby="wagle-strategic-weight-attribution-for-effective-and-modular-unlearning-in-large-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Vuak9eodET">WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</a></h3><p>Jinghan Jia, Jiancheng Liu, Yihua Zhang, Parikshit Ram, Nathalie Baracaldo, Sijia Liu</p><p class="fs-2">Keywords: <em>Machine Unlearning, LLMs</em></p><h3 id="masks-signs-and-learning-rate-rewinding"> <a href="#masks-signs-and-learning-rate-rewinding" class="anchor-heading" aria-labelledby="masks-signs-and-learning-rate-rewinding"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=W7Q5q3DEKZ">Masks, Signs, And Learning Rate Rewinding</a></h3><p>Advait Gadhikar, Rebekka Burkholz</p><p class="fs-2">Keywords: <em>sparsity, pruning, lottery tickets, learning rate rewinding, iterative magnitude pruning</em></p><h3 id="active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms"> <a href="#active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms" class="anchor-heading" aria-labelledby="active-dormant-attention-heads-mechanistically-demystifying-extreme-token-phenomena-in-llms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Zx6WUbE9J7">Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</a></h3><p>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei</p><p class="fs-2">Keywords: <em>attention sink, mechanistic interpretability, language models, transformers</em></p><h3 id="geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces"> <a href="#geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces" class="anchor-heading" aria-labelledby="geometry-of-neural-reinforcement-learning-in-continuous-state-and-action-spaces"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=aLGjGWqh8O">Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</a></h3><p>Saket Tiwari, Omer Gottesman, George Konidaris</p><p class="fs-2">Keywords: <em>resinforcement learning, continuous control, geometry</em></p><h3 id="out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers"> <a href="#out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers" class="anchor-heading" aria-labelledby="out-of-distribution-generalization-via-composition-a-lens-through-induction-heads-in-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=cC68jjGq6T">Out-of-distribution generalization via composition: a lens through induction heads in Transformers</a></h3><p>Jiajun Song, Zhuoyan Xu, Yiqiao Zhong</p><p class="fs-2">Keywords: <em>out-of-distribution generalization, low-dimensional subspace, composition, large language models, emergent ability, in-context learning</em></p><h3 id="dynamic-rescaling-for-training-gnns"> <a href="#dynamic-rescaling-for-training-gnns" class="anchor-heading" aria-labelledby="dynamic-rescaling-for-training-gnns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=cFsLHAGnyA">Dynamic Rescaling for Training GNNs</a></h3><p>Nimrah Mustafa, Rebekka Burkholz</p><p class="fs-2">Keywords: <em>graph neural network, rescale invariance, generalization, network balance</em></p><h3 id="sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training"> <a href="#sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training" class="anchor-heading" aria-labelledby="sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=d3E3LWmTar">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</a></h3><p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Sergey Levine, Yi Ma</p><p class="fs-2">Keywords: <em>foundation model post-training</em></p><h3 id="prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis"> <a href="#prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis" class="anchor-heading" aria-labelledby="prior-mismatch-and-adaptation-in-pnp-admm-with-a-nonconvex-convergence-analysis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=gMI4dRErrA">Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis</a></h3><p>Shirin Shoushtari, Jiaming Liu, Edward P. Chandler, M. Salman Asif, Ulugbek S. Kamilov</p><p class="fs-2">Keywords: <em>Computational Imaging, Plug-and-Play Priors, Imaging Inverse Problems, Mismatched Priors, Domain Adaptation</em></p><h3 id="relaxed-contrastive-learning-for-federated-learning"> <a href="#relaxed-contrastive-learning-for-federated-learning" class="anchor-heading" aria-labelledby="relaxed-contrastive-learning-for-federated-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=hduCLXDhS4">Relaxed Contrastive Learning for Federated Learning</a></h3><p>Seonguk Seo, Jinkyu Kim, Geeho Kim, Bohyung Han</p><p class="fs-2">Keywords: <em>dimensional collapse, transferability, federated learning, local deviation</em></p><h3 id="mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity"> <a href="#mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity" class="anchor-heading" aria-labelledby="mixture-of-mamba-enhancing-multi-modal-state-space-models-with-modality-aware-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=i1mxsDrzv3">Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity</a></h3><p>Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, LILI YU</p><p class="fs-2">Keywords: <em>Sparse architecture, Efficient deep architecture, Multi-modal foundation models, Mixture-of-Experts, State Space Model</em></p><h3 id="training-bayesian-neural-networks-with-sparse-subspace-variational-inference"> <a href="#training-bayesian-neural-networks-with-sparse-subspace-variational-inference" class="anchor-heading" aria-labelledby="training-bayesian-neural-networks-with-sparse-subspace-variational-inference"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=jMbvSOtpQD">Training Bayesian Neural Networks with Sparse Subspace Variational Inference</a></h3><p>Junbo Li, Zichen Miao, Qiang Qiu, Ruqi Zhang</p><p class="fs-2">Keywords: <em>Bayesian neural networks, sparse Bayesian learning, variational inference</em></p><h3 id="sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems"> <a href="#sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems" class="anchor-heading" aria-labelledby="sitcom-step-wise-triple-consistent-diffusion-sampling-for-inverse-problems"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=jj2st1jBnY">SITCOM: Step-wise Triple-Consistent Diffusion Sampling for Inverse Problems</a></h3><p>Ismail Alkhouri, Shijun Liang, Cheng-Han Huang, Jimmy Dai, Qing Qu, Saiprasad Ravishankar, Rongrong Wang</p><p class="fs-2">Keywords: <em>Image Restoration, Diffusion Models, Inverse Problems</em></p><h3 id="learning-with-exact-invariances-in-polynomial-time"> <a href="#learning-with-exact-invariances-in-polynomial-time" class="anchor-heading" aria-labelledby="learning-with-exact-invariances-in-polynomial-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=lhO8pJVIjh">Learning with Exact Invariances in Polynomial Time</a></h3><p>Ashkan Soleymani, Behrooz Tahmasebi, Stefanie Jegelka, Patrick Jaillet</p><p class="fs-2">Keywords: <em>Learning with Invariances, Kernels, Spectral Theory</em></p><h3 id="dependence-induced-representations"> <a href="#dependence-induced-representations" class="anchor-heading" aria-labelledby="dependence-induced-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=n6g3dVigo9">Dependence Induced Representations</a></h3><p>Xiangxiang Xu, Lizhong Zheng</p><p class="fs-2">Keywords: <em>representation learning, statistical dependence, maximal correlation, minimal sufficiency, neural collapse</em></p><h3 id="unlocking-global-optimality-in-bilevel-optimization-a-pilot-study"> <a href="#unlocking-global-optimality-in-bilevel-optimization-a-pilot-study" class="anchor-heading" aria-labelledby="unlocking-global-optimality-in-bilevel-optimization-a-pilot-study"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=ph1gNAlFoW">Unlocking Global Optimality in Bilevel Optimization: A Pilot Study</a></h3><p>Quan Xiao, Tianyi Chen</p><p class="fs-2">Keywords: <em>bilevel optimization; global convergence</em></p><h3 id="implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations"> <a href="#implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations" class="anchor-heading" aria-labelledby="implicit-geometry-of-next-token-prediction-from-language-sparsity-patterns-to-model-representations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=qxMTYliGHI">Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations</a></h3><p>Yize Zhao, Tina Behnia, Vala Vakilian, Christos Thrampoulidis</p><p class="fs-2">Keywords: <em>language models, neural embeddings, optimization, implicit regularization, low-rank matrix factorization, support-vector machines</em></p><h3 id="competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents"> <a href="#competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents" class="anchor-heading" aria-labelledby="competeai-understanding-the-competition-dynamics-of-large-language-model-based-agents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=rILX5xHcg4">CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents</a></h3><p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie</p><p class="fs-2">Keywords: <em>LLM-based Agent, Agent Based Modeling, Competition</em></p><h3 id="image-reconstruction-via-autoencoding-sequential-deep-image-prior"> <a href="#image-reconstruction-via-autoencoding-sequential-deep-image-prior" class="anchor-heading" aria-labelledby="image-reconstruction-via-autoencoding-sequential-deep-image-prior"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=s2pVNpUI4b">Image Reconstruction Via Autoencoding Sequential Deep Image Prior</a></h3><p>Ismail Alkhouri, Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar</p><p class="fs-2">Keywords: <em>Image Reconstruction, Deep Image Prior, Generative Models</em></p><h3 id="understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data"> <a href="#understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data" class="anchor-heading" aria-labelledby="understanding-how-nonlinear-networks-create-linearly-separable-features-for-low-dimensional-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=sACJw28GV4">Understanding How Nonlinear Networks Create Linearly Separable Features for Low-Dimensional Data</a></h3><p>Alec S Xu, Can Yaras, Peng Wang, Qing Qu</p><p class="fs-2">Keywords: <em>union of subspaces, shallow nonlinear networks, random feature model</em></p><h3 id="on-the-crucial-role-of-initialization-for-matrix-factorization"> <a href="#on-the-crucial-role-of-initialization-for-matrix-factorization" class="anchor-heading" aria-labelledby="on-the-crucial-role-of-initialization-for-matrix-factorization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=tCrEQpnilb">On the Crucial Role of Initialization for Matrix Factorization</a></h3><p>Bingcong Li, Liang Zhang, Aryan Mokhtari, Niao He</p><p class="fs-2">Keywords: <em>nonconvex optimization, initialization, quadratic rate, low rank adapter, lora</em></p><h3 id="non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity"> <a href="#non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity" class="anchor-heading" aria-labelledby="non-convex-matrix-sensing-breaking-the-quadratic-rank-barrier-in-the-sample-complexity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=tFoVuC5vg1">Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</a></h3><p>Dominik Stöger, Yizhe Zhu</p><p class="fs-2">Keywords: <em>non-convex optimization, factorized gradient descent, matrix sensing, sample complexity, virtual sequences</em></p><h3 id="deep-neural-regression-collapse"> <a href="#deep-neural-regression-collapse" class="anchor-heading" aria-labelledby="deep-neural-regression-collapse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=tIyeKqy2pd">Deep Neural Regression Collapse</a></h3><p>Akshay Rangamani, Altay Unal</p><p class="fs-2">Keywords: <em>Neural Collapse, Regression, Low Rank</em></p><h3 id="visual-prompting-reimagined-the-power-of-activation-prompts"> <a href="#visual-prompting-reimagined-the-power-of-activation-prompts" class="anchor-heading" aria-labelledby="visual-prompting-reimagined-the-power-of-activation-prompts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=thmB4XdOsD">Visual Prompting Reimagined: The Power of Activation Prompts</a></h3><p>Yihua Zhang, Hongkang Li, Yuguang Yao, Aochuan Chen, Shuai Zhang, Pin-Yu Chen, Meng Wang, Sijia Liu</p><p class="fs-2">Keywords: <em>visual prompt, parameter efficient finetuning, learning theory, generalization analysis</em></p><h3 id="primal-dual-spectral-representation-for-off-policy-evaluation"> <a href="#primal-dual-spectral-representation-for-off-policy-evaluation" class="anchor-heading" aria-labelledby="primal-dual-spectral-representation-for-off-policy-evaluation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=uNEuIgkBA3">Primal-Dual Spectral Representation for Off-policy Evaluation</a></h3><p>Yang Hu, Tianyi Chen, Na Li, Kai Wang, Bo Dai</p><p class="fs-2">Keywords: <em>reinforcement learning, off-policy evaluation, spectral representation, primal-dual representation</em></p><h3 id="learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability"> <a href="#learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability" class="anchor-heading" aria-labelledby="learning-dynamics-of-deep-matrix-factorization-beyond-the-edge-of-stability"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=vc0SPDkozY">Learning Dynamics of Deep Matrix Factorization Beyond the Edge of Stability</a></h3><p>Avrajit Ghosh, Soo Min Kwon, Rongrong Wang, Saiprasad Ravishankar, Qing Qu</p><p class="fs-2">Keywords: <em>edge of stability, deep linear networks</em></p><h3 id="characterizing-resnets-universal-approximation-capability"> <a href="#characterizing-resnets-universal-approximation-capability" class="anchor-heading" aria-labelledby="characterizing-resnets-universal-approximation-capability"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=x8fPISanbb">Characterizing ResNet’s Universal Approximation Capability</a></h3><p>Chenghao Liu, Enming Liang, Minghua Chen</p><p class="fs-2">Keywords: <em>universal approximation, ResNet, optimal approximation rate</em></p></main></div></div><div class="search-overlay"></div></div>
