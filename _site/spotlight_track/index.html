<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Spotlight Track | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Spotlight Track" /><meta property="og:locale" content="en_US" /><meta name="description" content="Accepted papers for CPAL 2024 Spotlight Track" /><meta property="og:description" content="Accepted papers for CPAL 2024 Spotlight Track" /><link rel="canonical" href="https://cpal.cc/spotlight_track/" /><meta property="og:url" content="https://cpal.cc/spotlight_track/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Spotlight Track" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Accepted papers for CPAL 2024 Spotlight Track","headline":"Spotlight Track","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/spotlight_track/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="true"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item active"> <a href="/spotlight_track/" class="nav-list-link active">Spotlight Track</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="false"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal-registration" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/accepted_papers/">Accepted Papers</a><li class="breadcrumb-nav-list-item"><span>Spotlight Track</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="spotlight-track-accepted-papers"> <a href="#spotlight-track-accepted-papers" class="anchor-heading" aria-labelledby="spotlight-track-accepted-papers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spotlight Track: Accepted Papers</h1><h2 id="presentation-format"> <a href="#presentation-format" class="anchor-heading" aria-labelledby="presentation-format"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presentation Format</h2><p>Accepted papers will be presented in one of two spotlight poster sessions during the conference.</p><p>The ordering of session numbers matches their chronological ordering. See the <a href="/program_schedule/">full program</a> for the precise time and location of each spotlight presentation session.</p><h2 id="spotlight-poster-session-1"> <a href="#spotlight-poster-session-1" class="anchor-heading" aria-labelledby="spotlight-poster-session-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spotlight Poster Session 1</h2><h4 id="time-day-2-jan-4--thursday--500-pm-to-630-pm"> <a href="#time-day-2-jan-4--thursday--500-pm-to-630-pm" class="anchor-heading" aria-labelledby="time-day-2-jan-4--thursday--500-pm-to-630-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 2 (Jan 4) – Thursday – 5:00 PM to 6:30 PM</a></h4><h3 id="principled-and-efficient-transfer-learning-of-deep-models-via-neural-collapse"> <a href="#principled-and-efficient-transfer-learning-of-deep-models-via-neural-collapse" class="anchor-heading" aria-labelledby="principled-and-efficient-transfer-learning-of-deep-models-via-neural-collapse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=y6mituhLfg">Principled and Efficient Transfer Learning of Deep Models via Neural Collapse</a></h3><p>Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, Qing Qu</p><p class="fs-2">Keywords: <em>representation learning, neural collapse, transfer learning</em></p><h3 id="variational-information-pursuit-for-interpretable-predictions"> <a href="#variational-information-pursuit-for-interpretable-predictions" class="anchor-heading" aria-labelledby="variational-information-pursuit-for-interpretable-predictions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=72GHTWLZKO">Variational Information Pursuit for Interpretable Predictions</a></h3><p>Aditya Chattopadhyay, Kwan Ho Ryan Chan, Benjamin David Haeffele, Donald Geman, Rene Vidal</p><p class="fs-2">Keywords: <em>Interpretable ML, Explainable AI, Information Pursuit</em></p><h3 id="classification-bias-on-a-data-diet"> <a href="#classification-bias-on-a-data-diet" class="anchor-heading" aria-labelledby="classification-bias-on-a-data-diet"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=7n6CQrcJI9">Classification Bias on a Data Diet</a></h3><p>Tejas Pote, Mohammed Adnan, Yigit Yargic, Yani Ioannou</p><p class="fs-2">Keywords: <em>data diet, model bias, classification bias, data pruning</em></p><h3 id="junk-dna-hypothesis-a-task-centric-angle-of-llm-pre-trained-weights-through-sparsity"> <a href="#junk-dna-hypothesis-a-task-centric-angle-of-llm-pre-trained-weights-through-sparsity" class="anchor-heading" aria-labelledby="junk-dna-hypothesis-a-task-centric-angle-of-llm-pre-trained-weights-through-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=AM3mM5fTFh">Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity</a></h3><p>Lu Yin, Shiwei Liu, AJAY KUMAR JAISWAL, Souvik Kundu, Zhangyang Wang</p><p class="fs-2">Keywords: <em>Junk DNA Hypothesis, low-magnitude weights, large-scale language models</em></p><h3 id="fednar-federated-optimization-with-normalized-annealing-regularization"> <a href="#fednar-federated-optimization-with-normalized-annealing-regularization" class="anchor-heading" aria-labelledby="fednar-federated-optimization-with-normalized-annealing-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=CNvOL0msT3">FedNAR: Federated Optimization with Normalized Annealing Regularization</a></h3><p>Junbo Li, Ang Li, Chong Tian, Qirong Ho, Eric Xing, Hongyi Wang</p><p class="fs-2">Keywords: <em>Federated learning, weight decay, adaptive hyperparameters</em></p><h3 id="model-compression-beyond-size-reduction"> <a href="#model-compression-beyond-size-reduction" class="anchor-heading" aria-labelledby="model-compression-beyond-size-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=HO0RdLgQtW">Model Compression Beyond Size Reduction</a></h3><p>Mubarek Mohammed</p><p class="fs-2">Keywords: <em>Knowledge Distillation, Pruning, Model Compression, Neural Networks</em></p><h3 id="block-coordinate-descent-on-smooth-manifolds-convergence-theory-and-twenty-one-examples"> <a href="#block-coordinate-descent-on-smooth-manifolds-convergence-theory-and-twenty-one-examples" class="anchor-heading" aria-labelledby="block-coordinate-descent-on-smooth-manifolds-convergence-theory-and-twenty-one-examples"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=In0Z9hgyuB">Block Coordinate Descent on Smooth Manifolds: Convergence Theory and Twenty-One Examples</a></h3><p>Liangzu Peng, Rene Vidal</p><p class="fs-2">Keywords: <em>Block Coordinate Descent, Alternating Minimization, Non-Convex Optimization, Manifold Optimization, Convergence Analysis</em></p><h3 id="stochastic-collapse-how-gradient-noise-attracts-sgd-dynamics-towards-simpler-subnetworks"> <a href="#stochastic-collapse-how-gradient-noise-attracts-sgd-dynamics-towards-simpler-subnetworks" class="anchor-heading" aria-labelledby="stochastic-collapse-how-gradient-noise-attracts-sgd-dynamics-towards-simpler-subnetworks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Qm9XcfCr2E">Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks</a></h3><p>Feng Chen, Daniel Kunin, Atsushi Yamamura, Surya Ganguli</p><p class="fs-2">Keywords: <em>Implicit Bias, sparsity, SGD Dynamics, Implicit regularization, Learning rate schedule, Stochastic Gradient Descent, Invariant set, Attractive saddle points, Stochastic collapse, Permutation invariance, Simplicity bias, Teacher-student</em></p><h3 id="benign-overfitting-and-grokking-in-relu-networks-for-xor-cluster-data"> <a href="#benign-overfitting-and-grokking-in-relu-networks-for-xor-cluster-data" class="anchor-heading" aria-labelledby="benign-overfitting-and-grokking-in-relu-networks-for-xor-cluster-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Sb3In0kXPK">Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data</a></h3><p>Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, Wei Hu</p><p class="fs-2">Keywords: <em>Grokking, benign overfitting, deep learning</em></p><h3 id="neural-dependencies-emerging-from-learning-massive-categories"> <a href="#neural-dependencies-emerging-from-learning-massive-categories" class="anchor-heading" aria-labelledby="neural-dependencies-emerging-from-learning-massive-categories"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=UhJlC4XzNM">Neural Dependencies Emerging from Learning Massive Categories</a></h3><p>Ruili Feng, Deli Zhao, Zheng-Jun Zha</p><p class="fs-2">Keywords: <em>Deep Learning Theory, Interpretability</em></p><h3 id="specformer-guarding-vision-transformer-robustness-via-maximum-singular-value-penalization"> <a href="#specformer-guarding-vision-transformer-robustness-via-maximum-singular-value-penalization" class="anchor-heading" aria-labelledby="specformer-guarding-vision-transformer-robustness-via-maximum-singular-value-penalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=VkttUQbnrg">SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization</a></h3><p>Xixu HU</p><p class="fs-2">Keywords: <em>Vision Transformer, Adversarial Robustness, Lipschitz Continuity, Computer Vision</em></p><h3 id="on-separability-of-covariance-in-multiway-data-analysis"> <a href="#on-separability-of-covariance-in-multiway-data-analysis" class="anchor-heading" aria-labelledby="on-separability-of-covariance-in-multiway-data-analysis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=XCLUXjTZhf">On Separability of Covariance in Multiway Data Analysis</a></h3><p>Dogyoon Song, Alfred Hero</p><p class="fs-2">Keywords: <em>Multiway data, Separable covariance, Kronecker PCA, Low-rank covariance model, Tensor decomposition, Frank-Wolfe method</em></p><h3 id="deja-vu-contextual-sparsity-for-efficient-llms-at-inference-time"> <a href="#deja-vu-contextual-sparsity-for-efficient-llms-at-inference-time" class="anchor-heading" aria-labelledby="deja-vu-contextual-sparsity-for-efficient-llms-at-inference-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=YJnE4IJ6nK">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a></h3><p>Zichang Liu, Jue WANG, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen</p><p class="fs-2">Keywords: <em>Large language Model; Efficient Inference; Sparsity</em></p><h3 id="towards-a-better-theoretical-understanding-of-independent-subnetwork-training"> <a href="#towards-a-better-theoretical-understanding-of-independent-subnetwork-training" class="anchor-heading" aria-labelledby="towards-a-better-theoretical-understanding-of-independent-subnetwork-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=YeVnr7Nci8">Towards a Better Theoretical Understanding of Independent Subnetwork Training</a></h3><p>Egor Shulgin, Peter Richtárik</p><p class="fs-2">Keywords: <em>Optimization, Distributed Learning, Independent Subnetwork Training, Federated Learning</em></p><h3 id="sparsity-aware-generalization-theory-for-deep-neural-networks"> <a href="#sparsity-aware-generalization-theory-for-deep-neural-networks" class="anchor-heading" aria-labelledby="sparsity-aware-generalization-theory-for-deep-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=b2n5trufKR">Sparsity-aware generalization theory for deep neural networks</a></h3><p>Ramchandran Muthukumar, Jeremias Sulam</p><p class="fs-2">Keywords: <em>Generalization, Sparsity, Sensitivity, PAC-Bayes</em></p><h3 id="gmrlnet-a-graph-based-manifold-regularization-learning-framework-for-placental-insufficiency-diagnosis-on-incomplete-multimodal-ultrasound-data"> <a href="#gmrlnet-a-graph-based-manifold-regularization-learning-framework-for-placental-insufficiency-diagnosis-on-incomplete-multimodal-ultrasound-data" class="anchor-heading" aria-labelledby="gmrlnet-a-graph-based-manifold-regularization-learning-framework-for-placental-insufficiency-diagnosis-on-incomplete-multimodal-ultrasound-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=dEAelrUND8">GMRLNet: A graph-based manifold regularization learning framework for placental insufficiency diagnosis on incomplete multimodal ultrasound data</a></h3><p>Jing Jiao, Huang Yi FDU, LiXiaokang, Yi Guo</p><p class="fs-2">Keywords: <em>Manifold regularization learning, Incomplete multimodal learning, graph neural network, knowledge transfer, prenatal diagnosis</em></p><h3 id="the-cost-of-down-scaling-language-models-fact-recall-deteriorates-before-in-context-learning"> <a href="#the-cost-of-down-scaling-language-models-fact-recall-deteriorates-before-in-context-learning" class="anchor-heading" aria-labelledby="the-cost-of-down-scaling-language-models-fact-recall-deteriorates-before-in-context-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=e0CMWYl2Zj">The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning</a></h3><p>Tian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina Dziugaite</p><p class="fs-2">Keywords: <em>large language model, scaling, pruning, sparsity</em></p><h3 id="outlier-weighed-layerwise-sparsity-owl-a-missing-secret-sauce-for-pruning-llms-to-high-sparsity"> <a href="#outlier-weighed-layerwise-sparsity-owl-a-missing-secret-sauce-for-pruning-llms-to-high-sparsity" class="anchor-heading" aria-labelledby="outlier-weighed-layerwise-sparsity-owl-a-missing-secret-sauce-for-pruning-llms-to-high-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=j5QkMqChDl">Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</a></h3><p>Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu</p><p class="fs-2">Keywords: <em>Large language model, pruning, sparsity</em></p><h3 id="profiling-and-pairing-catchments-and-hydrological-models-with-latent-factor-model"> <a href="#profiling-and-pairing-catchments-and-hydrological-models-with-latent-factor-model" class="anchor-heading" aria-labelledby="profiling-and-pairing-catchments-and-hydrological-models-with-latent-factor-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=jEe5ikHL6v">Profiling and Pairing Catchments and Hydrological Models With Latent Factor Model</a></h3><p>Yang Yang, Ting Fong May Chui</p><p class="fs-2">Keywords: <em>Hydrological modeling, latent factor model, recommender system, machine learning</em></p><h3 id="model-sparsity-can-simplify-machine-unlearning"> <a href="#model-sparsity-can-simplify-machine-unlearning" class="anchor-heading" aria-labelledby="model-sparsity-can-simplify-machine-unlearning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=lvhpn6SO87">Model Sparsity Can Simplify Machine Unlearning</a></h3><p>Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu</p><p class="fs-2">Keywords: <em>Truthworthy AI, Sparsity, Privacy</em></p><h3 id="alternating-updates-for-efficient-transformers"> <a href="#alternating-updates-for-efficient-transformers" class="anchor-heading" aria-labelledby="alternating-updates-for-efficient-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=muOku7bkid">Alternating Updates for Efficient Transformers</a></h3><p>Cenk Baykal, Dylan J Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, Xin Wang</p><p class="fs-2">Keywords: <em>efficiency, efficient transformers</em></p><h3 id="invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-linear-networks"> <a href="#invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-linear-networks" class="anchor-heading" aria-labelledby="invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-linear-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=oSzCKf1I5N">Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Linear Networks</a></h3><p>Can Yaras, Peng Wang, Wei Hu, Zhihui Zhu, Laura Balzano, Qing Qu</p><p class="fs-2">Keywords: <em>implicit bias, low dimensional structures, deep linear networks</em></p><h3 id="dynamic-sparsity-is-channel-level-sparsity-learner"> <a href="#dynamic-sparsity-is-channel-level-sparsity-learner" class="anchor-heading" aria-labelledby="dynamic-sparsity-is-channel-level-sparsity-learner"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=pYFQCmgxcO">Dynamic Sparsity Is Channel-Level Sparsity Learner</a></h3><p>Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu</p><p class="fs-2">Keywords: <em>dynamic sparsity, dynamic sparse training, channel-level sparsity</em></p><h3 id="three-way-trade-off-in-multi-objective-learning-optimization-generalization-and-conflict-avoidance"> <a href="#three-way-trade-off-in-multi-objective-learning-optimization-generalization-and-conflict-avoidance" class="anchor-heading" aria-labelledby="three-way-trade-off-in-multi-objective-learning-optimization-generalization-and-conflict-avoidance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=qtIW8WbWf8">Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance</a></h3><p>Lisha Chen, Heshan Devaka Fernando, Yiming Ying, Tianyi Chen</p><p class="fs-2">Keywords: <em>multi-objective learning, generalization, algorithm stability, stochastic optimization</em></p><h3 id="sparse-moe-with-language-guided-routing-for-multilingual-machine-translation"> <a href="#sparse-moe-with-language-guided-routing-for-multilingual-machine-translation" class="anchor-heading" aria-labelledby="sparse-moe-with-language-guided-routing-for-multilingual-machine-translation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=reWlVyjbff">Sparse MoE with Language Guided Routing for Multilingual Machine Translation</a></h3><p>Xinyu Zhao, Xuxi Chen, Yu Cheng, Tianlong Chen</p><p class="fs-2">Keywords: <em>Sparse Mixture-of-Experts, Multilingual Machine Translation, Language Guided Routing</em></p><h3 id="the-emergence-of-reproducibility-and-consistency-in-diffusion-models"> <a href="#the-emergence-of-reproducibility-and-consistency-in-diffusion-models" class="anchor-heading" aria-labelledby="the-emergence-of-reproducibility-and-consistency-in-diffusion-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=rl0tzGN5K0">The Emergence of Reproducibility and Consistency in Diffusion Models</a></h3><p>Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, Qing Qu</p><p class="fs-2">Keywords: <em>Diffusion model; Consistent model reproducibility; Phenomenon; Uniquely identifiable encoding</em></p><h3 id="graph-neural-networks-provably-benefit-from-structural-information-a-feature-learning-perspective"> <a href="#graph-neural-networks-provably-benefit-from-structural-information-a-feature-learning-perspective" class="anchor-heading" aria-labelledby="graph-neural-networks-provably-benefit-from-structural-information-a-feature-learning-perspective"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=ufR7iEMTlZ">Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective</a></h3><p>Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, Taiji Suzuki</p><p class="fs-2">Keywords: <em>Graph Neural Network, Feature Learning, Graph Convolution, Deep Learning Theory, Benign Overfitting</em></p><h3 id="nonparametric-classification-on-low-dimensional-manifolds-using-overparameterized-convolutional-residual-networks"> <a href="#nonparametric-classification-on-low-dimensional-manifolds-using-overparameterized-convolutional-residual-networks" class="anchor-heading" aria-labelledby="nonparametric-classification-on-low-dimensional-manifolds-using-overparameterized-convolutional-residual-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=va2Fr0nbpx">Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks</a></h3><p>Kaiqi Zhang, Zixuan Zhang, Minshuo Chen, Yuma Takeda, Mengdi Wang, Tuo Zhao, Yu-Xiang Wang</p><p class="fs-2">Keywords: <em>Nonparametric Classification; Low Dimensional Manifolds; Overparameterized ResNets; Function Approximation</em></p><h3 id="h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models"> <a href="#h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models" class="anchor-heading" aria-labelledby="h_2o-heavy-hitter-oracle-for-efficient-generative-inference-of-large-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=w4IRMAJYPk">$H_2O$: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a></h3><p>Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, Beidi Chen</p><p class="fs-2">Keywords: <em>Large Language Models; Efficient Generative Inference</em></p><h3 id="sparse-mixture-of-experts-are-domain-generalizable-learners"> <a href="#sparse-mixture-of-experts-are-domain-generalizable-learners" class="anchor-heading" aria-labelledby="sparse-mixture-of-experts-are-domain-generalizable-learners"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=xHmNRhyfEl">Sparse Mixture-of-Experts are Domain Generalizable Learners</a></h3><p>Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu</p><p class="fs-2">Keywords: <em>domain generalization, mixture-of-experts, algorithmic alignment, visual attributes</em></p><h3 id="low-rank-matrix-completion-theory-via-plucker-coordinates"> <a href="#low-rank-matrix-completion-theory-via-plucker-coordinates" class="anchor-heading" aria-labelledby="low-rank-matrix-completion-theory-via-plucker-coordinates"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=0tjO6ag7l3">Low-Rank Matrix Completion Theory via Plucker Coordinates</a></h3><p>Manolis C. Tsakiris</p><p class="fs-2">Keywords: <em>algebraic geometry, Grassmannian, low-rank matrix completion, non-random observation patterns, Plucker coordinates</em></p><h2 id="spotlight-poster-session-2"> <a href="#spotlight-poster-session-2" class="anchor-heading" aria-labelledby="spotlight-poster-session-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spotlight Poster Session 2</h2><h4 id="time-day-3-jan-5--friday--500-pm-to-630-pm"> <a href="#time-day-3-jan-5--friday--500-pm-to-630-pm" class="anchor-heading" aria-labelledby="time-day-3-jan-5--friday--500-pm-to-630-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Jan 5) – Friday – 5:00 PM to 6:30 PM</a></h4><h3 id="neural-collapse-in-multi-label-learning-with-pick-all-label-loss"> <a href="#neural-collapse-in-multi-label-learning-with-pick-all-label-loss" class="anchor-heading" aria-labelledby="neural-collapse-in-multi-label-learning-with-pick-all-label-loss"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=z79PVkfHLa">Neural Collapse in Multi-label Learning with Pick-all-label Loss</a></h3><p>Pengyu Li, Yutong Wang, Xiao Li, Qing Qu</p><p class="fs-2">Keywords: <em>Multi-label learning, Neural Collapse, Representation Learning</em></p><h3 id="efficient-low-dimensional-compression-of-overparameterized-networks"> <a href="#efficient-low-dimensional-compression-of-overparameterized-networks" class="anchor-heading" aria-labelledby="efficient-low-dimensional-compression-of-overparameterized-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=1AVb9oEdK7">Efficient Low-Dimensional Compression of Overparameterized Networks</a></h3><p>Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, Qing Qu</p><p class="fs-2">Keywords: <em>overparameterization, deep networks, low-dimensional modeling</em></p><h3 id="scan-and-snap-understanding-training-dynamics-and-token-composition-in-1-layer-transformer"> <a href="#scan-and-snap-understanding-training-dynamics-and-token-composition-in-1-layer-transformer" class="anchor-heading" aria-labelledby="scan-and-snap-understanding-training-dynamics-and-token-composition-in-1-layer-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=2iAvIOsc7u">Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer</a></h3><p>Yuandong Tian, Yiping Wang, Beidi Chen, Simon Shaolei Du</p><p class="fs-2">Keywords: <em>transformer, training dynamics, theoretical analysis, self-attention, interpretability, neural network understanding</em></p><h3 id="high-probability-guarantees-for-random-reshuffling"> <a href="#high-probability-guarantees-for-random-reshuffling" class="anchor-heading" aria-labelledby="high-probability-guarantees-for-random-reshuffling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=79SDtYibzD">High Probability Guarantees for Random Reshuffling</a></h3><p>Hengxu Yu, Xiao Li</p><p class="fs-2">Keywords: <em>random reshuffling, shuffled SGD, high-probability sample complexity, stopping criterion, the last iteration result</em></p><h3 id="a-linearly-convergent-gan-inversion-based-algorithm-for-reverse-engineering-of-deceptions"> <a href="#a-linearly-convergent-gan-inversion-based-algorithm-for-reverse-engineering-of-deceptions" class="anchor-heading" aria-labelledby="a-linearly-convergent-gan-inversion-based-algorithm-for-reverse-engineering-of-deceptions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=9MlJXVEE0b">A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions</a></h3><p>Darshan Thaker, Paris Giampouras, Rene Vidal</p><p class="fs-2">Keywords: <em>reverse engineering deceptions, GAN inversion, optimization, adversarial attacks, generative models, inverse problems</em></p><h3 id="simultaneous-linear-connectivity-of-neural-networks-modulo-permutation"> <a href="#simultaneous-linear-connectivity-of-neural-networks-modulo-permutation" class="anchor-heading" aria-labelledby="simultaneous-linear-connectivity-of-neural-networks-modulo-permutation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=BwhEtJrnir">Simultaneous linear connectivity of neural networks modulo permutation</a></h3><p>Ekansh Sharma, Devin Kwok, tom denton, Daniel M. Roy, David Rolnick, Gintare Karolina Dziugaite</p><p class="fs-2">Keywords: <em>linear mode connectivity, loss landscape, permutation symmetry, iterative magnitude pruning, lottery ticket</em></p><h3 id="accurate-neural-network-pruning-requires-rethinking-sparse-optimization"> <a href="#accurate-neural-network-pruning-requires-rethinking-sparse-optimization" class="anchor-heading" aria-labelledby="accurate-neural-network-pruning-requires-rethinking-sparse-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=HwyuE3QZtS">Accurate Neural Network Pruning Requires Rethinking Sparse Optimization</a></h3><p>Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, Dan Alistarh</p><p class="fs-2">Keywords: <em>Efficient ML, Pruning, optimization, sparsity</em></p><h3 id="the-lazy-neuron-phenomenon-on-emergence-of-activation-sparsity-in-transformers"> <a href="#the-lazy-neuron-phenomenon-on-emergence-of-activation-sparsity-in-transformers" class="anchor-heading" aria-labelledby="the-lazy-neuron-phenomenon-on-emergence-of-activation-sparsity-in-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=IgvJxt7Mn1">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a></h3><p>Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, Sanjiv Kumar</p><p class="fs-2">Keywords: <em>Transformer efficiency, activation sparsity, robustness, calibration</em></p><h3 id="dynamic-sparse-training-with-structured-sparsity"> <a href="#dynamic-sparse-training-with-structured-sparsity" class="anchor-heading" aria-labelledby="dynamic-sparse-training-with-structured-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=LWBzJRTBAH">Dynamic Sparse Training with Structured Sparsity</a></h3><p>Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou</p><p class="fs-2">Keywords: <em>Machine Learning, dynamic sparse training, structured sparsity, N:M sparsity, efficient deep learning, RigL, SRigL, constant fan-in, dynamic neuron ablation, neuron ablation, structured and fine-grained sparsity, online inference, accelerating inference</em></p><h3 id="ultrafast-neural-estimation-of-mutual-information"> <a href="#ultrafast-neural-estimation-of-mutual-information" class="anchor-heading" aria-labelledby="ultrafast-neural-estimation-of-mutual-information"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=LYaOT0RHFS">Ultrafast Neural Estimation of Mutual Information</a></h3><p>Zhengyang Hu, Song Kang, Qunsong Zeng, Kaibin Huang, Yanchao Yang</p><p class="fs-2">Keywords: <em>Deep Learning, Efficient Mutual Information Estimation, Real-Time Correlation Computation, Maximum Correlation Coefficient</em></p><h3 id="understanding-hierarchical-representations-in-deep-networks-via-feature-compression-and-discrimination"> <a href="#understanding-hierarchical-representations-in-deep-networks-via-feature-compression-and-discrimination" class="anchor-heading" aria-labelledby="understanding-hierarchical-representations-in-deep-networks-via-feature-compression-and-discrimination"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=Ovuu8LpGZu">Understanding Hierarchical Representations in Deep Networks via Feature Compression and Discrimination</a></h3><p>Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu</p><p class="fs-2">Keywords: <em>representation learning; neural collapse; deep linear networks</em></p><h3 id="deep-neural-network-initialization-with-sparsity-inducing-activations"> <a href="#deep-neural-network-initialization-with-sparsity-inducing-activations" class="anchor-heading" aria-labelledby="deep-neural-network-initialization-with-sparsity-inducing-activations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=QV5qqLfcYy">Deep Neural Network Initialization with Sparsity Inducing Activations</a></h3><p>Ilan Price, Nicholas Daultry Ball, Adam Christopher Jones, Samuel Chun Hei Lam, Jared Tanner</p><p class="fs-2">Keywords: <em>Deep neural network, random initialisation, sparsity, gaussian process</em></p><h3 id="how-structured-data-guides-feature-learning-a-case-study-of-sparse-parity-problem"> <a href="#how-structured-data-guides-feature-learning-a-case-study-of-sparse-parity-problem" class="anchor-heading" aria-labelledby="how-structured-data-guides-feature-learning-a-case-study-of-sparse-parity-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=QnfHUb41Db">How Structured Data Guides Feature Learning: A Case Study of Sparse Parity Problem</a></h3><p>Atsushi Nitanda, Kazusato Oko, Taiji Suzuki, Denny Wu</p><p class="fs-2">Keywords: <em>neural network optimization, representation learning, mean-field Langevin dynamics</em></p><h3 id="the-emergence-of-essential-sparsity-in-large-pre-trained-models-the-weights-that-matter"> <a href="#the-emergence-of-essential-sparsity-in-large-pre-trained-models-the-weights-that-matter" class="anchor-heading" aria-labelledby="the-emergence-of-essential-sparsity-in-large-pre-trained-models-the-weights-that-matter"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=RSyMBHv70m">The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter</a></h3><p>AJAY KUMAR JAISWAL, Shiwei Liu, Tianlong Chen, Zhangyang Wang</p><p class="fs-2">Keywords: <em>Pre-trained Models, Sparsity, Emergence, Transformers, Pruning</em></p><h3 id="divided-attention-unsupervised-multiple-object-discovery-and-segmentation-with-interpretable-contextually-separated-slots"> <a href="#divided-attention-unsupervised-multiple-object-discovery-and-segmentation-with-interpretable-contextually-separated-slots" class="anchor-heading" aria-labelledby="divided-attention-unsupervised-multiple-object-discovery-and-segmentation-with-interpretable-contextually-separated-slots"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=SrSylp7jKd">Divided Attention: Unsupervised Multiple-object Discovery and Segmentation with Interpretable Contextually Separated Slots</a></h3><p>Dong Lao, Zhengyang Hu, Francesco Locatello, Yanchao Yang, Stefano Soatto</p><p class="fs-2">Keywords: <em>Moving object segmentation, Slot attention, Unsupervised object discovery</em></p><h3 id="compressing-llms-the-truth-is-rarely-pure-and-never-simple"> <a href="#compressing-llms-the-truth-is-rarely-pure-and-never-simple" class="anchor-heading" aria-labelledby="compressing-llms-the-truth-is-rarely-pure-and-never-simple"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=UKfRD1hqtt">Compressing LLMs: The Truth is Rarely Pure and Never Simple</a></h3><p>AJAY KUMAR JAISWAL, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei Yang</p><p class="fs-2">Keywords: <em>Compression, Large Language Models, Pruning, Quantization</em></p><h3 id="on-bias-variance-alignment-in-deep-models"> <a href="#on-bias-variance-alignment-in-deep-models" class="anchor-heading" aria-labelledby="on-bias-variance-alignment-in-deep-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=UiPyyIN5eI">On Bias-Variance Alignment in Deep Models</a></h3><p>Lin Chen, Michal Lukasik, Wittawat Jitkrittum, Chong You, Sanjiv Kumar</p><p class="fs-2">Keywords: <em>bias-variance decomposition, ensemble, deep learning</em></p><h3 id="canonical-factors-for-hybrid-neural-fields"> <a href="#canonical-factors-for-hybrid-neural-fields" class="anchor-heading" aria-labelledby="canonical-factors-for-hybrid-neural-fields"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=VeiZDoUbDL">Canonical Factors for Hybrid Neural Fields</a></h3><p>Brent Yi, Weijia Zeng, Sam Buchanan, Yi Ma</p><p class="fs-2">Keywords: <em>3d representation learning, neural fields, NeRF, voxel grids, invariance, non-convex optimization</em></p><h3 id="low-complexity-homeomorphic-projection-to-ensure-neural-network-solution-feasibility-for-optimization-over-non-convex-set"> <a href="#low-complexity-homeomorphic-projection-to-ensure-neural-network-solution-feasibility-for-optimization-over-non-convex-set" class="anchor-heading" aria-labelledby="low-complexity-homeomorphic-projection-to-ensure-neural-network-solution-feasibility-for-optimization-over-non-convex-set"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=dwLwioZ1eJ">Low Complexity Homeomorphic Projection to Ensure Neural-Network Solution Feasibility for Optimization over (Non-)Convex Set</a></h3><p>Enming Liang, Minghua Chen, Steven Low</p><p class="fs-2">Keywords: <em>Constraint optimization, Feasibility, Neural Network, Homeomorphism, Invertible Neural Network, Projection</em></p><h3 id="generalized-neural-collapse-for-a-large-number-of-classes"> <a href="#generalized-neural-collapse-for-a-large-number-of-classes" class="anchor-heading" aria-labelledby="generalized-neural-collapse-for-a-large-number-of-classes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=ezH4FSN0KZ">Generalized Neural Collapse for A Large Number of Classes</a></h3><p>Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin G. Mixon, Chong You, Zhihui Zhu</p><p class="fs-2">Keywords: <em>Neural Collapse, Tammes Problem, Sphere Packing, Deep Learning</em></p><h3 id="sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning"> <a href="#sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning" class="anchor-heading" aria-labelledby="sparse-moe-as-a-new-treatment-addressing-forgetting-fitting-learning-issues-in-multi-modal-multi-task-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=fyzhsZzd28">Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning Issues in Multi-Modal Multi-Task Learning</a></h3><p>Jie Peng, Kaixiong Zhou, Ruida Zhou, Thomas Hartvigsen, Yanyong Zhang, Zhangyang Wang, Tianlong Chen</p><p class="fs-2">Keywords: <em>multi-task learning, multimodal learning, transformer</em></p><h3 id="solving-inverse-problems-with-latent-diffusion-models-via-hard-data-consistency"> <a href="#solving-inverse-problems-with-latent-diffusion-models-via-hard-data-consistency" class="anchor-heading" aria-labelledby="solving-inverse-problems-with-latent-diffusion-models-via-hard-data-consistency"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=iHcarDCZLn">Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency</a></h3><p>Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen</p><p class="fs-2">Keywords: <em>inverse problems; latent diffusion models</em></p><h3 id="unsupervised-manifold-linearizing-and-clustering"> <a href="#unsupervised-manifold-linearizing-and-clustering" class="anchor-heading" aria-labelledby="unsupervised-manifold-linearizing-and-clustering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=inGhsAjxPf">Unsupervised Manifold Linearizing and Clustering</a></h3><p>Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin David Haeffele</p><p class="fs-2">Keywords: <em>Clustering, Manifold Embedding, Manifold Clustering</em></p><h3 id="masked-completion-via-structured-diffusion-with-white-box-transformers"> <a href="#masked-completion-via-structured-diffusion-with-white-box-transformers" class="anchor-heading" aria-labelledby="masked-completion-via-structured-diffusion-with-white-box-transformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=muoEgWUZiS">Masked Completion via Structured Diffusion with White-Box Transformers</a></h3><p>Druv Pai, Ziyang Wu, Sam Buchanan, Tianzhe Chu, Yaodong Yu, Yi Ma</p><p class="fs-2">Keywords: <em>masked autoencoding, white-box transformers, coding rate reduction, representation learning</em></p><h3 id="approximately-equivariant-graph-networks"> <a href="#approximately-equivariant-graph-networks" class="anchor-heading" aria-labelledby="approximately-equivariant-graph-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=nS7BFSt3Fq">Approximately Equivariant Graph Networks</a></h3><p>Ningyuan Teresa Huang, Ron Levie, Soledad Villar</p><p class="fs-2">Keywords: <em>graph neural networks, equivariant machine learning, symmetry, generalization, statistical learning</em></p><h3 id="neural-collapse-meets-differential-privacy--curious-behaviors-of-noisysgd-with-near-perfect-representation-learning"> <a href="#neural-collapse-meets-differential-privacy--curious-behaviors-of-noisysgd-with-near-perfect-representation-learning" class="anchor-heading" aria-labelledby="neural-collapse-meets-differential-privacy--curious-behaviors-of-noisysgd-with-near-perfect-representation-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=okJ9ju1qJC">Neural Collapse meets Differential Privacy: Curious behaviors of NoisySGD with Near-Perfect Representation Learning</a></h3><p>Chendi Wang, Yuqing Zhu, Weijie J Su, Yu-Xiang Wang</p><p class="fs-2">Keywords: <em>Neural collapse, differential privacy, representation learning</em></p><h3 id="joma-demystifying-multilayer-transformers-via-joint-dynamics-of-mlp-and-attention"> <a href="#joma-demystifying-multilayer-transformers-via-joint-dynamics-of-mlp-and-attention" class="anchor-heading" aria-labelledby="joma-demystifying-multilayer-transformers-via-joint-dynamics-of-mlp-and-attention"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=s7ubAYxMhA">JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention</a></h3><p>Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Shaolei Du</p><p class="fs-2">Keywords: <em>transformer, training dynamics, theoretical analysis, self-attention, interpretability, neural network understanding</em></p><h3 id="learning-in-the-presence-of-low-dimensional-structure-a-spiked-random-matrix-perspective"> <a href="#learning-in-the-presence-of-low-dimensional-structure-a-spiked-random-matrix-perspective" class="anchor-heading" aria-labelledby="learning-in-the-presence-of-low-dimensional-structure-a-spiked-random-matrix-perspective"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=sDBBjwc73g">Learning in the Presence of Low-dimensional Structure: A Spiked Random Matrix Perspective</a></h3><p>Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu</p><p class="fs-2">Keywords: <em>random matrix theory, high-dimensional statistics, neural network, kernel method, representation learning</em></p><h3 id="how-over-parameterization-slows-down-gradient-descent-in-matrix-sensing-the-curses-of-symmetry-and-initialization"> <a href="#how-over-parameterization-slows-down-gradient-descent-in-matrix-sensing-the-curses-of-symmetry-and-initialization" class="anchor-heading" aria-labelledby="how-over-parameterization-slows-down-gradient-descent-in-matrix-sensing-the-curses-of-symmetry-and-initialization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=ujhfVwLtlY">How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization</a></h3><p>Nuoya Xiong, Lijun Ding, Simon Shaolei Du</p><p class="fs-2">Keywords: <em>non-convex optimization, random initialization, global convergence, matrix recovery, matrix sensing</em></p><h3 id="robust-physics-based-deep-mri-reconstruction-via-diffusion-purification"> <a href="#robust-physics-based-deep-mri-reconstruction-via-diffusion-purification" class="anchor-heading" aria-labelledby="robust-physics-based-deep-mri-reconstruction-via-diffusion-purification"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=xFLFTO0YJE">Robust Physics-based Deep MRI Reconstruction Via Diffusion Purification</a></h3><p>Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, Saiprasad Ravishankar</p><p class="fs-2">Keywords: <em>Robust MRI reconstruction, model-based deep learning, diffusion purification, computational imaging, machine learning</em></p><h3 id="sparsity-enhances-non-gaussian-data-statistics-during-local-receptive-field-formation"> <a href="#sparsity-enhances-non-gaussian-data-statistics-during-local-receptive-field-formation" class="anchor-heading" aria-labelledby="sparsity-enhances-non-gaussian-data-statistics-during-local-receptive-field-formation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://openreview.net/forum?id=0eP6GSS5KU">Sparsity Enhances Non-Gaussian Data Statistics During Local Receptive Field Formation</a></h3><p>William T Redman, Zhangyang Wang, Alessandro Ingrosso, Sebastian Goldt</p><p class="fs-2">Keywords: <em>iterative magnitude pruning, sparse machine learning, statistics of internal representations, learning local receptive fields</em></p></main></div></div><div class="search-overlay"></div></div>
