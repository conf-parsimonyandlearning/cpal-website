<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Tutorials | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Tutorials" /><meta property="og:locale" content="en_US" /><meta name="description" content="Information about last-day tutorials at CPAL (open to the public)" /><meta property="og:description" content="Information about last-day tutorials at CPAL (open to the public)" /><link rel="canonical" href="https://2024.cpal.cc/tutorials/" /><meta property="og:url" content="https://2024.cpal.cc/tutorials/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://2024.cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://2024.cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Tutorials" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Information about last-day tutorials at CPAL (open to the public)","headline":"Tutorials","image":"https://2024.cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://2024.cpal.cc/assets/images/logo.svg"}},"url":"https://2024.cpal.cc/tutorials/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">Venue</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings/" class="nav-list-link">Conference Proceedings</a><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/program_highlights/" class="nav-list-link">Program Highlights</a><li class="nav-list-item "> <a href="/oral_and_spotlight_presentations/" class="nav-list-link">Orals and Recent Spotlights</a><li class="nav-list-item "> <a href="/rising_stars_presentations/" class="nav-list-link">Rising Stars Presentations</a><li class="nav-list-item active"> <a href="/tutorials/" class="nav-list-link active">Tutorials</a><li class="nav-list-item "> <a href="/wellness/" class="nav-list-link">Tailored Wellness Sessions</a><li class="nav-list-item "> <a href="/social/" class="nav-list-link">Social Events</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a><li class="nav-list-item"> <a href="/other_years/" class="nav-list-link">Past CPAL Websites</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://proceedings.mlr.press/v234/" class="site-button" > CPAL Proceedings </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/program/">Conference Program</a><li class="breadcrumb-nav-list-item"><span>Tutorials</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="tutorials"> <a href="#tutorials" class="anchor-heading" aria-labelledby="tutorials"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Tutorials</h1><p>The final day of the conference features tutorial presentations, which are open to the public. These tutorials present an up-to-date account of the intersection between low-dimensional modeling and deep learning in an accessible format.</p><p>The tutorials consist of two parallel tracks, respectively titled <strong><em><a href="/tutorials/#track-learning-deep-low-dimensional-models-from-high-dimensional-data-from-theory-to-practice">Learning Deep Low-dimensional Models from High-Dimensional Data: From Theory to Practice</a></em></strong>, and <strong><em><a href="/tutorials/#track-advances-in-machine-learning-for-image-reconstruction-sparse-models-to-deep-networks">Advances in Machine Learning for Image Reconstruction: Sparse Models to Deep Networks</a></em></strong>.</p><p>Each track consists of four lectures. The planned content of the two tracks is summarized below. See the schedule for the precise times of each tutorial.</p><h2 id="track-learning-deep-low-dimensional-models-from-high-dimensional-data-from-theory-to-practice"> <a href="#track-learning-deep-low-dimensional-models-from-high-dimensional-data-from-theory-to-practice" class="anchor-heading" aria-labelledby="track-learning-deep-low-dimensional-models-from-high-dimensional-data-from-theory-to-practice"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Track: Learning Deep Low-dimensional Models from High-Dimensional Data: From Theory to Practice</h2><h3 id="lecture-1--redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction"> <a href="#lecture-1--redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction" class="anchor-heading" aria-labelledby="lecture-1--redunet-a-white-box-deep-network-from-the-principle-of-maximizing-rate-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 1 – ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ma.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a></h3><p>UC Berkeley / HKU IDS</p><p class="organizer-meta">Professor</p></div></div><h4 id="abstract"> <a href="#abstract" class="anchor-heading" aria-labelledby="abstract"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>To begin, we will focus on the special yet highly useful case of learning the data distribution and transforming it to an <em>linear discriminative representation</em> (LDR). We will discuss the information theoretic and statistical principles behind such a representation, and design a loss function, called the <em>coding rate reduction</em>, which is optimized at such a representation. By unrolling the gradient ascent on the coding rate reduction, we will construct a deep network architecture, called the ReduNet, where each operator in the network has a mathematically precise (hence white-box and interpretable) function in the transformation of the data distribution towards an LDR. Also, the ReduNet may be constructed layer-wise in a forward-propagation manner, that is, without <em>any</em> back-propagation required.</p><h3 id="lecture-2--white-box-transformers-via-sparse-rate-reduction"> <a href="#lecture-2--white-box-transformers-via-sparse-rate-reduction" class="anchor-heading" aria-labelledby="lecture-2--white-box-transformers-via-sparse-rate-reduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 2 – White-Box Transformers via Sparse Rate Reduction</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/sdb.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sdbuchanan.com">Sam Buchanan</a></h3><p>TTIC</p><p class="organizer-meta">Research Assistant Professor</p></div></div><h4 id="abstract-1"> <a href="#abstract-1" class="anchor-heading" aria-labelledby="abstract-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>We demonstrate how combining sparse coding and rate reduction yields sparse linear discriminative representations using an objective named “sparse rate reduction”. We develop CRATE, a deep network architecture, by unrolling the optimization of this objective and parameterizing feature distribution in each layer. CRATE’s operators are mathematically interpretable, with each layer representing an optimization step, making the network a transparent “white box”. Although CRATE’s design significantly differs from ReduNet, both aim for a similar goal, showcasing the versatility of the unrolled optimization approach. Remarkably, CRATE closely resembles the transformer architecture, suggesting that the interpretability gains from such networks might also improve our understanding of current, practical deep architectures.</p><h3 id="lecture-3--understanding-deep-representation-learning-via-neural-collapse"> <a href="#lecture-3--understanding-deep-representation-learning-via-neural-collapse" class="anchor-heading" aria-labelledby="lecture-3--understanding-deep-representation-learning-via-neural-collapse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 3 – Understanding Deep Representation Learning via Neural Collapse</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/zz.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://zhihuizhu.github.io">Zhihui Zhu</a></h3><p>Ohio State University</p><p class="organizer-meta">Assistant Professor</p></div></div><h4 id="abstract-2"> <a href="#abstract-2" class="anchor-heading" aria-labelledby="abstract-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>The session focuses on the strong conceptual connections between low-dimensional structures and deep models in terms of learned representation. We start with the introduction of an intriguing Neural Collapse phenomenon in the last-layer representation and its universality in deep network, and lays out the mathematical foundations of understanding its cause by studying its optimization landscapes. We then generalize and explain this phenomenon and its implications under data imbalanceness. Furthermore, we demonstrate the practical algorithmic implications of Neural Collapse on training deep neural networks.</p><h3 id="lecture-4--invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks"> <a href="#lecture-4--invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks" class="anchor-heading" aria-labelledby="lecture-4--invariant-low-dimensional-subspaces-in-gradient-descent-for-learning-deep-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 4 – Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Networks</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/qq.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://qingqu.engin.umich.edu/">Qing Qu</a></h3><p>University of Michigan</p><p class="organizer-meta">Assistant Professor</p></div></div><h4 id="abstract-3"> <a href="#abstract-3" class="anchor-heading" aria-labelledby="abstract-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>To conclude, we show that low-dimensional structures also emerge in training dynamics of deep networks. Specifically, we show that the evolution of gradient descent only affects a minimal portion of singular vector spaces across all weight matrices. The analysis enables us to considerably improve training efficiency by taking advantage of the low-dimensional structure in learning dynamics. We can construct smaller, equivalent deep linear networks without sacrificing the benefits associated with the wider counterparts. Moreover, it allows us to better understand deep representation learning by elucidating the progressive feature compression and discrimination from shallow to deep layers.</p><h2 id="track-advances-in-machine-learning-for-image-reconstruction-sparse-models-to-deep-networks"> <a href="#track-advances-in-machine-learning-for-image-reconstruction-sparse-models-to-deep-networks" class="anchor-heading" aria-labelledby="track-advances-in-machine-learning-for-image-reconstruction-sparse-models-to-deep-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Track: Advances in Machine Learning for Image Reconstruction: Sparse Models to Deep Networks</h2><p>Lectures 1-3 will cover a diverse spectrum of topics across sparse modeling and deep learning and theory with applications in medical imaging and image restoration/computer vision. A subset of works across topics will be discussed including works from tutorial presenters.</p><h3 id="lecture-1--sparse-modeling-for-image-reconstruction"> <a href="#lecture-1--sparse-modeling-for-image-reconstruction" class="anchor-heading" aria-labelledby="lecture-1--sparse-modeling-for-image-reconstruction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 1 – Sparse Modeling for Image Reconstruction</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ravishankar.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a></h3><p>Michigan State University</p><p class="organizer-meta">Assistant Professor</p></div></div><h4 id="abstract-4"> <a href="#abstract-4" class="anchor-heading" aria-labelledby="abstract-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Data-driven and machine learning techniques have received increasing attention in recent years for solving various problems in computational imaging. First, we will introduce basics of image reconstruction and sparse modeling before discussing the learning of various classical sparse signal models, particularly synthesis dictionaries and sparsifying transforms. The application of dictionary and transform learning to primarily medical image reconstruction will be discussed. We will also briefly discuss the combination of sparsity with other priors (e.g., low-rank) and variants such as convolutional dictionary learning and multi-layer sparse models used in image reconstruction.<br /><em>Time: 50 minutes</em></p><h3 id="lecture-2--sparse-modeling-to-deep-learning-for-image-restoration"> <a href="#lecture-2--sparse-modeling-to-deep-learning-for-image-restoration" class="anchor-heading" aria-labelledby="lecture-2--sparse-modeling-to-deep-learning-for-image-restoration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 2 – Sparse Modeling to Deep Learning for Image Restoration</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/wen.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen</a></h3><p>Nanyang Technological University</p><p class="organizer-meta">Nanyang Assistant Professor</p></div></div><h4 id="abstract-5"> <a href="#abstract-5" class="anchor-heading" aria-labelledby="abstract-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>The second talk will focus more on integration of learned sparsity and nonlocal image modeling via group sparsity, low-rank structures, etc. Applications will be shown for image restoration and medical imaging. This part will also transition to aspects of deep learning for image restoration and offer several examples. How deep learning can be combined with conventional model-based approaches will also be touched upon.<br /><em>Time: 50 minutes</em></p><h3 id="lecture-3--deep-learning-for-imaging"> <a href="#lecture-3--deep-learning-for-imaging" class="anchor-heading" aria-labelledby="lecture-3--deep-learning-for-imaging"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 3 – Deep Learning for Imaging</h3><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ravishankar.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a></h3><p>Michigan State University</p><p class="organizer-meta">Assistant Professor</p></div></div><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ismail.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/view/ismailalkhouri/about">Ismail Alkhouri</a></h3><p>Michigan State University / University of Michigan, Ann Arbor</p><p class="organizer-meta">Postdoc / Visiting Scholar</p></div></div><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/ghosh.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://sites.google.com/view/avrajitghosh">Avrajit Ghosh</a></h3><p>Michigan State University</p><p class="organizer-meta">Ph.D. Student</p></div></div><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/maliakal.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> Gabriel Maliakal</h3><p>Michigan State University</p><p class="organizer-meta">Ph.D. Student</p></div></div><h4 id="abstract-6"> <a href="#abstract-6" class="anchor-heading" aria-labelledby="abstract-6"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>We will introduce modern deep learning-based methods for image reconstruction, particularly in medical imaging, including image-domain or sensor-domain neural network denoisers, and hybrid-domain deep learning schemes that combine physics-based forward models together with neural networks. Hybrid methods, both supervised and unsupervised or self-supervised, will be a key focus and amongst them, we will review plug and play (PnP) priors, consensus equilibrium, regularization by denoising (RED), deep unrolling methods, deep equilibrium models, and bilevel optimization based methods. Then, generative models for image reconstruction will be presented including generative adversarial networks (GANs), deep image prior, and diffusion models (DMs). We will provide a brief introduction to score-based DMs and introduce Diffusion Posterior Sampling ReSample algorithms. Other recent trends in image reconstruction will also be covered including exploiting deep reinforcement learning, unifying deep learning and sparse modeling, and methods focused on improving robustness of deep learning based image reconstruction (to various perturbations, train-test disparities, etc.) via randomized smoothing, diffusion models, etc. Other topics will also be covered briefly including learning sparse neural networks and joint or end-to-end training of sensing and image reconstruction setups. The session will conclude with brief discussion of future directions for the field.<br /><br /> Lecture 3 will involve multiple speakers covering different themes. Avrajit Ghosh will present many of the hybrid or physics-based deep learning methods. Dr. Ismail Alkhouri will speak on key ideas involving diffusion models. Gabriel Maliakal will discuss GANs and deep reinforcement learning and Dr. Saiprasad Ravishankar will speak on the key other topics.<br /><br /><em>Time: 140–150 minutes (coffee break in between)</em></p></main></div></div><div class="search-overlay"></div></div>
