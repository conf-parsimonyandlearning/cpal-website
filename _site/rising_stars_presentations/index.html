<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Rising Stars Presentations | Conference on Parsimony and Learning (CPAL)</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Rising Stars Presentations" /><meta property="og:locale" content="en_US" /><meta name="description" content="Scheduling information for Rising Stars presentations" /><meta property="og:description" content="Scheduling information for Rising Stars presentations" /><link rel="canonical" href="https://cpal.cc/rising_stars_presentations/" /><meta property="og:url" content="https://cpal.cc/rising_stars_presentations/" /><meta property="og:site_name" content="Conference on Parsimony and Learning (CPAL)" /><meta property="og:image" content="https://cpal.cc/assets/images/card.png" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://cpal.cc/assets/images/card.png" /><meta property="twitter:title" content="Rising Stars Presentations" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Scheduling information for Rising Stars presentations","headline":"Rising Stars Presentations","image":"https://cpal.cc/assets/images/card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://cpal.cc/assets/images/logo.svg"}},"url":"https://cpal.cc/rising_stars_presentations/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="Conference on Parsimony and Learning (CPAL)"></div></a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Register & Attend category" aria-pressed="false"> Register & Attend </button><ul class="nav-list"><li class="nav-list-item "> <a href="/registration/" class="nav-list-link">Registration</a><li class="nav-list-item "> <a href="/venue/" class="nav-list-link">Venue</a><li class="nav-list-item "> <a href="/visa/" class="nav-list-link">Travel: Visa Information</a><li class="nav-list-item "> <a href="/hotels/" class="nav-list-link">Travel: Hotels</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Accepted Papers category" aria-pressed="false"> Accepted Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/proceedings_track/" class="nav-list-link">Proceedings Track</a><li class="nav-list-item "> <a href="/spotlight_track/" class="nav-list-link">Spotlight Track</a></ul><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Conference Program category" aria-pressed="true"> Conference Program </button><ul class="nav-list"><li class="nav-list-item "> <a href="/program_schedule/" class="nav-list-link">Program at a Glance</a><li class="nav-list-item "> <a href="/program_highlights/" class="nav-list-link">Program Highlights</a><li class="nav-list-item "> <a href="/oral_and_spotlight_presentations/" class="nav-list-link">Orals and Recent Spotlights</a><li class="nav-list-item active"> <a href="/rising_stars_presentations/" class="nav-list-link active">Rising Stars Presentations</a><li class="nav-list-item "> <a href="/tutorials/" class="nav-list-link">Tutorials</a><li class="nav-list-item "> <a href="/wellness/" class="nav-list-link">Tailored Wellness Sessions</a></ul><li class="nav-list-item"> <a href="/speakers/" class="nav-list-link">Keynote Speakers</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Rising Stars Award category" aria-pressed="false"> Rising Stars Award </button><ul class="nav-list"><li class="nav-list-item "> <a href="/rising_stars_guidelines/" class="nav-list-link">Application</a><li class="nav-list-item "> <a href="/rising_stars_awardees/" class="nav-list-link">Awardees</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Call for Papers category" aria-pressed="false"> Call for Papers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/tracks/" class="nav-list-link">Submission Tracks</a><li class="nav-list-item "> <a href="/subject_areas/" class="nav-list-link">Subject Areas</a><li class="nav-list-item "> <a href="/review_guidelines/" class="nav-list-link">Review Guidelines</a><li class="nav-list-item "> <a href="/code_of_conduct/" class="nav-list-link">Code of Conduct</a></ul><li class="nav-list-item"> <a href="/deadlines/" class="nav-list-link">Key Dates</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <button class="nav-list-link-expander btn-reset" aria-label="toggle items in Organizers category" aria-pressed="false"> Organizers </button><ul class="nav-list"><li class="nav-list-item "> <a href="/organization_committee/" class="nav-list-link">Organization Committee</a><li class="nav-list-item "> <a href="/advisory/" class="nav-list-link">Advisory Committee</a><li class="nav-list-item "> <a href="/area_chairs/" class="nav-list-link">Area Chairs</a></ul><li class="nav-list-item"> <a href="/sponsors/" class="nav-list-link">Sponsors</a><li class="nav-list-item"> <a href="/vision/" class="nav-list-link">Conference Vision</a></ul></nav><footer class="site-footer"> Connect: <br> <a href="mailto:pcs@cpal.cc"><img src=/assets/images/email.svg alt="Email icon"></a> <a href="https://twitter.com/CPALconf"><img src=/assets/images/twitter.svg alt="Twitter icon"></a> <a href="https://www.linkedin.com/company/conference-on-parsimony-and-learning-cpal/"><img src=/assets/images/linkedin.svg alt="Linkedin icon"></a> <br> <credit>Credit: <a href="https://github.com/just-the-docs/just-the-docs">theme</a></credit></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CPAL" aria-label="Search CPAL" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal-registration" class="site-button" > Registration </a><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=CPAL.cc/2024" class="site-button" > CPAL OpenReview </a><li class="aux-nav-list-item"> <a href="https://datascience.hku.hk/cpal/" class="site-button" > CPAL at HKU </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/program/">Conference Program</a><li class="breadcrumb-nav-list-item"><span>Rising Stars Presentations</span></ol></nav><div id="main-content" class="main-content"><main><div class="splash"> <img src="/assets/images/hku.jpeg" alt="Splash photo of HKU" /><div class="topleft"> Conference on Parsimony and Learning (CPAL)</div><div class="bottomright"> January 2024,&nbsp;HKU</div></div><h1 id="cpal-rising-stars-presentation-sessions"> <a href="#cpal-rising-stars-presentation-sessions" class="anchor-heading" aria-labelledby="cpal-rising-stars-presentation-sessions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> CPAL Rising Stars Presentation Sessions</h1><h2 id="presentation-format"> <a href="#presentation-format" class="anchor-heading" aria-labelledby="presentation-format"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Presentation Format</h2><p>Each awarded <a href="/rising_stars_awardees/">CPAL Rising Star</a> will give a talk about their research in one of three sessions during the conference.</p><p>Presentations are ten minutes in duration, with two minutes for Q&amp;A.</p><p>The ordering of session numbers matches their chronological ordering, and presentations will be delivered in the order they are listed. See the <a href="/program_schedule/">full program</a> for the precise time and location of each CPAL Rising Stars session.</p><h2 id="rising-stars-session-1"> <a href="#rising-stars-session-1" class="anchor-heading" aria-labelledby="rising-stars-session-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Rising Stars Session 1</h2><h4 id="time-day-1-jan-3--wednesday--1120-am-to-1220-pm"> <a href="#time-day-1-jan-3--wednesday--1120-am-to-1220-pm" class="anchor-heading" aria-labelledby="time-day-1-jan-3--wednesday--1120-am-to-1220-pm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 1 (Jan 3) – Wednesday – 11:20 AM to 12:20 PM</a></h4><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/kunin.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://daniel-kunin.com/">Daniel Paul Kunin</a></h3><p>Stanford University</p><p class="organizer-meta">Ph.D. Student</p></div></div><p><strong>Title</strong>: Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks</p><p><strong>Abstract</strong>: In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler (sparse or low-rank) subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape’s curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/yu.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://yaodongyu.github.io/">Yaodong Yu</a></h3><p>University of California, Berkeley</p><p class="organizer-meta">Ph.D. Student</p></div></div><p><strong>Title</strong>: White-Box Transformers via Sparse Rate Reduction</p><p><strong>Abstract</strong>: In this talk, I will present the white-box transformer — CRATE (i.e., Coding RAte reduction TransformEr). We contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing lossy coding rate. This leads to a family of white-box transformer architectures which are mathematically interpretable. Our experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers (ViTs). I will also present some recent theoretical and empirical results of CRATE on emergence behavior, language modeling, and auto-encoding.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/wangp.jpeg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://peng8wang.github.io/">Peng Wang</a></h3><p>University of Michigan</p><p class="organizer-meta">Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Understanding Hierarchical Representations in Deep Networks via Intermediate Features</p><p><strong>Abstract</strong>: Over the past decade, deep learning has proven to be a highly effective method for learning meaningful features from raw data. This work attempts to unveil the mystery of hierarchical feature learning in deep networks. Specifically, in the context of multi-class classification problems, we explore how deep networks transform input data by investigating the output (i.e., features) of each layer after training. Towards this goal, we first define metrics for within-class compression and between-class discrimination of intermediate features, respectively. Through an analysis of these two metrics, we show that the evolution of features follows a simple and quantitative law from shallow to deep layers: Each layer of linear networks progressively compresses within-class features at a linear rate and discriminates between-class features at a sublinear rate. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep networks. Moreover, our extensive experiments validate our theoretical findings numerically.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/lejeune.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://dlej.net/">Daniel LeJeune</a></h3><p>Stanford University</p><p class="organizer-meta">Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Emergent properties of heuristics in machine learning</p><p><strong>Abstract</strong>: Successful methods in modern machine learning practice are built on solid intuition and theoretical insight by their designers, but are often ultimately heuristic and exhibit unintended emergent behaviors. Sometimes these emergent behaviors are detrimental, but surprisingly, many provide unexpected desirable benefits. By theoretically characterizing these emergent behaviors, we can develop a more robust methods development process, where more and more of these desirable behaviors can be included by design and leveraged in powerful ways. I will discuss several examples of heuristics and emergent behavior: subsampling and sketching in linear regression and their equivalence to ridge regularization; empirical risk minimization and the universality of relative performances under distribution shifts; and adaptivity in dropout and feature learning models which are equivalent to parsimony-promoting sparse or low-rank regularization.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/pal.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://www.cis.jhu.edu/~ambar/">Ambar Pal</a></h3><p>Johns Hopkins University</p><p class="organizer-meta">Ph.D. Student</p></div></div><p><strong>Title</strong>: The Role of Parsimonious Structures in Data for Trustworthy Machine Learning</p><p><strong>Abstract</strong>: This talk overviews recent theoretical results in the geometric foundations of adversarially robust machine learning. Modern ML classifiers can fail spectacularly when subject to specially crafted input-perturbations, called adversarial examples. On the other hand, we humans are quite robust for several tasks involving vision. Motivated by this disconnect, in the first part of this talk we will take a deeper dive into the question of when exactly we can avoid adversarial examples. We will see that a key geometric property of the data-distribution — concentration on small-volume subsets of the input space — characterizes whether any robust classifier exists. In particular, this suggests that natural image distributions are concentrated. In the second part of this talk, we will empirically instantiate these results for a few concentrated data-distributions, and discover that utilizing such structure in data leads to classifiers that enjoy better provable robustness guarantees in several regimes. This talk is based on work at NeurIPS ’23, ’20 and TMLR ’23.</p><h2 id="rising-stars-session-2"> <a href="#rising-stars-session-2" class="anchor-heading" aria-labelledby="rising-stars-session-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Rising Stars Session 2</h2><h4 id="time-day-2-jan-4--thursday--1000-am-to-1100-am"> <a href="#time-day-2-jan-4--thursday--1000-am-to-1100-am" class="anchor-heading" aria-labelledby="time-day-2-jan-4--thursday--1000-am-to-1100-am"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 2 (Jan 4) – Thursday – 10:00 AM to 11:00 AM</a></h4><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/muthukumar.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://ramcha24.github.io/">Ramchandran Muthukumar</a></h3><p>Johns Hopkins University</p><p class="organizer-meta">Ph.D. Student</p></div></div><p><strong>Title</strong>: Sparsity-aware generalization theory for deep neural networks</p><p><strong>Abstract</strong>: Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/parhi.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://rahul.sh/">Rahul Parhi</a></h3><p>École Polytechnique Fédérale de Lausanne</p><p class="organizer-meta">Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: On the Sparsity-Promoting Effect of Weight Decay in Deep Learning</p><p><strong>Abstract</strong>: Deep learning has been wildly successful in practice and most state-of-the-art artificial intelligence systems are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this talk, I present a new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of trained neural networks through the lens of sparsity. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory. This framework explains the effect of weight decay regularization in neural network training, the importance of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networks can perform well in high-dimensional problems.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/liu.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://shiweiliuiiiiiii.github.io/">Shiwei Liu</a></h3><p>University of Texas at Austin / Eindhoven University of Technology / University of Oxford</p><p class="organizer-meta">IFML Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Sparsity in Neural Networks: Science and Practice</p><p><strong>Abstract</strong>: Sparsity has demonstrated its remarkable performance in the realm of model compression through the selectively eliminating a large portion of model parameters. Nevertheless, conventional methods to discover strong sparse neural networks often necessitate the training of an over-parameterized dense model, followed by iterative cycles of pruning and re-training. As the size of modern neural networks exponentially increases, the costs of dense pre-training and updates have become increasingly prohibitive. In this talk, I will introduce an approach that enables the training of sparse neural networks from scratch, without the need for any pre-training steps or dense updates. By achieving the property of over-parameterization in time, our approach demonstrates the capacity to achieve performance levels equivalent to fully dense networks while utilizing only a very small fraction of weights. Beyond the advantages in model compression, I will also elucidate a broader spectrum of benefits of sparsity in neural networks including scalability, robustness, and fairness, and great potentials build large-scale responsible AI.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/montasser.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://home.ttic.edu/~omar/">Omar Montasser</a></h3><p>University of California, Berkeley</p><p class="organizer-meta">FODSI-Simons Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Theoretical Foundations of Adversarially Robust Learning</p><p><strong>Abstract</strong>: Despite extraordinary progress, current machine learning systems have been shown to be brittle against adversarial examples: seemingly innocuous but carefully crafted perturbations of test examples that cause machine learning predictors to misclassify. Can we learn predictors robust to adversarial examples? and how? There has been much empirical interest in this major challenge in machine learning, and in this talk, we will present a theoretical perspective. We will illustrate the need to go beyond traditional approaches and principles, such as empirical (robust) risk minimization, and present new algorithmic ideas with stronger robust learning guarantees.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/huang.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://nhuang37.github.io/">Ningyuan Huang</a></h3><p>Johns Hopkins University</p><p class="organizer-meta">Ph.D. Student</p></div></div><p><strong>Title</strong>: Approximately Equivariant Graph Networks</p><p><strong>Abstract</strong>: Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to active symmetries, whereas the permutation equivariance of GNNs corresponds to passive symmetries. In this talk, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We propose approximately equivariant graph networks to implement these symmetries and investigate the symmetry model selection problem. We theoretically and empirically show a bias-variance tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group.</p><h2 id="rising-stars-session-3"> <a href="#rising-stars-session-3" class="anchor-heading" aria-labelledby="rising-stars-session-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Rising Stars Session 3</h2><h4 id="time-day-3-jan-5--friday--1000-am-to-1100-am"> <a href="#time-day-3-jan-5--friday--1000-am-to-1100-am" class="anchor-heading" aria-labelledby="time-day-3-jan-5--friday--1000-am-to-1100-am"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time: <a href="/program_schedule/">Day 3 (Jan 5) – Friday – 10:00 AM to 11:00 AM</a></h4><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/ding.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://www.lijunding.net/">Lijun Ding</a></h3><p>University of Wisconsin / University of Washington</p><p class="organizer-meta">IFDS Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Optimization for statistical learning with low dimensional structure: regularity and conditioning</p><p><strong>Abstract</strong>: Many statistical machine learning problems, where one aims to recover an underlying low-dimensional signal, are based on optimization. Existing work often overlooked the computational complexity in solving the optimization problem, or required case-specific algorithm and analysis – especially for nonconvex problems. This talk addresses the above two issues from a unified perspective of conditioning. In particular, we show that once the sample size exceeds the intrinsic dimension, (1) a broad class of convex and nonsmooth nonconvex problems are well-conditioned, (2) well conditioning in turn ensures the efficiency of out-of-box optimization methods and inspires new algorithms. Lastly, we show that a conditioning notion called flatness leads to accurate recovery in overparametrized models.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/li.jpg" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://www.ece.iastate.edu/~lishuang/">Shuang Li</a></h3><p>Iowa State University</p><p class="organizer-meta">Assistant Professor</p></div></div><p><strong>Title</strong>: The Future Geometric Analysis of Optimization Problems in Signal Processing and Machine Learning</p><p><strong>Abstract</strong>: High-dimensional data analysis and estimation appear in many signal processing and machine learning applications. The underlying low-dimensional structure in these high-dimensional data inspires us to develop optimality guarantees as well as optimization-based techniques for the fundamental problems in signal processing and machine learning. In recent years, non-convex optimization widely appears in engineering and is solved by many heuristic local algorithms, but lacks global guarantees. The recent geometric/landscape analysis provides a way to determine whether an iterative algorithm can reach global optimality. The landscape of empirical risk has been widely studied in a series of machine learning problems, including low-rank matrix factorization, matrix sensing, matrix completion, and phase retrieval. A favorable geometry guarantees that many algorithms can avoid saddle points and converge to local minima. In this presentation, I will discuss potential directions for the future geometric analysis of optimization problems in signal processing and machine learning.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/tolooshams.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://btolooshams.github.io/">Bahareh Tolooshams</a></h3><p>California Institute of Technology</p><p class="organizer-meta">Postdoctoral Researcher</p></div></div><p><strong>Title</strong>: Deep Interpretable Generative Learning for Science and Engineering</p><p><strong>Abstract</strong>: Discriminative and generative AI are two deep learning paradigms that revolutionized prediction and generation of high-quality images from text prompts. Nonetheless, discriminative learning is unable to generate data, and deep generative models struggle with decoding capabilities. Moreover, both approaches are data-hungry and have low interpretability. These drawbacks have posed significant barriers to the adoption of deep learning in applications where a) acquiring supervised data is expensive or infeasible, and b) goals extend beyond data fitting to attain scientific insights. Furthermore, deep learning applications are fairly unexplored in fields with rich mathematical and optimization frameworks such as inverse problems, or those in which interpretability matters. This talk discusses the theory and applications of deep learning in data-limited or unsupervised inverse problems. These include applications in radar sensing, Poisson image denoising, and computational neuroscience.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/ziv.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://www.ravid-shwartz-ziv.com/">Ravid Shwartz Ziv</a></h3><p>New York University</p><p class="organizer-meta">CDS Faculty Fellow</p></div></div><p><strong>Title</strong>: Decoding the Information Bottleneck in Self-Supervised Learning: Pathway to Optimal Representation</p><p><strong>Abstract</strong>: Deep Neural Networks (DNNs) have excelled in many fields, largely due to their proficiency in supervised learning tasks. However, the dependence on vast labeled data becomes a constraint when such data is scarce. Self-Supervised Learning (SSL), a promising approach, harnesses unlabeled data to derive meaningful representations. Yet, how SSL filters irrelevant information without explicit labels remains unclear. In this talk, we aim to unravel the enigma of SSL using the lens of Information Theory, with a spotlight on the Information Bottleneck principle. This principle, while providing a sound understanding of the balance between compressing and preserving relevant features in supervised learning, presents a puzzle when applied to SSL due to the absence of labels during training. We will delve into the concept of ‘optimal representation’ in SSL, its relationship with data augmentations, optimization methods, and downstream tasks, and how SSL training learns and achieves optimal representations. Our discussion unveils our pioneering discoveries, demonstrating how SSL training naturally leads to the creation of optimal, compact representations that correlate with semantic labels. Remarkably, SSL seems to orchestrate an alignment of learned representations with semantic classes across multiple hierarchical levels, an alignment that intensifies during training and grows more defined deeper into the network. Considering these insights and their implications for class set performance, we conclude our talk by applying our analysis to devise more robust SSL-based information algorithms. These enhancements in transfer learning could lead to more efficient learning systems, particularly in data-scarce environments.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/wangh.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://hwang595.github.io/">Hongyi Wang</a></h3><p>Carnegie Mellon University</p><p class="organizer-meta">Senior Project Scientist</p></div></div><p><strong>Title</strong>: Speeding up Large-Scale Machine Learning Model Development Using Low-Rank Models and Gradients</p><p><strong>Abstract</strong>: Large-scale machine learning (ML) models, such as GPT-4 and Llama2, are at the forefront of advances in the field of AI. Nonetheless, developing these large-scale ML models demands substantial computational resources and a deep understanding of distributed ML and systems. In this presentation, I will introduce three frameworks, namely ATOMO, Pufferfish, and Cuttlefish, which use low-rank approximations on model gradients and model weights to significantly expedite ML model training. ATOMO is a general compression framework that has experimentally established that using low-rank gradients, as opposed to sparse ones, can lead to substantially faster distributed training. Pufferfish further bypasses the cost of compression by directly training low-rank models. However, directly training low-rank models usually leads to a loss in accuracy. Pufferfish mitigates this issue by training a full-rank model and then converting to a low-rank model early in the training process. Nonetheless, Pufferfish necessitates extra hyperparameter tuning, such as determining the optimal transition time from full-rank to low-rank. Cuttlefish addresses this issue by automatically estimating and adjusting these hyperparameters during training. I will present extensive experimental results on the distributed training of large-scale ML models, including LLMs, to demonstrate the efficacy of these frameworks.</p><div class="organizer" style="width: 545px;"> <img class="organizer-image" src="/assets/images/risingstars/lu.png" alt="" /><div style="padding: 10px 0;"><h3 class="organizer-name no_anchor"> <a href="https://2prime.github.io/">Yiping Lu</a></h3><p>New York University</p><p class="organizer-meta">Courant Instructor</p></div></div><p><strong>Title</strong>: Simulation-Calibrated Scientific Machine Learning</p><p><strong>Abstract</strong>: Machine learning (ML) has achieved great success in a variety of applications suggesting a new way to build flexible, universal, and efficient approximators for complex high-dimensional data. These successes have inspired many researchers to apply ML to other scientific applications such as industrial engineering, scientific computing, and operational research, where similar challenges often occur. However, the luminous success of ML is overshadowed by persistent concerns that the mathematical theory of large-scale machine learning, especially deep learning, is still lacking and the trained ML predictor is always biased. In this talk, I’ll introduce a novel framework of (S)imulation-(Ca)librated (S)cientific (M)achine (L)earning (SCaSML), which can leverage the structure of physical models to achieve the following goals: 1) make unbiased predictions even based on biased machine learning predictors; 2) beat the curse of dimensionality with an estimator suffers from it. The SCASML paradigm combines a (possibly) biased machine learning algorithm with a de-biasing step design using rigorous numerical analysis and stochastic simulation. Theoretically, I’ll try to understand whether the SCaSML algorithms are optimal and what factors (e.g., smoothness, dimension, and boundness) determine the improvement of the convergence rate. Empirically, I’ll introduce different estimators that enable unbiased and trustworthy estimation for physical quantities with a biased machine learning estimator. Applications include but are not limited to estimating the moment of a function, simulating high-dimensional stochastic processes, uncertainty quantification using bootstrap methods, and randomized linear algebra.</p></main></div></div><div class="search-overlay"></div></div>
